import logging
import os
import asyncio
# Nh√≥m c√°c import th∆∞ vi·ªán b√™n ngo√†i
import feedparser
import httpx
import redis.asyncio as aioredis
import google.generativeai as genai
# Nh√≥m import telegram
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes, MessageHandler, filters
# Nh√≥m c√°c import kh√°c
import re
from urllib.parse import urlparse
import unicodedata
import datetime
import pytz
from typing import Dict, List, Any, Optional
import pickle
from functools import wraps
# Import cho vi·ªác ph√°t hi·ªán tin tr√πng l·∫∑p
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Import cho sentiment analysis ti·∫øng Vi·ªát
import numpy as np
import requests
import json
# Import th√™m signal ƒë·ªÉ x·ª≠ l√Ω t√≠n hi·ªáu ƒë√≥ng/kh·ªüi ƒë·ªông l·∫°i
import signal
import sys
import hashlib
# Th√™m import cho ph√°t hi·ªán ng√¥n ng·ªØ v√† d·ªãch
from langdetect import detect
from googletrans import Translator

# --- 1. Config & setup ---
class Config:
    BOT_TOKEN = os.getenv("BOT_TOKEN")
    WEBHOOK_URL = os.getenv("WEBHOOK_URL")
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost")
    DB_URL = os.getenv("DATABASE_URL")
    ADMIN_ID = int(os.getenv("ADMIN_ID", "1225226589"))
    GOOGLE_GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-preview-05-20")
    OPENROUTER_FALLBACK_MODEL = os.getenv("OPENROUTER_FALLBACK_MODEL", "deepseek/deepseek-chat-v3-0324:free")
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    GROQ_MODEL = os.getenv("GROQ_MODEL", "deepseek-r1-distill-llama-70b")
    FEED_URLS = [
        # Google News theo t·ª´ kh√≥a
        "https://news.google.com/rss/search?q=kinh+t%E1%BA%BF&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch%E1%BB%A9ng+kho%C3%A1n&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=v%C4%A9+m%C3%B4&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=chi%E1%BA%BFn+tranh&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=l%C3%A3i+su%E1%BA%A5t&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=fed&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=tin+n%C3%B3ng&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=%C4%91%E1%BA%A7u+t%C6%B0&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=doanh+nghi%E1%BB%87p&hl=vi&gl=VN&ceid=VN:vi",
        # Ch√≠nh tr·ªã th·∫ø gi·ªõi, quan h·ªá qu·ªëc t·∫ø
        "https://news.google.com/rss/search?q=ch%C3%ADnh+tr%E1%BB%8B+th%E1%BA%BF+gi%E1%BB%9Bi&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=geopolitics&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=world+politics&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=international+relations&hl=vi&gl=VN&ceid=VN:vi",
        # Qu·ªëc t·∫ø (Google News search c√°c ngu·ªìn qu·ªëc t·∫ø)
        "https://news.google.com/rss/search?q=site:bloomberg.com+stock+OR+market+OR+finance&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:ft.com+stock+OR+market+OR+finance&hl=vi&gl=VN&ceid=VN:vi",
        # B·ªï sung c√°c ngu·ªìn Google News RSS t·ªëi ∆∞u
        # Ch·ª©ng kho√°n Vi·ªát Nam t·ª´ c√°c b√°o l·ªõn
        "https://news.google.com/rss/search?q=ch·ª©ng+kho√°n+site:cafef.vn&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch·ª©ng+kho√°n+site:vnexpress.net&hl=vi&gl=VN&ceid=VN:vi",
        # Ch√≠nh s√°ch vƒ© m√¥
        "https://news.google.com/rss/search?q=ch√≠nh+s√°ch+vƒ©+m√¥&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=l√£i+su·∫•t+site:sbv.gov.vn&hl=vi&gl=VN&ceid=VN:vi",
        # Bi·∫øn ƒë·ªông th·∫ø gi·ªõi, t√†i ch√≠nh qu·ªëc t·∫ø
        "https://news.google.com/rss/search?q=market+crash&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:reuters.com+economy+OR+policy&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:theguardian.com+world+OR+politics&hl=vi&gl=VN&ceid=VN:vi",
        # --- Th√™m ngu·ªìn PiQ v√† CNWire ---
        # PiQ Twitter/X (d·∫°ng RSS qua Nitter ho·∫∑c d·ªãch v·ª• RSS Twitter n·∫øu c√≥, v√≠ d·ª• nitter.net)
        "https://nitter.net/piqsuite/rss",  # N·∫øu kh√¥ng d√πng ƒë∆∞·ª£c, c·∫ßn thay b·∫±ng d·ªãch v·ª• RSS Twitter kh√°c
        # CNWire (Canada Newswire RSS)
        "https://www.newswire.ca/news-releases/news-releases-rss-feed-en.xml",
    ]
    REDIS_TTL = int(os.getenv("REDIS_TTL", "60000"))  # 6h
    NEWS_JOB_INTERVAL = int(os.getenv("NEWS_JOB_INTERVAL", "800"))
    HOURLY_JOB_INTERVAL = int(os.getenv("HOURLY_JOB_INTERVAL", "500"))  # ... ph√∫t/l·∫ßn
    FETCH_LIMIT_DAYS = int(os.getenv("FETCH_LIMIT_DAYS", "1"))  # Ch·ªâ l·∫•y tin 1 ng√†y g·∫ßn nh·∫•t 
    MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))  # S·ªë l·∫ßn th·ª≠ l·∫°i khi feed l·ªói
    MAX_NEWS_PER_CYCLE = int(os.getenv("MAX_NEWS_PER_CYCLE", "1"))  # T·ªëi ƒëa 1 tin m·ªói l·∫ßn
    TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')  # Timezone chu·∫©n cho Vi·ªát Nam
    
    # C·∫•u h√¨nh ph√°t hi·ªán tin tr√πng l·∫∑p - n√¢ng c·∫•p
    DUPLICATE_THRESHOLD = float(os.getenv("DUPLICATE_THRESHOLD", "0.65"))  # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán tr√πng l·∫∑p n·ªôi dung
    TITLE_SIMILARITY_THRESHOLD = float(os.getenv("TITLE_SIMILARITY_THRESHOLD", "0.92"))  # Ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±
    LOG_SIMILARITY_DETAILS = os.getenv("LOG_SIMILARITY_DETAILS", "False").lower() == "true"  # B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity
    
    RECENT_NEWS_DAYS = int(os.getenv("RECENT_NEWS_DAYS", "2"))  # S·ªë ng√†y ƒë·ªÉ l·∫•y tin g·∫ßn ƒë√¢y ƒë·ªÉ so s√°nh
    
    # C·∫•u h√¨nh ph√°t hi·ªán tin n√≥ng
    HOT_NEWS_KEYWORDS = [
        "kh·∫©n c·∫•p", "tin n√≥ng", "breaking", "kh·ªßng ho·∫£ng", "crash", "s·∫≠p", "b√πng n·ªï", "tin nhanh ch·ª©ng kho√°n", "tr∆∞·ªõc gi·ªù giao d·ªãch", 
        "shock", "·∫£nh h∆∞·ªüng l·ªõn", "th·∫£m kh·ªëc", "th·∫£m h·ªça", "market crash", "sell off", "VNINDEX", "vnindex", "Trump", "fed", "FED",
        "r∆°i m·∫°nh", "tƒÉng m·∫°nh", "gi·∫£m m·∫°nh", "s·ª•p ƒë·ªï", "b·∫•t th∆∞·ªùng", "emergency", 
        "urgent", "alert", "c·∫£nh b√°o", "ƒë·ªôt bi·∫øn", "l·ªãch s·ª≠", "k·ª∑ l·ª•c", "cao nh·∫•t"
    ]
    HOT_NEWS_IMPACT_PHRASES = [
        "t√°c ƒë·ªông m·∫°nh", "·∫£nh h∆∞·ªüng nghi√™m tr·ªçng", "thay ƒë·ªïi l·ªõn", "bi·∫øn ƒë·ªông m·∫°nh",
        "tr·ªçng ƒëi·ªÉm", "quan tr·ªçng", "ƒë√°ng ch√∫ √Ω", "ƒë√°ng lo ng·∫°i", "c·∫ßn l∆∞u √Ω"
    ]
    
    # Danh s√°ch t·ª´ kh√≥a l·ªçc tin t·ª©c li√™n quan (m·ªü r·ªông)
    RELEVANT_KEYWORDS = [
        # Ch√≠nh tr·ªã, vƒ© m√¥, doanh nghi·ªáp, ch·ª©ng kho√°n, chi·∫øn tranh 
        "ch√≠nh tr·ªã", "vƒ© m√¥", "doanh nghi·ªáp", "ch·ª©ng kho√°n", "chi·∫øn tranh", "ch√≠nh s√°ch", "l√£i su·∫•t", "fed",
        "phe", "ƒë·∫£ng", "ch√≠nh ph·ªß", "qu·ªëc h·ªôi", "nh√† n∆∞·ªõc", "b·ªô tr∆∞·ªüng", "th·ªß t∆∞·ªõng", "ch·ªß t·ªãch",
        # Nh√≥m ng√†nh, bluechip, midcap, th·ªã tr∆∞·ªùng
        "bluechip", "midcap", "ng√¢n h√†ng", "b·∫•t ƒë·ªông s·∫£n", "th√©p", "d·∫ßu kh√≠", "c√¥ng ngh·ªá", "b√°n l·∫ª",
        "xu·∫•t kh·∫©u", "ƒëi·ªán", "x√¢y d·ª±ng", "th·ªßy s·∫£n", "d∆∞·ª£c ph·∫©m", "logistics", "v·∫≠n t·∫£i", 
        # C√°c m√£ ch·ª©ng kho√°n, ch·ªâ s·ªë
        "vn30", "hnx", "upcom", "vnindex", "c·ªï phi·∫øu", "th·ªã tr∆∞·ªùng", "t√†i ch√≠nh", "kinh t·∫ø", 
        "gdp", "l·∫°m ph√°t", "t√≠n d·ª•ng", "tr√°i phi·∫øu", "ph√°i sinh", "qu·ªπ etf", 
        # C√°c m√£ bluechip VN30
        "fpt", "vnm", "vcb", "ssi", "msn", "mwg", "vic", "vhm", "hpg", "ctg", "bid", "mbb", "stb",
        "hdb", "bvh", "vpb", "nvl", "pdr", "tcb", "tpb", "bcm", "pnj", "acb", "vib", "plx",
        # C√°c m√£ midcap, c√°c ch·ªâ b√°o kinh t·∫ø
        "vnm", "cpi", "pmi", "m2", "ƒë·∫ßu t∆∞", "gdp", "xu·∫•t kh·∫©u", "nh·∫≠p kh·∫©u", "d·ª± tr·ªØ", "d·ª± b√°o",
        # T·ª´ kh√≥a t√†i ch√≠nh qu·ªëc t·∫ø
        "fed", "ecb", "boj", "pboc", "imf", "world bank", "nasdaq", "dow jones", "s&p", "nikkei",
        "treasury", "usd", "eur", "jpy", "cny", "bitcoin", "crypto", "commodities", "wti", "brent",
        # M√£ c·ªï phi·∫øu n·ªïi b·∫≠t
        "vnd", "ssi", "hpg", "vic", "vhm", "vnm", "mwg", "ctg", "bid", "tcb", "acb", "vib", "stb", "mbb", "shb",
        # T√™n c√¥ng ty l·ªõn
        "vinamilk", "vietcombank", "vietinbank", "masan", "fpt", "hoa phat", "vietjet", "petro vietnam",
        # S·ª± ki·ªán kinh t·∫ø
        "l·∫°m ph√°t", "tƒÉng tr∆∞·ªüng", "gi·∫£m ph√°t", "GDP", "CPI", "PMI", "xu·∫•t kh·∫©u", "nh·∫≠p kh·∫©u", "t√≠n d·ª•ng", "tr√°i phi·∫øu",
        # Thu·∫≠t ng·ªØ th·ªã tr∆∞·ªùng
        "bull", "bear", "breakout", "margin", "room ngo·∫°i", "ETF", "IPO", "ni√™m y·∫øt", "ph√°t h√†nh", "c·ªï t·ª©c", "chia th∆∞·ªüng",
        # S·ª± ki·ªán qu·ªëc t·∫ø
        "fed", "ecb", "boj", "nasdaq", "dow jones", "s&p", "nikkei", "usd", "eur", "jpy", "bitcoin", "crypto",
    ]

# Danh s√°ch t·ª´ kh√≥a b·ªï sung
additional_keywords = []

# Danh s√°ch t·ª´ kh√≥a ƒë·ªÉ lo·∫°i tr·ª´ tin kh√¥ng li√™n quan
EXCLUDE_KEYWORDS = [
    "khuy·∫øn m√£i", "gi·∫£m gi√°", "mua ngay", "tuy·ªÉn d·ª•ng", "s·ª± ki·ªán", "gi·∫£i tr√≠", "th·ªÉ thao", "lifestyle", 
    "du l·ªãch", "·∫©m th·ª±c", "hot deal", "sale off", "qu·∫£ng c√°o", "ƒë·∫∑t h√†ng", "shoppe", "tiki", "lazada"
]

# --- Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng b·∫Øt bu·ªôc ---
REQUIRED_ENV_VARS = ["BOT_TOKEN", "OPENROUTER_API_KEY"]
for var in REQUIRED_ENV_VARS:
    if not os.getenv(var):
        raise RuntimeError(f"Missing required environment variable: {var}")

# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Redis ---
redis_client = None

# L∆∞u tr·ªØ danh s√°ch user ƒë√£ ƒë∆∞·ª£c duy·ªát
approved_users = set()

# --- Flag ƒë·ªÉ ki·ªÉm so√°t vi·ªác t·∫Øt bot ---
shutdown_flag = False

# --- Cleanup function ---
async def cleanup_resources():
    """D·ªçn d·∫πp t√†i nguy√™n khi bot t·∫Øt"""
    global redis_client, application
    
    logger.info("ƒêang d·ªçn d·∫πp t√†i nguy√™n tr∆∞·ªõc khi t·∫Øt...")
    
    # D·ª´ng job queue n·∫øu ƒëang ch·∫°y
    if application and application.job_queue:
        logger.info("D·ª´ng job queue...")
        await application.job_queue.stop()
    
    # ƒê√≥ng k·∫øt n·ªëi Redis
    if redis_client:
        logger.info("ƒê√≥ng k·∫øt n·ªëi Redis...")
        await redis_client.close()
        redis_client = None
    
    logger.info("ƒê√£ d·ªçn d·∫πp t·∫•t c·∫£ t√†i nguy√™n.")

# --- Signal handlers ---
def signal_handler(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu t·∫Øt t·ª´ h·ªá th·ªëng"""
    global shutdown_flag
    
    if shutdown_flag:
        logger.warning("Nh·∫≠n t√≠n hi·ªáu t·∫Øt l·∫ßn hai, t·∫Øt ngay l·∫≠p t·ª©c!")
        sys.exit(1)
    
    logger.info(f"Nh·∫≠n t√≠n hi·ªáu {sig}, chu·∫©n b·ªã t·∫Øt bot...")
    shutdown_flag = True
    
    # Ch·∫°y cleanup trong asyncio event loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(shutdown())
        else:
            loop.run_until_complete(shutdown())
    except Exception as e:
        logger.error(f"L·ªói khi d·ªçn d·∫πp t√†i nguy√™n: {e}")
        sys.exit(1)

async def shutdown():
    """X·ª≠ l√Ω shutdown m·ªôt c√°ch ƒë·ªìng b·ªô"""
    global application
    
    logger.info("B·∫Øt ƒë·∫ßu quy tr√¨nh shutdown...")
    
    # D·ªçn d·∫πp t√†i nguy√™n
    await cleanup_resources()
    
    # D·ª´ng bot n·∫øu ƒëang ch·∫°y
    if application:
        logger.info("D·ª´ng bot...")
        if hasattr(application, 'stop'):
            await application.stop()
        application = None
    
    logger.info("Bot ƒë√£ t·∫Øt ho√†n to√†n.")
    sys.exit(0)

async def is_sent(entry_id):
    return await redis_client.sismember("sent_news", entry_id)

async def mark_sent(entry_id):
    await redis_client.sadd("sent_news", entry_id)
    await redis_client.expire("sent_news", Config.REDIS_TTL)

# --- AI Analysis (Gemini) ---
GEMINI_MODEL = Config.GEMINI_MODEL
OPENROUTER_FALLBACK_MODEL = Config.OPENROUTER_FALLBACK_MODEL
GOOGLE_GEMINI_API_KEY = Config.GOOGLE_GEMINI_API_KEY

async def call_groq_api(prompt, model=None):
    """G·ªçi Groq API ƒë·ªÉ l·∫•y k·∫øt qu·∫£ AI ph√¢n t√≠ch"""
    api_key = Config.GROQ_API_KEY
    model = model or Config.GROQ_MODEL
    if not api_key:
        raise RuntimeError("GROQ_API_KEY is not set")
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7
                }
            )
            result = response.json()
            if "choices" not in result:
                logging.error(f"Groq API error (model={model}): {result}")
                raise RuntimeError(f"Groq API error: {result}")
            return result["choices"][0]["message"]["content"]
    except Exception as e:
        logging.error(f"Groq API l·ªói: {e}")
        raise e

async def analyze_news(prompt, model=None):
    try:
        # G·ªçi Google Gemini API ch√≠nh th·ª©c
        genai.configure(api_key=GOOGLE_GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash-preview-04-17")
        response = await asyncio.to_thread(model.generate_content, prompt)
        return response.text
    except Exception as e:
        logging.error(f"Gemini API l·ªói: {e}, fallback sang OpenRouter {OPENROUTER_FALLBACK_MODEL}")
        # Fallback sang OpenRouter
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}",},
                    json={
                        "model": OPENROUTER_FALLBACK_MODEL,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7
                    }
                )
                result = response.json()
                if "choices" not in result:
                    logging.error(f"OpenRouter API error (model={OPENROUTER_FALLBACK_MODEL}): {result}")
                    raise RuntimeError(f"OpenRouter API error: {result}")
                return result["choices"][0]["message"]["content"]
        except Exception as e2:
            logging.error(f"OpenRouter fallback c≈©ng l·ªói: {e2}, th·ª≠ ti·∫øp Groq...")
            # Fallback sang Groq
            try:
                return await call_groq_api(prompt)
            except Exception as e3:
                logging.error(f"Groq fallback c≈©ng l·ªói: {e3}")
                raise e3

# --- Extract sentiment from AI result ---
def extract_sentiment_rule_based(ai_summary):
    """Extract sentiment from AI summary using rule-based approach"""
    sentiment = "Trung l·∫≠p"  # Default
    try:
        # T√¨m ki·∫øm d√≤ng c√≥ ch·ª©a 'c·∫£m x√∫c'
        for line in ai_summary.splitlines():
            line_lower = line.lower()
            if "c·∫£m x√∫c:" in line_lower or "sentiment:" in line_lower:
                sentiment_text = line.split(":")[-1].strip().lower()
                if "t√≠ch c·ª±c" in sentiment_text or "positive" in sentiment_text:
                    return "T√≠ch c·ª±c"
                elif "ti√™u c·ª±c" in sentiment_text or "negative" in sentiment_text:
                    return "Ti√™u c·ª±c"
                else:
                    return "Trung l·∫≠p"
        # N·∫øu kh√¥ng t√¨m th·∫•y d√≤ng c·∫£m x√∫c, d√πng regex t√¨m to√†n vƒÉn b·∫£n
        text = ai_summary.lower()
        if re.search(r"(t√≠ch c·ª±c|positive|l·∫°c quan|upbeat|bullish)", text):
            return "T√≠ch c·ª±c"
        elif re.search(r"(ti√™u c·ª±c|negative|bi quan|bearish|lo ng·∫°i|lo l·∫Øng)", text):
            return "Ti√™u c·ª±c"
    except Exception as e:
        logging.warning(f"L·ªói khi parse sentiment rule-based: {e}")
    return sentiment

async def extract_sentiment(ai_summary):
    """Extract sentiment from AI summary, ch·ªâ s·ª≠ d·ª•ng rule-based"""
    return extract_sentiment_rule_based(ai_summary)

def is_hot_news(entry, ai_summary, sentiment):
    """Ph√°t hi·ªán tin n√≥ng d·ª±a tr√™n ph√¢n t√≠ch n·ªôi dung, t·ª´ kh√≥a v√† c·∫£m x√∫c"""
    try:
        title = getattr(entry, 'title', '').lower()
        summary = getattr(entry, 'summary', '').lower()
        content_text = f"{title} {summary}".lower()
        
        # 1. Ki·ªÉm tra t·ª´ kh√≥a tin n√≥ng trong ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung
        for keyword in Config.HOT_NEWS_KEYWORDS:
            if keyword.lower() in content_text:
                logging.info(f"Hot news ph√°t hi·ªán b·ªüi t·ª´ kh√≥a '{keyword}': {title}")
                return True
                
        # 2. Ki·ªÉm tra c√°c c·ª•m t·ª´ ·∫£nh h∆∞·ªüng trong AI summary
        ai_text = ai_summary.lower()
        for phrase in Config.HOT_NEWS_IMPACT_PHRASES:
            if phrase.lower() in ai_text:
                logging.info(f"Hot news ph√°t hi·ªán b·ªüi c·ª•m t·ª´ ·∫£nh h∆∞·ªüng '{phrase}': {title}")
                return True
        
        # 3. Ph√¢n t√≠ch d·ª±a tr√™n c·∫£m x√∫c v√† m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng
        if sentiment != "Trung l·∫≠p":
            # N·∫øu c√≥ c·∫£m x√∫c v√† c√°c t·ª´ ch·ªâ m·ª©c ƒë·ªô cao trong ph√¢n t√≠ch AI
            intensity_words = ["r·∫•t", "m·∫°nh", "nghi√™m tr·ªçng", "ƒë√°ng k·ªÉ", "l·ªõn", "quan tr·ªçng"]
            for word in intensity_words:
                if word in ai_text and (
                    "th·ªã tr∆∞·ªùng" in ai_text or "nh√† ƒë·∫ßu t∆∞" in ai_text or "·∫£nh h∆∞·ªüng" in ai_text
                ):
                    logging.info(f"Hot news ph√°t hi·ªán b·ªüi c·∫£m x√∫c v√† m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng: {title}")
                    return True
        
        return False
    except Exception as e:
        logging.warning(f"L·ªói khi ph√°t hi·ªán tin n√≥ng: {e}")
        return False

# --- Parse RSS Feed & News Processing ---
def normalize_text(text):
    if not text:
        return ""
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ l·∫°i ch·ªØ v√† s·ªë
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)
    # Vi·∫øt th∆∞·ªùng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = text.lower().strip()
    return text

def is_recent_news(entry, days=Config.FETCH_LIMIT_DAYS):
    """
    Ch·ªâ l·∫•y tin c√≥ ng√†y ƒëƒÉng l√† h√¥m nay (theo timezone Vi·ªát Nam).
    N·∫øu kh√¥ng parse ƒë∆∞·ª£c ng√†y th√¨ b·ªè qua (kh√¥ng l·∫•y).
    """
    published = getattr(entry, 'published', None) or getattr(entry, 'updated', None)
    if not published:
        return False
    try:
        # Th·ª≠ parse nhi·ªÅu ƒë·ªãnh d·∫°ng ng√†y th√°ng
        try:
            dt = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %Z')
        except ValueError:
            try:
                dt = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
            except ValueError:
                # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, b·ªè qua tin n√†y
                return False
        # ƒê·∫£m b·∫£o c√≥ timezone
        if not dt.tzinfo:
            dt = Config.TIMEZONE.localize(dt)
        now = get_now_with_tz()
        # So s√°nh ng√†y (kh√¥ng so s√°nh gi·ªù)
        return dt.date() == now.date()
    except Exception as e:
        logger.warning(f"L·ªói khi ki·ªÉm tra th·ªùi gian tin: {e}")
        # N·∫øu c√≥ l·ªói, b·ªè qua tin n√†y
        return False

def is_relevant_news_smart(entry):
    """
    L·ªçc tin th√¥ng minh: nhi·ªÅu t·ª´ kh√≥a li√™n quan, lo·∫°i tr·ª´ spam/PR.
    """
    title = normalize_text(getattr(entry, 'title', ''))
    summary = normalize_text(getattr(entry, 'summary', ''))
    content_text = f"{title} {summary}"

    # Lo·∫°i tr·ª´ tin spam/PR
    for ex_kw in EXCLUDE_KEYWORDS:
        if normalize_text(ex_kw) in content_text:
            return False

    # ƒê·∫øm s·ªë t·ª´ kh√≥a li√™n quan xu·∫•t hi·ªán
    all_keywords = [normalize_text(k) for k in Config.RELEVANT_KEYWORDS] + [normalize_text(k) for k in additional_keywords]
    match_count = sum(1 for kw in all_keywords if kw and kw in content_text)
    
    # Ph·∫£i c√≥ √≠t nh·∫•t 1 t·ª´ kh√≥a
    return match_count >= 1

def is_hot_news_simple(entry):
    """
    Ph√°t hi·ªán tin n√≥ng m√† kh√¥ng d√πng AI, ch·ªâ d·ª±a tr√™n t·ª´ kh√≥a.
    """
    title = getattr(entry, 'title', '').lower()
    summary = getattr(entry, 'summary', '').lower()
    content_text = f"{title} {summary}".lower()
    
    for keyword in Config.HOT_NEWS_KEYWORDS:
        if keyword.lower() in content_text:
            logger.info(f"Hot news ƒë∆°n gi·∫£n ph√°t hi·ªán b·ªüi t·ª´ kh√≥a '{keyword}': {title}")
            return True
    
    return False

def is_relevant_news(entry):
    """
    Ki·ªÉm tra xem tin t·ª©c c√≥ li√™n quan ƒë·∫øn c√°c ch·ªß ƒë·ªÅ quan t√¢m kh√¥ng d·ª±a tr√™n t·ª´ kh√≥a (chu·∫©n h√≥a)
    """
    # L·∫•y n·ªôi dung t·ª´ ti√™u ƒë·ªÅ v√† t√≥m t·∫Øt, chu·∫©n h√≥a
    title = normalize_text(getattr(entry, 'title', ''))
    summary = normalize_text(getattr(entry, 'summary', ''))
    content_text = f"{title} {summary}"

    # Chu·∫©n h√≥a t·ª´ kh√≥a m·∫∑c ƒë·ªãnh v√† b·ªï sung
    all_keywords = [normalize_text(k) for k in Config.RELEVANT_KEYWORDS] + [normalize_text(k) for k in additional_keywords]

    # So kh·ªõp t·ª´ kh√≥a
    for keyword in all_keywords:
        if keyword and keyword in content_text:
            return True
    return False

async def parse_feed(url):
    try:
        feed_data = await asyncio.to_thread(feedparser.parse, url)
        if not feed_data.entries:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y tin t·ª©c t·ª´ feed: {url}")
            return []
        return feed_data.entries
    except Exception as e:
        logger.error(f"L·ªói khi parse RSS feed {url}: {e}")
        return []

def extract_image_url(entry):
    """Extract image URL from entry if available"""
    try:
        if 'media_content' in entry and entry.media_content:
            for media in entry.media_content:
                if 'url' in media:
                    return media['url']
        
        # Try finding image in content
        if 'content' in entry and entry.content:
            for content in entry.content:
                if 'value' in content:
                    match = re.search(r'<img[^>]+src="([^">]+)"', content['value'])
                    if match:
                        return match.group(1)
        
        # Try finding image in summary
        if hasattr(entry, 'summary'):
            match = re.search(r'<img[^>]+src="([^">]+)"', entry.summary)
            if match:
                return match.group(1)
    except Exception as e:
        logger.warning(f"L·ªói khi extract ·∫£nh: {e}")
    
    return None
    
# --- Command Handler Functions ---

# Admin only decorator
def admin_only(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        if user_id != Config.ADMIN_ID:
            await update.message.reply_text("‚ùå B·∫°n kh√¥ng c√≥ quy·ªÅn s·ª≠ d·ª•ng l·ªánh n√†y.")
            return
        return await func(update, context, *args, **kwargs)
    return wrapped

# --- Redis-only user approval ---
async def is_user_approved(user_id):
    # Lu√¥n coi ADMIN_ID l√† approved, kh√¥ng c·∫ßn l∆∞u v√†o Redis
    return user_id == Config.ADMIN_ID or await redis_client.sismember("approved_users", user_id)

# --- User approval logic using Redis set ---
async def approve_user_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    action, user_id = query.data.split("_")
    user_id = int(user_id)
    if action == "approve":
        if user_id != Config.ADMIN_ID:
            await redis_client.sadd("approved_users", user_id)
        await query.edit_message_text(f"‚úÖ User {user_id} ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát.")
        await context.bot.send_message(chat_id=user_id, text="‚úÖ B·∫°n ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng Bot News! G√µ /help ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    else:
        await query.edit_message_text(f"‚ùå ƒê√£ t·ª´ ch·ªëi y√™u c·∫ßu t·ª´ user {user_id}.")
        await context.bot.send_message(chat_id=user_id, text="‚ùå Y√™u c·∫ßu s·ª≠ d·ª•ng bot c·ªßa b·∫°n ƒë√£ b·ªã t·ª´ ch·ªëi.")

# --- L∆∞u v√† l·∫•y ti√™u ƒë·ªÅ/tin g·∫ßn ƒë√¢y b·∫±ng Redis list (gi·ªõi h·∫°n 200) ---
async def add_recent_title(normalized_title, limit=200):
    await redis_client.lpush("recent_titles", normalized_title)
    await redis_client.ltrim("recent_titles", 0, limit-1)

async def get_recent_titles(limit=200):
    return [title.decode() if isinstance(title, bytes) else title for title in await redis_client.lrange("recent_titles", 0, limit-1)]

async def add_recent_news_text(news_text, limit=200):
    await redis_client.lpush("recent_news_texts", news_text)
    await redis_client.ltrim("recent_news_texts", 0, limit-1)

async def get_recent_news_texts(limit=200):
    return [txt.decode() if isinstance(txt, bytes) else txt for txt in await redis_client.lrange("recent_news_texts", 0, limit-1)]

# --- Registration commands
async def register_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    user_id = user.id
    
    # Check if already approved
    if await is_user_approved(user_id):
        await update.message.reply_text("‚úÖ B·∫°n ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω s·ª≠ d·ª•ng bot r·ªìi!")
        return
    
    # Notify admin about registration request
    keyboard = [
        [
            InlineKeyboardButton("Approve ‚úÖ", callback_data=f"approve_{user_id}"),
            InlineKeyboardButton("Deny ‚ùå", callback_data=f"deny_{user_id}")
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await context.bot.send_message(
        chat_id=Config.ADMIN_ID,
        text=(
            f"üîî Y√™u c·∫ßu ƒëƒÉng k√Ω m·ªõi:\n"
            f"User ID: {user_id}\n"
            f"Name: {user.first_name} {user.last_name or ''}\n"
            f"Username: @{user.username or 'N/A'}"
        ),
        reply_markup=reply_markup
    )
    
    await update.message.reply_text(
        "üìù Y√™u c·∫ßu ƒëƒÉng k√Ω c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c g·ª≠i t·ªõi admin. "
        "B·∫°n s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o khi y√™u c·∫ßu ƒë∆∞·ª£c x·ª≠ l√Ω."
    )

# --- Timezone Helper Functions ---
def get_now_with_tz():
    """Return current datetime with timezone"""
    return datetime.datetime.now(Config.TIMEZONE)

def format_datetime(dt, format='%Y-%m-%d %H:%M:%S'):
    """Format datetime with timezone if needed"""
    if dt is None:
        return get_now_with_tz().strftime(format)
    
    # Check if datetime has timezone info
    if not dt.tzinfo:
        # Add timezone info if missing
        dt = Config.TIMEZONE.localize(dt)
    
    return dt.strftime(format)

def ensure_timezone_aware(dt):
    """ƒê·∫£m b·∫£o datetime object c√≥ timezone tr∆∞·ªõc khi ƒë∆∞a v√†o DB"""
    if dt is None:
        return get_now_with_tz()
    # N·∫øu ƒë√£ c√≥ tzinfo v√† offset, tr·∫£ v·ªÅ nguy√™n b·∫£n
    if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None:
        return dt
    # N·∫øu ch∆∞a c√≥ timezone, th√™m v√†o
    try:
        return Config.TIMEZONE.localize(dt)
    except (ValueError, AttributeError):
        # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá
        logger.warning(f"Kh√¥ng th·ªÉ th√™m timezone cho datetime: {dt}")
        # T·∫°o m·ªõi datetime v·ªõi timezone
        return datetime.datetime(
            dt.year, dt.month, dt.day, 
            dt.hour, dt.minute, dt.second, 
            dt.microsecond, Config.TIMEZONE
        )

async def normalize_title(title):
    """Chu·∫©n h√≥a ti√™u ƒë·ªÅ tin t·ª©c ƒë·ªÉ so s√°nh"""
    if not title:
        return ""
    
    # Remove HTML tags if any
    title = re.sub(r'<[^>]+>', '', title)
    
    # Normalize unicode
    title = unicodedata.normalize('NFD', title)
    title = ''.join([c for c in title if unicodedata.category(c) != 'Mn'])
    
    # Convert to lowercase and remove extra spaces
    title = re.sub(r'\s+', ' ', title.lower().strip())
    
    # Remove special characters
    title = re.sub(r'[^\w\s]', '', title)
    
    return title

async def is_title_sent(normalized_title):
    """Ki·ªÉm tra xem ti√™u ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c g·ª≠i ch∆∞a (d·ª±a tr√™n ti√™u ƒë·ªÅ chu·∫©n h√≥a)"""
    return await redis_client.sismember("sent_titles", normalized_title)

async def mark_title_sent(normalized_title):
    """ƒê√°nh d·∫•u ti√™u ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c g·ª≠i"""
    await redis_client.sadd("sent_titles", normalized_title)
    await redis_client.expire("sent_titles", Config.REDIS_TTL)

# --- News Deduplication by Hash ---
async def is_hash_sent(content_hash):
    return await redis_client.sismember("sent_hashes", content_hash)

async def mark_hash_sent(content_hash):
    await redis_client.sadd("sent_hashes", content_hash)
    await redis_client.expire("sent_hashes", Config.REDIS_TTL)

def is_similar_title(new_title, recent_titles, threshold=None):
    """So s√°nh ti√™u ƒë·ªÅ m·ªõi v·ªõi c√°c ti√™u ƒë·ªÅ c≈©, n·∫øu similarity > threshold th√¨ coi l√† tr√πng"""
    if not recent_titles:
        return False
        
    # S·ª≠ d·ª•ng ng∆∞·ª°ng t·ª´ Config n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if threshold is None:
        threshold = Config.TITLE_SIMILARITY_THRESHOLD
        
    try:
        titles = [new_title] + recent_titles
        vectorizer = TfidfVectorizer().fit_transform(titles)
        vectors = vectorizer.toarray()
        sim_scores = cosine_similarity([vectors[0]], vectors[1:])[0]
        max_sim = max(sim_scores) if len(sim_scores) > 0 else 0
        
        # Ghi log chi ti·∫øt n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh
        if Config.LOG_SIMILARITY_DETAILS and max_sim > 0.6:
            max_idx = sim_scores.argmax() if len(sim_scores) > 0 else -1
            similar_title = recent_titles[max_idx] if max_idx >= 0 else "N/A"
            logger.info(f"Similarity ti√™u ƒë·ªÅ: {max_sim:.2f} (ng∆∞·ª°ng: {threshold:.2f})")
            logger.debug(f"Ti√™u ƒë·ªÅ m·ªõi: {new_title[:30]}... | Ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {similar_title[:30]}...")
            
        return max_sim > threshold
    except Exception as e:
        logger.error(f"L·ªói khi so s√°nh ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {e}")
        return False

# --- News Duplication Detection ---
def is_duplicate_by_content(new_text, recent_texts, threshold=Config.DUPLICATE_THRESHOLD):
    """
    Ph√°t hi·ªán tin tr√πng l·∫∑p b·∫±ng TF-IDF v√† Cosine Similarity
    N√¢ng c·∫•p:
    - S·ª≠ d·ª•ng ngram t·ª´ 1-3 ƒë·ªÉ b·∫Øt c·ª•m t·ª´ d√†i h∆°n
    - Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát
    - T·ªëi ∆∞u vector h√≥a
    - Ph√°t hi·ªán ch√≠nh x√°c h∆°n c√°c tin c√≥ c√πng n·ªôi dung nh∆∞ng kh√°c ngu·ªìn
    """
    if not recent_texts:
        return False
    
    try:
        # Danh s√°ch stopwords ti·∫øng Vi·ªát c∆° b·∫£n
        vn_stopwords = {
            "v√†", "l√†", "c·ªßa", "c√≥", "ƒë∆∞·ª£c", "trong", "cho", "kh√¥ng", "ƒë√£", "v·ªõi", "ƒë∆∞·ª£c", "n√†y",
            "ƒë·∫øn", "t·ª´", "khi", "nh∆∞", "ng∆∞·ªùi", "nh·ªØng", "s·∫Ω", "v√†o", "v·ªÅ", "c√≤n", "b·ªã", "theo",
            "ƒë·ªÉ", "t·∫°i", "nh∆∞ng", "ra", "n√™n", "m·ªôt", "c√°c", "c≈©ng", "ƒëang", "t·ªõi", "tr√™n", "t√¥i",
            "b·∫°n", "ch√∫ng", "r·∫±ng", "th√¨", "ƒë√≥", "l√†m", "n·∫øu", "n√≥i", "b·ªüi", "l√™n", "kh√°c", "h·ªç"
        }
        
        # Th√™m tin m·ªõi v√†o ƒë·∫ßu danh s√°ch ƒë·ªÉ vector h√≥a
        texts = [new_text] + recent_texts
        
        # Ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu v√† gi·ªØ l·∫°i n·ªôi dung quan tr·ªçng
        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ ph√°t hi·ªán tin c√πng n·ªôi dung t·ª´ c√°c ngu·ªìn kh√°c nhau
        processed_texts = []
        for text in texts:
            # Chu·∫©n h√≥a tin, ch·ªâ gi·ªØ t·ª´ kh√≥a ch√≠nh
            words = text.lower().split()
            words = [w for w in words if w not in vn_stopwords and len(w) > 1]
            processed_texts.append(' '.join(words))
        
        # T√≠nh vector TF-IDF, d√πng ngram t·ª´ 1-3 ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c c·ª•m t·ª´ c√≥ √Ω nghƒ©a
        vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words="english", # V·∫´n gi·ªØ cho c√°c t·ª´ ti·∫øng Anh
            ngram_range=(1, 3),   # N√¢ng l√™n (1, 3) ƒë·ªÉ b·∫Øt c·ª•m t·ª´ d√†i h∆°n
            min_df=1,
            max_features=10000    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·ªÉ tƒÉng hi·ªáu su·∫•t
        ).fit_transform(processed_texts)
        
        # Chuy·ªÉn sang m·∫£ng ƒë·ªÉ so s√°nh
        vectors = vectorizer.toarray()
        
        # T√≠nh cosine similarity gi·ªØa tin m·ªõi v√† c√°c tin c≈©
        sim_scores = cosine_similarity([vectors[0]], vectors[1:])[0]
        
        # Ki·ªÉm tra c√≥ tr√πng l·∫∑p kh√¥ng (similarity > threshold)
        max_similarity = max(sim_scores) if len(sim_scores) > 0 else 0
        is_duplicate = max_similarity > threshold
        
        if is_duplicate:
            max_idx = sim_scores.argmax() if len(sim_scores) > 0 else -1
            similar_text = recent_texts[max_idx] if max_idx >= 0 else "N/A"
            logger.info(f"Ph√°t hi·ªán tin tr√πng l·∫∑p! Similarity: {max_similarity:.2f}, Threshold: {threshold}")
            logger.debug(f"Tin m·ªõi: {new_text[:50]}... | Tin tr√πng: {similar_text[:50]}...")
        
        return is_duplicate
    except Exception as e:
        logger.error(f"L·ªói khi ph√°t hi·ªán tin tr√πng l·∫∑p: {e}")
        return False  # N·∫øu l·ªói, coi nh∆∞ kh√¥ng tr√πng ƒë·ªÉ x·ª≠ l√Ω tin

async def init_redis():
    """Initialize Redis connection"""
    global redis_client
    try:
        redis_client = aioredis.from_url(Config.REDIS_URL)
        
        # Load additional keywords from Redis if they exist
        global additional_keywords
        keywords_data = await redis_client.get("additional_keywords")
        if keywords_data:
            additional_keywords = pickle.loads(keywords_data)
            logger.info(f"Loaded {len(additional_keywords)} additional keywords from Redis")
        
        # Load duplicate detection settings
        dup_threshold = await redis_client.get("duplicate_threshold")
        if dup_threshold:
            try:
                Config.DUPLICATE_THRESHOLD = float(dup_threshold.decode())
                logger.info(f"Loaded duplicate threshold: {Config.DUPLICATE_THRESHOLD}")
            except (ValueError, AttributeError) as e:
                logger.warning(f"Failed to load duplicate threshold: {e}")
                
        title_threshold = await redis_client.get("title_threshold")
        if title_threshold:
            try:
                Config.TITLE_SIMILARITY_THRESHOLD = float(title_threshold.decode())
                logger.info(f"Loaded title similarity threshold: {Config.TITLE_SIMILARITY_THRESHOLD}")
            except (ValueError, AttributeError) as e:
                logger.warning(f"Failed to load title threshold: {e}")
                
        log_similarity = await redis_client.get("log_similarity_details")
        if log_similarity:
            try:
                Config.LOG_SIMILARITY_DETAILS = log_similarity.decode().lower() == "true"
                logger.info(f"Loaded similarity logging setting: {Config.LOG_SIMILARITY_DETAILS}")
            except AttributeError as e:
                logger.warning(f"Failed to load similarity logging setting: {e}")
        
        logger.info("Redis initialized successfully")
        return True
    except Exception as e:
        logger.error(f"Error initializing Redis: {e}")
        return False

async def clear_news_queues():
    """X√≥a t·∫•t c·∫£ tin t·ª©c trong queue khi kh·ªüi ƒë·ªông l·∫°i bot ƒë·ªÉ tr√°nh g·ª≠i l·∫°i tin c≈©"""
    try:
        # X√≥a c√°c queue tin t·ª©c
        await redis_client.delete("hot_news_queue")
        await redis_client.delete("news_queue")
        # Gi·ªØ l·∫°i ids ƒë√£ g·ª≠i ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        logger.info("ƒê√£ x√≥a t·∫•t c·∫£ tin trong queue ƒë·ªÉ chu·∫©n b·ªã cho qu√©t m·ªõi")
    except Exception as e:
        logger.error(f"L·ªói khi x√≥a queue tin t·ª©c: {e}")

# --- Helper for rotating RSS feed index ---
async def get_current_feed_index():
    idx = await redis_client.get("current_feed_index")
    if idx is not None:
        return int(idx)
    return 0

async def set_current_feed_index(idx):
    await redis_client.set("current_feed_index", str(idx))

# --- Main Function ---

# Global application variable to store our bot instance
application = None

async def send_message_to_user(user_id, message, entry=None, is_hot_news=False):
    """Send a news message to a user"""
    try:
        # Chu·∫©n b·ªã n·ªôi dung tin nh·∫Øn
        title = getattr(entry, 'title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        link = getattr(entry, 'link', '#')
        # L·∫•y published date v·ªõi x·ª≠ l√Ω timezone
        published = getattr(entry, 'published', None)
        # N·∫øu published l√† string, convert sang datetime
        if isinstance(published, str):
            try:
                published = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %Z')
                # ƒê·∫£m b·∫£o published c√≥ timezone
                published = ensure_timezone_aware(published)
            except ValueError:
                try:
                    # Th·ª≠ v·ªõi format kh√°c (RSS feeds c√≥ th·ªÉ kh√°c nhau)
                    published = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
                except ValueError:
                    # Fallback n·∫øu parse th·∫•t b·∫°i
                    published = None
        # Format date
        date = format_datetime(published) if published else format_datetime(None)
        # Extract domain from link
        domain = urlparse(link).netloc
        # Create message with emoji based on news type
        prefix = "üî• TIN N√ìNG: " if is_hot_news else "üì∞ TIN M·ªöI: "
        # Format message
        formatted_message = (
            f"{prefix}<b>{title}</b>\n\n"
            f"<pre>{message}</pre>\n\n"
            f"<i>Ngu·ªìn: {domain} ‚Ä¢ {date}</i>\n"
            f"<a href='{link}'>ƒê·ªçc chi ti·∫øt</a>"
        )
        # T·∫°o n√∫t ƒë·ªçc chi ti·∫øt
        keyboard = [[InlineKeyboardButton("ƒê·ªçc chi ti·∫øt", url=link)]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        # Get the global application's bot
        global application
        if application and application.bot:
            bot = application.bot
        else:
            from telegram import Bot
            bot = Bot(token=Config.BOT_TOKEN)
        # Lu√¥n g·ª≠i tin nh·∫Øn text, kh√¥ng g·ª≠i ·∫£nh
        await bot.send_message(
            chat_id=user_id,
            text=formatted_message,
            reply_markup=reply_markup,
            parse_mode='HTML',
            disable_web_page_preview=False
        )
    except Exception as e:
        logger.error(f"L·ªói khi g·ª≠i tin t·ª©c cho user {user_id}: {e}")

# --- C√°c job ƒë·ªãnh k·ª≥ m·ªõi ---

async def process_and_send_news(news_data):
    """
    X·ª≠ l√Ω (AI, d·ªãch, ƒë√°nh d·∫•u ƒë√£ g·ª≠i) v√† g·ª≠i tin cho t·∫•t c·∫£ user ƒë√£ duy·ªát. D√πng cho c·∫£ g·ª≠i ƒë·ªãnh k·ª≥ v√† g·ª≠i ngay l·∫≠p t·ª©c.
    """
    try:
        # L·∫•y danh s√°ch ng∆∞·ªùi d√πng ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát
        approved_users_list = [int(uid) for uid in await redis_client.smembers("approved_users")]
        if Config.ADMIN_ID not in approved_users_list:
            approved_users_list.append(Config.ADMIN_ID)
        if not approved_users_list:
            logger.warning("Kh√¥ng c√≥ ng∆∞·ªùi d√πng n√†o ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ g·ª≠i tin.")
            return

        # Ph√¢n t√≠ch tin t·ª©c b·∫±ng AI
        domain = urlparse(news_data['link']).netloc if 'link' in news_data else 'N/A'
        prompt = f"""
        T√≥m t·∫Øt v√† ph√¢n t√≠ch tin t·ª©c sau cho nh√† ƒë·∫ßu t∆∞ ch·ª©ng kho√°n Vi·ªát Nam.
        \nTi√™u ƒë·ªÅ: {news_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}
        T√≥m t·∫Øt: {news_data.get('summary', 'Kh√¥ng c√≥ t√≥m t·∫Øt')}
        Ngu·ªìn: {domain}
        \n1. T√≥m t·∫Øt ng·∫Øn g·ªçn n·ªôi dung 
        2. ƒê√°nh gi√° t√°c ƒë·ªông ( 2-3 c√¢u ). C·∫£m x√∫c (T√≠ch c·ª±c/Ti√™u c·ª±c/Trung l·∫≠p)
        3. M√£/ng√†nh li√™n quan
        """
        try:
            ai_summary = await analyze_news(prompt)
        except Exception as e:
            logger.error(f"L·ªói khi ph√¢n t√≠ch tin b·∫±ng AI: {e}")
            ai_summary = news_data.get('summary', 'Kh√¥ng c√≥ ph√¢n t√≠ch n√†o.')

        # N·∫øu l√† ti·∫øng Anh th√¨ d·ªãch sang ti·∫øng Vi·ªát
        lang = detect_language(news_data.get('title', '') + ' ' + news_data.get('summary', ''))
        if lang == 'en':
            ai_summary = await translate_to_vietnamese(ai_summary)

        # T·∫°o ƒë·ªëi t∆∞·ª£ng entry t·ª´ news_data ƒë·ªÉ truy·ªÅn v√†o h√†m g·ª≠i
        class EntryObject:
            pass
        entry = EntryObject()
        for key, value in news_data.items():
            setattr(entry, key, value)

        # X·ª≠ l√Ω sentiment n·∫øu c·∫ßn
        is_hot = news_data.get('is_hot', False)
        sentiment = await extract_sentiment(ai_summary) if is_hot else 'Trung l·∫≠p'

        # ƒê√°nh d·∫•u tin ƒë√£ ƒë∆∞·ª£c g·ª≠i
        await mark_sent(news_data.get('id', '') or news_data.get('link', ''))

        # G·ª≠i tin cho t·∫•t c·∫£ ng∆∞·ªùi d√πng
        sent_count = 0
        for user_id in approved_users_list:
            try:
                await send_message_to_user(user_id, ai_summary, entry, news_data.get('is_hot', False))
                sent_count += 1
            except Exception as e:
                logger.error(f"L·ªói khi g·ª≠i tin cho user {user_id}: {e}")
        logger.info(f"ƒê√£ g·ª≠i tin '{news_data.get('title', '')[:30]}...' cho {sent_count}/{len(approved_users_list)} ng∆∞·ªùi d√πng.")
    except Exception as e:
        logger.error(f"L·ªói trong process_and_send_news: {e}")

async def fetch_and_cache_news(context: ContextTypes.DEFAULT_TYPE):
    """
    Job ch·∫°y m·ªói 10 ph√∫t ƒë·ªÉ qu√©t 1 RSS, l·ªçc tin v√† ƒë·∫©y v√†o queue (lu√¢n phi√™n t·ª´ng ngu·ªìn).
    N√¢ng c·∫•p: ch·ªâ l·∫•y tin trong ng√†y, l·ªçc tr√πng ti√™u ƒë·ªÅ ƒë√£ g·ª≠i, chu·∫©n h√≥a n·ªôi dung khi so s√°nh tr√πng l·∫∑p, 
    l·ªçc hash n·ªôi dung, l·ªçc ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±, l·ªçc n·ªôi dung t∆∞∆°ng t·ª± t·ª´ c√°c ngu·ªìn kh√°c nhau.
    """
    try:
        logger.info("ƒêang qu√©t 1 RSS v√† cache tin m·ªõi...")
        recent_news_texts_raw = await get_recent_news_texts()
        recent_news_texts = [normalize_text(txt) for txt in recent_news_texts_raw]
        # L·∫•y danh s√°ch ti√™u ƒë·ªÅ chu·∫©n h√≥a g·∫ßn ƒë√¢y (gi·ªõi h·∫°n 200)
        recent_titles = await get_recent_titles(limit=200)
        queued_count = 0
        skipped_count = 0
        duplicate_content_count = 0
        hot_news_count = 0
        feeds = Config.FEED_URLS
        feed_idx = await get_current_feed_index()
        feed_url = feeds[feed_idx % len(feeds)]
        logger.info(f"Qu√©t RSS: {feed_url}")
        entries = await parse_feed(feed_url)
        for entry in entries:
            try:
                if not is_recent_news(entry, days=Config.FETCH_LIMIT_DAYS):
                    continue
                if not is_relevant_news_smart(entry):
                    continue
                entry_id = getattr(entry, 'id', '') or getattr(entry, 'link', '')
                normalized_title = await normalize_title(getattr(entry, 'title', ''))
                # 1. L·ªçc hash n·ªôi dung
                entry_text = f"{getattr(entry, 'title', '')} {getattr(entry, 'summary', '')}"
                entry_text_norm = normalize_text(entry_text)
                content_hash = hashlib.sha256(entry_text_norm.encode('utf-8')).hexdigest()
                if await is_hash_sent(content_hash):
                    skipped_count += 1
                    continue
                # 2. L·ªçc ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±
                if is_similar_title(normalized_title, recent_titles, threshold=Config.TITLE_SIMILARITY_THRESHOLD):
                    skipped_count += 1
                    continue
                # 3. N√¢ng c·∫•p: L·ªçc tin c√≥ n·ªôi dung t∆∞∆°ng t·ª± t·ª´ c√°c ngu·ªìn kh√°c nhau
                prepared_content = normalize_text(f"{getattr(entry, 'title', '')} {getattr(entry, 'summary', '')}")
                if is_duplicate_by_content(prepared_content, recent_news_texts, threshold=Config.DUPLICATE_THRESHOLD):
                    logger.info(f"Ph√°t hi·ªán tin tr√πng l·∫∑p n·ªôi dung t·ª´ ngu·ªìn kh√°c nhau: {getattr(entry, 'title', '')[:50]}...")
                    duplicate_content_count += 1
                    continue
                recent_titles.append(normalized_title)
                # 4. L·ªçc ti√™u ƒë·ªÅ ƒë√£ g·ª≠i tuy·ªát ƒë·ªëi
                if await is_title_sent(normalized_title):
                    skipped_count += 1
                    continue
                if await redis_client.sismember("news_queue_ids", entry_id):
                    skipped_count += 1
                    continue
                if await is_sent(entry_id):
                    skipped_count += 1
                    continue
                # Th√™m v√†o danh s√°ch c√°c n·ªôi dung tin g·∫ßn ƒë√¢y ƒë·ªÉ so s√°nh cho tin sau
                recent_news_texts.append(entry_text_norm)
                # X√°c ƒë·ªãnh lo·∫°i tin & l∆∞u v√†o queue
                is_hot = is_hot_news_simple(entry)
                news_data = {
                    "id": entry_id,
                    "title": getattr(entry, "title", ""),
                    "link": getattr(entry, "link", ""),
                    "summary": getattr(entry, "summary", ""),
                    "published": getattr(entry, "published", ""),
                    "is_hot": is_hot,
                }
                if is_hot:
                    await redis_client.rpush("hot_news_queue", json.dumps(news_data))
                    hot_news_count += 1
                    # G·ª≠i ngay l·∫≠p t·ª©c tin n√≥ng
                    await process_and_send_news(news_data)
                else:
                    await redis_client.rpush("news_queue", json.dumps(news_data))
                await redis_client.sadd("news_queue_ids", entry_id)
                await redis_client.expire("news_queue_ids", Config.REDIS_TTL)
                await mark_title_sent(normalized_title)
                await mark_hash_sent(content_hash)
                queued_count += 1
                await add_recent_title(normalized_title)
                await add_recent_news_text(entry_text_norm)
            except Exception as e:
                logger.warning(f"L·ªói khi x·ª≠ l√Ω tin t·ª´ feed {feed_url}: {e}")
        hot_queue_len = await redis_client.llen("hot_news_queue")
        normal_queue_len = await redis_client.llen("news_queue")
        logger.info(f"Qu√©t RSS ho√†n t·∫•t: ƒê√£ cache {queued_count} tin m·ªõi ({hot_news_count} tin n√≥ng), "
                   f"b·ªè qua {skipped_count} tin tr√πng l·∫∑p th√¥ng th∆∞·ªùng, {duplicate_content_count} tin tr√πng n·ªôi dung. "
                   f"S·ªë tin trong queue: {hot_queue_len} tin n√≥ng, {normal_queue_len} tin th∆∞·ªùng.")
        if queued_count == 0:
            logger.info("Kh√¥ng c√≥ tin m·ªõi, chuy·ªÉn sang feed ti·∫øp theo v√† s·∫Ω th·ª≠ sau 1 ph√∫t.")
            await set_current_feed_index((feed_idx + 1) % len(feeds))
            context.job_queue.run_once(fetch_and_cache_news, 60)
        else:
            await set_current_feed_index((feed_idx + 1) % len(feeds))
    except Exception as e:
        logger.error(f"L·ªói trong job fetch_and_cache_news: {e}")

async def send_news_from_queue(context: ContextTypes.DEFAULT_TYPE):
    """
    Job ch·∫°y m·ªói 800s ƒë·ªÉ l·∫•y 1 tin t·ª´ queue v√† g·ª≠i cho user.
    ∆Øu ti√™n tin n√≥ng tr∆∞·ªõc.
    """
    try:
        # ∆Øu ti√™n l·∫•y tin n√≥ng tr∆∞·ªõc
        news_json = await redis_client.lpop("hot_news_queue")
        if not news_json:
            # N·∫øu kh√¥ng c√≥ tin n√≥ng, l·∫•y tin th∆∞·ªùng
            news_json = await redis_client.lpop("news_queue")
        if not news_json:
            logger.info("Kh√¥ng c√≤n tin trong c·∫£ hai queue. ƒê·ª£i chu k·ª≥ fetch ti·∫øp theo.")
            return
        # Parse JSON th√†nh dict
        news_data = json.loads(news_json)
        await process_and_send_news(news_data)
    except Exception as e:
        logger.error(f"L·ªói trong job send_news_from_queue: {e}")

# H√†m ph√°t hi·ªán ng√¥n ng·ªØ
def detect_language(text):
    try:
        return detect(text)
    except Exception:
        return "unknown"

# H√†m d·ªãch sang ti·∫øng Vi·ªát
async def translate_to_vietnamese(text):
    try:
        translator = Translator()
        # googletrans kh√¥ng async, n√™n d√πng to_thread
        result = await asyncio.to_thread(translator.translate, text, dest='vi')
        return result.text
    except Exception as e:
        logging.error(f"L·ªói khi d·ªãch sang ti·∫øng Vi·ªát: {e}")
        return text

# Function to handle callback queries
async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    # Handle different callback data
    if query.data == "view_keywords":
        # Reuse view_keywords_command logic
        user_id = update.effective_user.id
        
        if not await is_user_approved(user_id):
            await query.message.reply_text(
                "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
            )
            return
        
        global additional_keywords
        default_keywords = Config.RELEVANT_KEYWORDS
        
        message = (
            f"üìã *Danh s√°ch t·ª´ kh√≥a hi·ªán t·∫°i*\n\n"
            f"*T·ª´ kh√≥a m·∫∑c ƒë·ªãnh ({len(default_keywords)})*: Bao g·ªìm c√°c t·ª´ kh√≥a v·ªÅ ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh...\n\n"
        )
        
        if additional_keywords:
            message += f"*T·ª´ kh√≥a b·ªï sung ({len(additional_keywords)})*:\n{', '.join(additional_keywords)}\n\n"
        else:
            message += "*T·ª´ kh√≥a b·ªï sung*: Ch∆∞a c√≥\n\n"
        
        message += (
            "S·ª≠ d·ª•ng /set_keywords ƒë·ªÉ th√™m t·ª´ kh√≥a b·ªï sung.\n"
            "S·ª≠ d·ª•ng /clear_keywords ƒë·ªÉ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung."
        )
        
        await query.message.reply_text(message, parse_mode='Markdown')
    
    elif query.data == "help":
        # Show help message
        await help_command(update, context)
    
    # Handle other callbacks
    elif query.data.startswith("approve_") or query.data.startswith("deny_"):
        await approve_user_callback(update, context)

async def job_ping(context: ContextTypes.DEFAULT_TYPE):
    try:
        url = "https://anvt.onrender.com/health"  # N·∫øu endpoint n√†y kh√¥ng c√≥, ƒë·ªïi th√†nh "/"
        async with httpx.AsyncClient(timeout=10) as client:
            await client.get(url)
        logger.info(f"Ping gi·ªØ bot awake t·ªõi {url}")
    except Exception as e:
        logger.error(f"L·ªói khi ping gi·ªØ awake: {e}")

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    await update.message.reply_text(
        f"üëã Xin ch√†o {user.first_name or 'b·∫°n'}! ƒê√¢y l√† bot t·ªïng h·ª£p tin t·ª©c ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh.\n"
        "G√µ /help ƒë·ªÉ xem h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    is_admin = user_id == Config.ADMIN_ID
    
    basic_commands = (
        "C√°c l·ªánh h·ªó tr·ª£:\n"
        "/start - B·∫Øt ƒë·∫ßu\n"
        "/help - H∆∞·ªõng d·∫´n\n"
        "/register - ƒêƒÉng k√Ω s·ª≠ d·ª•ng bot\n"
        "/keywords - Xem t·ª´ kh√≥a l·ªçc tin\n"
        "/set_keywords - Th√™m t·ª´ kh√≥a l·ªçc tin\n"
        "/clear_keywords - X√≥a t·ª´ kh√≥a b·ªï sung"
    )
    
    if is_admin:
        admin_commands = (
            "\n\nL·ªánh d√†nh ri√™ng cho admin:\n"
            "/check_dup_settings - Ki·ªÉm tra c√†i ƒë·∫∑t l·ªçc tin tr√πng\n"
            "/set_dup_threshold - ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán n·ªôi dung tr√πng\n"
            "/set_title_threshold - ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±\n"
            "/toggle_sim_log - B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity"
        )
        await update.message.reply_text(basic_commands + admin_commands)
    else:
        await update.message.reply_text(basic_commands)

async def view_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    default_keywords = Config.RELEVANT_KEYWORDS

    message = (
        f"üìã *Danh s√°ch t·ª´ kh√≥a hi·ªán t·∫°i*\n\n"
        f"*T·ª´ kh√≥a m·∫∑c ƒë·ªãnh ({len(default_keywords)})*: Bao g·ªìm c√°c t·ª´ kh√≥a v·ªÅ ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh...\n\n"
    )

    if additional_keywords:
        message += f"*T·ª´ kh√≥a b·ªï sung ({len(additional_keywords)})*:\n{', '.join(additional_keywords)}\n\n"
    else:
        message += "*T·ª´ kh√≥a b·ªï sung*: Ch∆∞a c√≥\n\n"

    message += (
        "S·ª≠ d·ª•ng /set_keywords ƒë·ªÉ th√™m t·ª´ kh√≥a b·ªï sung.\n"
        "S·ª≠ d·ª•ng /clear_keywords ƒë·ªÉ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung."
    )

    await update.message.reply_text(message, parse_mode='Markdown')

async def set_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    if not context.args:
        await update.message.reply_text(
            "‚úèÔ∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a b·∫°n mu·ªën th√™m. V√≠ d·ª•: `/set_keywords bitcoin, eth`"
        )
        return

    new_keywords = [normalize_text(kw.strip()) for kw in ' '.join(context.args).split(',') if kw.strip()]
    
    if not new_keywords:
        await update.message.reply_text("‚ö†Ô∏è Kh√¥ng c√≥ t·ª´ kh√≥a h·ª£p l·ªá n√†o ƒë∆∞·ª£c cung c·∫•p.")
        return

    added_count = 0
    for kw in new_keywords:
        if kw not in additional_keywords:
            additional_keywords.append(kw)
            added_count += 1
    
    if added_count > 0:
        # Save to Redis
        await redis_client.set("additional_keywords", pickle.dumps(additional_keywords))
        await update.message.reply_text(
            f"‚úÖ ƒê√£ th√™m {added_count} t·ª´ kh√≥a m·ªõi. Hi·ªán c√≥ {len(additional_keywords)} t·ª´ kh√≥a b·ªï sung.\n"
            "G√µ /keywords ƒë·ªÉ xem danh s√°ch ƒë·∫ßy ƒë·ªß."
        )
    else:
        await update.message.reply_text(
            "‚ÑπÔ∏è C√°c t·ª´ kh√≥a b·∫°n nh·∫≠p ƒë√£ c√≥ s·∫µn ho·∫∑c kh√¥ng h·ª£p l·ªá."
        )

async def clear_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    if not additional_keywords:
        await update.message.reply_text("‚ÑπÔ∏è Hi·ªán kh√¥ng c√≥ t·ª´ kh√≥a b·ªï sung n√†o ƒë·ªÉ x√≥a.")
        return
        
    additional_keywords.clear()
    # Save to Redis
    await redis_client.delete("additional_keywords")
    await update.message.reply_text(
        "üóëÔ∏è ƒê√£ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung. Bot s·∫Ω ch·ªâ s·ª≠ d·ª•ng danh s√°ch t·ª´ kh√≥a m·∫∑c ƒë·ªãnh.\n"
        "G√µ /keywords ƒë·ªÉ xem l·∫°i."
    )

@admin_only
async def set_duplicate_threshold(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to set the duplicate detection threshold without restarting the bot
    Usage: /set_dup_threshold 0.65
    """
    if not context.args or len(context.args) != 1:
        await update.message.reply_text(
            "‚ùå C√∫ ph√°p sai. S·ª≠ d·ª•ng: `/set_dup_threshold 0.65`\n"
            "Gi√° tr·ªã t·ª´ 0.0 ƒë·∫øn 1.0, c√†ng th·∫•p c√†ng nh·∫°y v·ªõi vi·ªác ph√°t hi·ªán tr√πng l·∫∑p."
        )
        return
        
    try:
        threshold = float(context.args[0])
        if threshold < 0.0 or threshold > 1.0:
            await update.message.reply_text("‚ùå Gi√° tr·ªã ph·∫£i t·ª´ 0.0 ƒë·∫øn 1.0")
            return
            
        # C·∫≠p nh·∫≠t gi√° tr·ªã trong Config
        Config.DUPLICATE_THRESHOLD = threshold
        
        # L∆∞u v√†o Redis ƒë·ªÉ gi·ªØ gi√° tr·ªã khi kh·ªüi ƒë·ªông l·∫°i
        await redis_client.set("duplicate_threshold", str(threshold))
        
        await update.message.reply_text(
            f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t ng∆∞·ª°ng ph√°t hi·ªán tin tr√πng l·∫∑p n·ªôi dung: {threshold}\n"
            f"√Åp d·ª•ng ngay cho c√°c l·∫ßn qu√©t RSS ti·∫øp theo."
        )
    except ValueError:
        await update.message.reply_text("‚ùå Gi√° tr·ªã kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p s·ªë t·ª´ 0.0 ƒë·∫øn 1.0")

@admin_only
async def toggle_similarity_logging(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to toggle detailed similarity logging
    Usage: /toggle_sim_log
    """
    # Toggle gi√° tr·ªã
    Config.LOG_SIMILARITY_DETAILS = not Config.LOG_SIMILARITY_DETAILS
    
    # L∆∞u v√†o Redis
    await redis_client.set("log_similarity_details", str(Config.LOG_SIMILARITY_DETAILS).lower())
    
    status = "b·∫≠t" if Config.LOG_SIMILARITY_DETAILS else "t·∫Øt"
    await update.message.reply_text(
        f"‚úÖ ƒê√£ {status} ghi log chi ti·∫øt v·ªÅ ph√°t hi·ªán tin tr√πng l·∫∑p.\n"
        "Xem log h·ªá th·ªëng ƒë·ªÉ theo d√µi th√¥ng tin chi ti·∫øt v·ªÅ similarity."
    )

@admin_only
async def check_dup_settings(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to check current duplicate detection settings
    Usage: /check_dup_settings
    """
    settings = (
        f"üìä *C√†i ƒë·∫∑t ph√°t hi·ªán tin tr√πng l·∫∑p*\n\n"
        f"‚Ä¢ Ng∆∞·ª°ng tin tr√πng n·ªôi dung (DUPLICATE_THRESHOLD): {Config.DUPLICATE_THRESHOLD}\n"
        f"‚Ä¢ Ng∆∞·ª°ng ti√™u ƒë·ªÅ t∆∞∆°ng t·ª± (TITLE_SIMILARITY_THRESHOLD): {Config.TITLE_SIMILARITY_THRESHOLD}\n"
        f"‚Ä¢ Ghi log chi ti·∫øt: {'B·∫≠t' if Config.LOG_SIMILARITY_DETAILS else 'T·∫Øt'}\n\n"
        f"L·ªánh ƒëi·ªÅu ch·ªânh:\n"
        f"‚Ä¢ /set_dup_threshold [0.0-1.0] - ƒê·∫∑t ng∆∞·ª°ng ph√°t hi·ªán n·ªôi dung tr√πng l·∫∑p\n"
        f"‚Ä¢ /set_title_threshold [0.0-1.0] - ƒê·∫∑t ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±\n"
        f"‚Ä¢ /toggle_sim_log - B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity"
    )
    await update.message.reply_text(settings, parse_mode='Markdown')

@admin_only
async def set_title_threshold(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to set the title similarity threshold
    Usage: /set_title_threshold 0.92
    """
    if not context.args or len(context.args) != 1:
        await update.message.reply_text(
            "‚ùå C√∫ ph√°p sai. S·ª≠ d·ª•ng: `/set_title_threshold 0.92`\n"
            "Gi√° tr·ªã t·ª´ 0.0 ƒë·∫øn 1.0, c√†ng cao c√†ng y√™u c·∫ßu ti√™u ƒë·ªÅ gi·ªëng nhau m·ªõi coi l√† tr√πng."
        )
        return
        
    try:
        threshold = float(context.args[0])
        if threshold < 0.0 or threshold > 1.0:
            await update.message.reply_text("‚ùå Gi√° tr·ªã ph·∫£i t·ª´ 0.0 ƒë·∫øn 1.0")
            return
            
        # C·∫≠p nh·∫≠t gi√° tr·ªã trong Config
        Config.TITLE_SIMILARITY_THRESHOLD = threshold
        
        # L∆∞u v√†o Redis ƒë·ªÉ gi·ªØ gi√° tr·ªã khi kh·ªüi ƒë·ªông l·∫°i
        await redis_client.set("title_threshold", str(threshold))
        
        await update.message.reply_text(
            f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {threshold}\n"
            f"√Åp d·ª•ng ngay cho c√°c l·∫ßn qu√©t RSS ti·∫øp theo."
        )
    except ValueError:
        await update.message.reply_text("‚ùå Gi√° tr·ªã kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p s·ªë t·ª´ 0.0 ƒë·∫øn 1.0")

def main():
    global application, shutdown_flag
    
    # Reset shutdown flag
    shutdown_flag = False
    
    # Thi·∫øt l·∫≠p signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        application = Application.builder().token(Config.BOT_TOKEN).build()

        # Initialize database and Redis
        loop = asyncio.get_event_loop()
        redis_ok = loop.run_until_complete(init_redis())
        
        # T·∫£i c√†i ƒë·∫∑t ng∆∞·ª°ng t·ª´ Redis n·∫øu c√≥
        if redis_ok:
            dup_threshold = loop.run_until_complete(redis_client.get("duplicate_threshold"))
            if dup_threshold:
                try:
                    Config.DUPLICATE_THRESHOLD = float(dup_threshold.decode())
                except (ValueError, AttributeError):
                    pass
                    
            title_threshold = loop.run_until_complete(redis_client.get("title_threshold"))
            if title_threshold:
                try:
                    Config.TITLE_SIMILARITY_THRESHOLD = float(title_threshold.decode())
                except (ValueError, AttributeError):
                    pass
                    
            log_similarity = loop.run_until_complete(redis_client.get("log_similarity_details"))
            if log_similarity:
                try:
                    Config.LOG_SIMILARITY_DETAILS = log_similarity.decode().lower() == "true"
                except (AttributeError):
                    pass
                    
            logger.info(f"C√†i ƒë·∫∑t l·ªçc tin tr√πng: DUPLICATE_THRESHOLD={Config.DUPLICATE_THRESHOLD}, " 
                      f"TITLE_SIMILARITY_THRESHOLD={Config.TITLE_SIMILARITY_THRESHOLD}, "
                      f"LOG_SIMILARITY_DETAILS={Config.LOG_SIMILARITY_DETAILS}")

        if not redis_ok:
            logger.error("Failed to initialize Redis. Exiting.")
            return

        # X√≥a queue c≈© khi kh·ªüi ƒë·ªông l·∫°i
        loop.run_until_complete(clear_news_queues())

        # Add command handlers
        application.add_handler(CommandHandler("start", start_command))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("register", register_user))
        application.add_handler(CommandHandler("keywords", view_keywords_command))
        application.add_handler(CommandHandler("set_keywords", set_keywords_command))
        application.add_handler(CommandHandler("clear_keywords", clear_keywords_command))
        
        # Th√™m l·ªánh qu·∫£n l√Ω c√†i ƒë·∫∑t l·ªçc tin tr√πng
        application.add_handler(CommandHandler("check_dup_settings", check_dup_settings))
        application.add_handler(CommandHandler("set_dup_threshold", set_duplicate_threshold))
        application.add_handler(CommandHandler("set_title_threshold", set_title_threshold))
        application.add_handler(CommandHandler("toggle_sim_log", toggle_similarity_logging))

        # Add callback query handler
        application.add_handler(CallbackQueryHandler(button_callback))

        # Set up the job queue
        job_queue = application.job_queue
        
        # C·∫•u h√¨nh c√°c job ƒë·ªãnh k·ª≥ m·ªõi:
        # 1. Job qu√©t RSS m·ªói gi·ªù v√† cache tin
        job_queue.run_repeating(fetch_and_cache_news, interval=Config.HOURLY_JOB_INTERVAL, first=10)
        
        # 2. Job g·ª≠i tin t·ª´ queue m·ªói 800s
        job_queue.run_repeating(send_news_from_queue, interval=Config.NEWS_JOB_INTERVAL, first=30)
        
        # 3. Job ping gi·ªØ awake m·ªói 5 ph√∫t
        job_queue.run_repeating(job_ping, interval=300, first=60)
        
        # In th√¥ng tin job
        logger.info(f"ƒê√£ thi·∫øt l·∫≠p 3 job ƒë·ªãnh k·ª≥:\n"
                    f"- Qu√©t RSS & cache: {Config.HOURLY_JOB_INTERVAL}s/l·∫ßn\n"
                    f"- G·ª≠i tin t·ª´ queue: {Config.NEWS_JOB_INTERVAL}s/l·∫ßn\n"
                    f"- Ping gi·ªØ awake: 300s/l·∫ßn")

        if Config.WEBHOOK_URL:
            webhook_port = int(os.environ.get("PORT", 8443))
            logger.info(f"Starting webhook on port {webhook_port} with URL: {Config.WEBHOOK_URL}")
            application.run_webhook(
                listen="0.0.0.0",
                port=webhook_port,
                url_path=Config.BOT_TOKEN,
                webhook_url=f"{Config.WEBHOOK_URL}/{Config.BOT_TOKEN}"
            )
        else:
            logger.info("Starting bot in polling mode")
            application.run_polling(allowed_updates=Update.ALL_TYPES)
            
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c trong h√†m main: {e}")
        # D·ªçn d·∫πp t√†i nguy√™n khi c√≥ l·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(cleanup_resources())
        else:
            loop.run_until_complete(cleanup_resources())
        sys.exit(1)

if __name__ == "__main__":
    main()
