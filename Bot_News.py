import logging
import os
import asyncio
# Nh√≥m c√°c import th∆∞ vi·ªán b√™n ngo√†i
import feedparser
import httpx
import redis.asyncio as aioredis
import google.generativeai as genai
# Nh√≥m import telegram
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton, InputMediaPhoto, InputMediaVideo
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes, MessageHandler, filters
# Nh√≥m c√°c import kh√°c
import re
from urllib.parse import urlparse
import unicodedata
import datetime
import pytz
from typing import Dict, List, Any, Optional
import pickle
from functools import wraps
# Import cho vi·ªác ph√°t hi·ªán tin tr√πng l·∫∑p
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Import cho sentiment analysis ti·∫øng Vi·ªát
import numpy as np
import requests
import json
# Import th√™m signal ƒë·ªÉ x·ª≠ l√Ω t√≠n hi·ªáu ƒë√≥ng/kh·ªüi ƒë·ªông l·∫°i
import signal
import sys
import hashlib
# Th√™m import cho ph√°t hi·ªán ng√¥n ng·ªØ v√† d·ªãch
from langdetect import detect
from deep_translator import GoogleTranslator

# --- 1. Config & setup ---
class Config:
    BOT_TOKEN = os.getenv("BOT_TOKEN")
    WEBHOOK_URL = os.getenv("WEBHOOK_URL")
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost")
    DB_URL = os.getenv("DATABASE_URL")
    ADMIN_ID = int(os.getenv("ADMIN_ID", "1225226589"))
    GOOGLE_GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-preview-05-20")
    OPENROUTER_FALLBACK_MODEL = os.getenv("OPENROUTER_FALLBACK_MODEL", "deepseek/deepseek-chat-v3-0324:free")
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    GROQ_MODEL = os.getenv("GROQ_MODEL", "deepseek-r1-distill-llama-70b")
    FEED_URLS = [
        # Google News theo t·ª´ kh√≥a (gi·ªØ nguy√™n c√°c ngu·ªìn Vi·ªát Nam)
        "https://news.google.com/rss/search?q=kinh+t%E1%BA%BF&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch%E1%BB%A9ng+kho%C3%A1n&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=v%C4%A9+m%C3%B4&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=chi%E1%BA%BFn+tranh&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=l%C3%A3i+su%E1%BA%A5t&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=fed&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=tin+n%C3%B3ng&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=%C4%91%E1%BA%A7u+t%C6%B0&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=doanh+nghi%E1%BB%87p&hl=vi&gl=VN&ceid=VN:vi",
        # Ch√≠nh tr·ªã th·∫ø gi·ªõi, quan h·ªá qu·ªëc t·∫ø
        "https://news.google.com/rss/search?q=ch%C3%ADnh+tr%E1%BB%8B+th%E1%BA%BF+gi%E1%BB%9Bi&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=geopolitics&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=world+politics&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=international+relations&hl=vi&gl=VN&ceid=VN:vi",
        # Qu·ªëc t·∫ø (Google News search c√°c ngu·ªìn qu·ªëc t·∫ø)
        "https://news.google.com/rss/search?q=site:bloomberg.com+stock+OR+market+OR+finance&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:ft.com+stock+OR+market+OR+finance&hl=vi&gl=VN&ceid=VN:vi",
        # B·ªï sung c√°c ngu·ªìn Google News RSS t·ªëi ∆∞u
        "https://news.google.com/rss/search?q=ch·ª©ng+kho√°n+site:cafef.vn&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch·ª©ng+kho√°n+site:vnexpress.net&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch√≠nh+s√°ch+vƒ©+m√¥&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=l√£i+su·∫•t+site:sbv.gov.vn&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=market+crash&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:reuters.com+economy+OR+policy&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=site:theguardian.com+world+OR+politics&hl=vi&gl=VN&ceid=VN:vi",
        # --- C√°c ngu·ªìn qu·ªëc t·∫ø ·ªïn ƒë·ªãnh ---
        "https://feeds.reuters.com/reuters/topNews",
        "https://feeds.marketwatch.com/marketwatch/topstories/",
        "https://finance.yahoo.com/news/rssindex",
        "https://www.bloomberg.com/feed/podcast/etf-report.xml",
    ]
    REDIS_TTL = int(os.getenv("REDIS_TTL", "60000"))  # 6h
    NEWS_JOB_INTERVAL = int(os.getenv("NEWS_JOB_INTERVAL", "800"))
    HOURLY_JOB_INTERVAL = int(os.getenv("HOURLY_JOB_INTERVAL", "500"))  # ... ph√∫t/l·∫ßn
    FETCH_LIMIT_DAYS = int(os.getenv("FETCH_LIMIT_DAYS", "1"))  # Ch·ªâ l·∫•y tin 1 ng√†y g·∫ßn nh·∫•t 
    MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))  # S·ªë l·∫ßn th·ª≠ l·∫°i khi feed l·ªói
    MAX_NEWS_PER_CYCLE = int(os.getenv("MAX_NEWS_PER_CYCLE", "1"))  # T·ªëi ƒëa 1 tin m·ªói l·∫ßn
    TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')  # Timezone chu·∫©n cho Vi·ªát Nam
    
    # C·∫•u h√¨nh ph√°t hi·ªán tin tr√πng l·∫∑p - n√¢ng c·∫•p
    DUPLICATE_THRESHOLD = float(os.getenv("DUPLICATE_THRESHOLD", "0.65"))  # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán tr√πng l·∫∑p n·ªôi dung
    TITLE_SIMILARITY_THRESHOLD = float(os.getenv("TITLE_SIMILARITY_THRESHOLD", "0.92"))  # Ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±
    LOG_SIMILARITY_DETAILS = os.getenv("LOG_SIMILARITY_DETAILS", "False").lower() == "true"  # B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity
    
    RECENT_NEWS_DAYS = int(os.getenv("RECENT_NEWS_DAYS", "2"))  # S·ªë ng√†y ƒë·ªÉ l·∫•y tin g·∫ßn ƒë√¢y ƒë·ªÉ so s√°nh
    
    # C·∫•u h√¨nh ph√°t hi·ªán tin n√≥ng
    HOT_NEWS_KEYWORDS = [
        "kh·∫©n c·∫•p", "tin n√≥ng", "breaking", "kh·ªßng ho·∫£ng", "crash", "s·∫≠p", "b√πng n·ªï", "tin nhanh ch·ª©ng kho√°n", "tr∆∞·ªõc gi·ªù giao d·ªãch", 
        "shock", "·∫£nh h∆∞·ªüng l·ªõn", "th·∫£m kh·ªëc", "th·∫£m h·ªça", "market crash", "sell off", "VNINDEX", "vnindex", "Trump", "fed", "FED",
        "r∆°i m·∫°nh", "tƒÉng m·∫°nh", "gi·∫£m m·∫°nh", "s·ª•p ƒë·ªï", "b·∫•t th∆∞·ªùng", "emergency", "ch·ª©ng kho√°n",
        "urgent", "alert", "c·∫£nh b√°o", "ƒë·ªôt bi·∫øn", "l·ªãch s·ª≠", "k·ª∑ l·ª•c", "cao nh·∫•t"
    ]
    HOT_NEWS_IMPACT_PHRASES = [
        "t√°c ƒë·ªông m·∫°nh", "·∫£nh h∆∞·ªüng nghi√™m tr·ªçng", "thay ƒë·ªïi l·ªõn", "bi·∫øn ƒë·ªông m·∫°nh",
        "tr·ªçng ƒëi·ªÉm", "quan tr·ªçng", "ƒë√°ng ch√∫ √Ω", "ƒë√°ng lo ng·∫°i", "c·∫ßn l∆∞u √Ω"
    ]
    
    # Danh s√°ch t·ª´ kh√≥a l·ªçc tin t·ª©c li√™n quan (m·ªü r·ªông)
    RELEVANT_KEYWORDS = [
        # Ch√≠nh tr·ªã, vƒ© m√¥, doanh nghi·ªáp, ch·ª©ng kho√°n, chi·∫øn tranh 
        "ch√≠nh tr·ªã", "vƒ© m√¥", "doanh nghi·ªáp", "ch·ª©ng kho√°n", "chi·∫øn tranh", "ch√≠nh s√°ch", "l√£i su·∫•t", "fed",
        "phe", "ƒë·∫£ng", "ch√≠nh ph·ªß", "qu·ªëc h·ªôi", "nh√† n∆∞·ªõc", "b·ªô tr∆∞·ªüng", "th·ªß t∆∞·ªõng", "ch·ªß t·ªãch",
        # Nh√≥m ng√†nh, bluechip, midcap, th·ªã tr∆∞·ªùng
        "bluechip", "midcap", "ng√¢n h√†ng", "b·∫•t ƒë·ªông s·∫£n", "th√©p", "d·∫ßu kh√≠", "c√¥ng ngh·ªá", "b√°n l·∫ª",
        "xu·∫•t kh·∫©u", "ƒëi·ªán", "x√¢y d·ª±ng", "th·ªßy s·∫£n", "d∆∞·ª£c ph·∫©m", "logistics", "v·∫≠n t·∫£i", 
        # C√°c m√£ ch·ª©ng kho√°n, ch·ªâ s·ªë
        "vn30", "hnx", "upcom", "vnindex", "c·ªï phi·∫øu", "th·ªã tr∆∞·ªùng", "t√†i ch√≠nh", "kinh t·∫ø", 
        "gdp", "l·∫°m ph√°t", "t√≠n d·ª•ng", "tr√°i phi·∫øu", "ph√°i sinh", "qu·ªπ etf", 
        # C√°c m√£ bluechip VN30
        "fpt", "vnm", "vcb", "ssi", "msn", "mwg", "vic", "vhm", "hpg", "ctg", "bid", "mbb", "stb",
        "hdb", "bvh", "vpb", "nvl", "pdr", "tcb", "tpb", "bcm", "pnj", "acb", "vib", "plx",
        # C√°c m√£ midcap, c√°c ch·ªâ b√°o kinh t·∫ø
        "vnm", "cpi", "pmi", "m2", "ƒë·∫ßu t∆∞", "gdp", "xu·∫•t kh·∫©u", "nh·∫≠p kh·∫©u", "d·ª± tr·ªØ", "d·ª± b√°o",
        # T·ª´ kh√≥a t√†i ch√≠nh qu·ªëc t·∫ø
        "fed", "ecb", "boj", "pboc", "imf", "world bank", "nasdaq", "dow jones", "s&p", "nikkei",
        "treasury", "usd", "eur", "jpy", "cny", "bitcoin", "crypto", "commodities", "wti", "brent",
        # M√£ c·ªï phi·∫øu n·ªïi b·∫≠t
        "vnd", "ssi", "hpg", "vic", "vhm", "vnm", "mwg", "ctg", "bid", "tcb", "acb", "vib", "stb", "mbb", "shb",
        # T√™n c√¥ng ty l·ªõn
        "vinamilk", "vietcombank", "vietinbank", "masan", "fpt", "hoa phat", "vietjet", "petro vietnam",
        # S·ª± ki·ªán kinh t·∫ø
        "l·∫°m ph√°t", "tƒÉng tr∆∞·ªüng", "gi·∫£m ph√°t", "GDP", "CPI", "PMI", "xu·∫•t kh·∫©u", "nh·∫≠p kh·∫©u", "t√≠n d·ª•ng", "tr√°i phi·∫øu",
        # Thu·∫≠t ng·ªØ th·ªã tr∆∞·ªùng
        "bull", "bear", "breakout", "margin", "room ngo·∫°i", "ETF", "IPO", "ni√™m y·∫øt", "ph√°t h√†nh", "c·ªï t·ª©c", "chia th∆∞·ªüng",
        # S·ª± ki·ªán qu·ªëc t·∫ø
        "fed", "ecb", "boj", "nasdaq", "dow jones", "s&p", "nikkei", "usd", "eur", "jpy", "bitcoin", "crypto",
        # --- B·ªï sung t·ª´ kh√≥a ti·∫øng Anh v·ªÅ kinh t·∫ø, ƒë·ªãa ch√≠nh tr·ªã ---
        "economy", "macroeconomics", "geopolitics", "geopolitical", "inflation", "interest rate", "recession", "debt ceiling", "federal reserve", "central bank", "monetary policy", "fiscal policy", "trade war", "sanctions", "supply chain", "emerging market", "developed market", "stock market", "bond market", "currency", "exchange rate", "usd", "eur", "cny", "jpy", "oil price", "energy crisis", "commodity", "gold", "crude oil", "brent", "wti", "opec", "gdp", "cpi", "pmi", "unemployment", "jobless", "stimulus", "bailout", "default", "bankruptcy", "sovereign debt", "credit rating", "imf", "world bank", "g20", "g7", "us-china", "us-eu", "russia", "ukraine", "middle east", "conflict", "war", "sanction", "tariff", "trade agreement", "globalization", "deglobalization", "supply disruption", "food crisis", "migration", "refugee", "political risk", "regime change", "election", "summit", "treaty", "alliance", "nato", "united nations", "eu", "china", "us", "usa", "europe", "asia", "africa", "latin america", "brics", "asean", "indo-pacific", "south china sea", "taiwan strait", "north korea", "iran", "israel", "palestine", "syria", "yemen", "afghanistan", "terrorism", "cybersecurity", "espionage", "intelligence", "military", "defense", "nuclear", "missile", "sanction", "diplomacy", "summit", "treaty"
    ]

# Danh s√°ch t·ª´ kh√≥a b·ªï sung
additional_keywords = []

# Danh s√°ch t·ª´ kh√≥a ƒë·ªÉ lo·∫°i tr·ª´ tin kh√¥ng li√™n quan
EXCLUDE_KEYWORDS = [
    "khuy·∫øn m√£i", "gi·∫£m gi√°", "mua ngay", "tuy·ªÉn d·ª•ng", "s·ª± ki·ªán", "gi·∫£i tr√≠", "th·ªÉ thao", "lifestyle", 
    "du l·ªãch", "·∫©m th·ª±c", "hot deal", "sale off", "qu·∫£ng c√°o", "ƒë·∫∑t h√†ng", "shoppe", "tiki", "lazada"
]

# --- Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng b·∫Øt bu·ªôc ---
REQUIRED_ENV_VARS = ["BOT_TOKEN", "OPENROUTER_API_KEY"]
for var in REQUIRED_ENV_VARS:
    if not os.getenv(var):
        raise RuntimeError(f"Missing required environment variable: {var}")

# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Redis ---
redis_client = None

# L∆∞u tr·ªØ danh s√°ch user ƒë√£ ƒë∆∞·ª£c duy·ªát
approved_users = set()

# --- Flag ƒë·ªÉ ki·ªÉm so√°t vi·ªác t·∫Øt bot ---
shutdown_flag = False

# --- Cleanup function ---
async def cleanup_resources():
    """D·ªçn d·∫πp t√†i nguy√™n khi bot t·∫Øt"""
    global redis_client, application
    
    logger.info("ƒêang d·ªçn d·∫πp t√†i nguy√™n tr∆∞·ªõc khi t·∫Øt...")
    
    # D·ª´ng job queue n·∫øu ƒëang ch·∫°y
    if application and application.job_queue:
        logger.info("D·ª´ng job queue...")
        await application.job_queue.stop()
    
    # ƒê√≥ng k·∫øt n·ªëi Redis
    if redis_client:
        logger.info("ƒê√≥ng k·∫øt n·ªëi Redis...")
        await redis_client.close()
        redis_client = None
    
    logger.info("ƒê√£ d·ªçn d·∫πp t·∫•t c·∫£ t√†i nguy√™n.")

# --- Signal handlers ---
def signal_handler(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu t·∫Øt t·ª´ h·ªá th·ªëng"""
    global shutdown_flag
    
    if shutdown_flag:
        logger.warning("Nh·∫≠n t√≠n hi·ªáu t·∫Øt l·∫ßn hai, t·∫Øt ngay l·∫≠p t·ª©c!")
        sys.exit(1)
    
    logger.info(f"Nh·∫≠n t√≠n hi·ªáu {sig}, chu·∫©n b·ªã t·∫Øt bot...")
    shutdown_flag = True
    
    # Ch·∫°y cleanup trong asyncio event loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(shutdown())
        else:
            loop.run_until_complete(shutdown())
    except Exception as e:
        logger.error(f"L·ªói khi d·ªçn d·∫πp t√†i nguy√™n: {e}")
        sys.exit(1)

async def shutdown():
    """X·ª≠ l√Ω shutdown m·ªôt c√°ch ƒë·ªìng b·ªô"""
    global application
    
    logger.info("B·∫Øt ƒë·∫ßu quy tr√¨nh shutdown...")
    
    # D·ªçn d·∫πp t√†i nguy√™n
    await cleanup_resources()
    
    # D·ª´ng bot n·∫øu ƒëang ch·∫°y
    if application:
        logger.info("D·ª´ng bot...")
        if hasattr(application, 'stop'):
            await application.stop()
        application = None
    
    logger.info("Bot ƒë√£ t·∫Øt ho√†n to√†n.")
    sys.exit(0)

async def is_sent(entry_id):
    return await redis_client.sismember("sent_news", entry_id)

async def mark_sent(entry_id):
    await redis_client.sadd("sent_news", entry_id)
    await redis_client.expire("sent_news", Config.REDIS_TTL)

# --- AI Analysis (Gemini) ---
GEMINI_MODEL = Config.GEMINI_MODEL
OPENROUTER_FALLBACK_MODEL = Config.OPENROUTER_FALLBACK_MODEL
GOOGLE_GEMINI_API_KEY = Config.GOOGLE_GEMINI_API_KEY

async def call_groq_api(prompt, model=None):
    """G·ªçi Groq API ƒë·ªÉ l·∫•y k·∫øt qu·∫£ AI ph√¢n t√≠ch"""
    api_key = Config.GROQ_API_KEY
    model = model or Config.GROQ_MODEL
    if not api_key:
        raise RuntimeError("GROQ_API_KEY is not set")
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7
                }
            )
            result = response.json()
            if "choices" not in result:
                logging.error(f"Groq API error (model={model}): {result}")
                raise RuntimeError(f"Groq API error: {result}")
            return result["choices"][0]["message"]["content"]
    except Exception as e:
        logging.error(f"Groq API l·ªói: {e}")
        raise e

async def analyze_news(prompt, model=None):
    try:
        # G·ªçi Google Gemini API ch√≠nh th·ª©c
        genai.configure(api_key=GOOGLE_GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash-preview-04-17")
        response = await asyncio.to_thread(model.generate_content, prompt)
        return response.text
    except Exception as e:
        logging.error(f"Gemini API l·ªói: {e}, fallback sang OpenRouter {OPENROUTER_FALLBACK_MODEL}")
        # Fallback sang OpenRouter
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}",},
                    json={
                        "model": OPENROUTER_FALLBACK_MODEL,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7
                    }
                )
                result = response.json()
                if "choices" not in result:
                    logging.error(f"OpenRouter API error (model={OPENROUTER_FALLBACK_MODEL}): {result}")
                    raise RuntimeError(f"OpenRouter API error: {result}")
                return result["choices"][0]["message"]["content"]
        except Exception as e2:
            logging.error(f"OpenRouter fallback c≈©ng l·ªói: {e2}, th·ª≠ ti·∫øp Groq...")
            # Fallback sang Groq
            try:
                return await call_groq_api(prompt)
            except Exception as e3:
                logging.error(f"Groq fallback c≈©ng l·ªói: {e3}")
                raise e3

# --- Extract sentiment from AI result ---
def extract_sentiment_rule_based(ai_summary):
    """Extract sentiment from AI summary using rule-based approach"""
    sentiment = "Trung l·∫≠p"  # Default
    try:
        # T√¨m ki·∫øm d√≤ng c√≥ ch·ª©a 'c·∫£m x√∫c'
        for line in ai_summary.splitlines():
            line_lower = line.lower()
            if "c·∫£m x√∫c:" in line_lower or "sentiment:" in line_lower:
                sentiment_text = line.split(":")[-1].strip().lower()
                if "t√≠ch c·ª±c" in sentiment_text or "positive" in sentiment_text:
                    return "T√≠ch c·ª±c"
                elif "ti√™u c·ª±c" in sentiment_text or "negative" in sentiment_text:
                    return "Ti√™u c·ª±c"
                else:
                    return "Trung l·∫≠p"
        # N·∫øu kh√¥ng t√¨m th·∫•y d√≤ng c·∫£m x√∫c, d√πng regex t√¨m to√†n vƒÉn b·∫£n
        text = ai_summary.lower()
        if re.search(r"(t√≠ch c·ª±c|positive|l·∫°c quan|upbeat|bullish)", text):
            return "T√≠ch c·ª±c"
        elif re.search(r"(ti√™u c·ª±c|negative|bi quan|bearish|lo ng·∫°i|lo l·∫Øng)", text):
            return "Ti√™u c·ª±c"
    except Exception as e:
        logging.warning(f"L·ªói khi parse sentiment rule-based: {e}")
    return sentiment

async def extract_sentiment(ai_summary):
    """Extract sentiment from AI summary, ch·ªâ s·ª≠ d·ª•ng rule-based"""
    return extract_sentiment_rule_based(ai_summary)

def is_hot_news(entry, ai_summary, sentiment):
    """Ph√°t hi·ªán tin n√≥ng d·ª±a tr√™n ph√¢n t√≠ch n·ªôi dung, t·ª´ kh√≥a v√† c·∫£m x√∫c"""
    try:
        title = getattr(entry, 'title', '').lower()
        summary = getattr(entry, 'summary', '').lower()
        content_text = f"{title} {summary}".lower()
        
        # 1. Ki·ªÉm tra t·ª´ kh√≥a tin n√≥ng trong ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung
        for keyword in Config.HOT_NEWS_KEYWORDS:
            if keyword.lower() in content_text:
                logging.info(f"Hot news ph√°t hi·ªán b·ªüi t·ª´ kh√≥a '{keyword}': {title}")
                return True
                
        # 2. Ki·ªÉm tra c√°c c·ª•m t·ª´ ·∫£nh h∆∞·ªüng trong AI summary
        ai_text = ai_summary.lower()
        for phrase in Config.HOT_NEWS_IMPACT_PHRASES:
            if phrase.lower() in ai_text:
                logging.info(f"Hot news ph√°t hi·ªán b·ªüi c·ª•m t·ª´ ·∫£nh h∆∞·ªüng '{phrase}': {title}")
                return True
        
        # 3. Ph√¢n t√≠ch d·ª±a tr√™n c·∫£m x√∫c v√† m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng
        if sentiment != "Trung l·∫≠p":
            # N·∫øu c√≥ c·∫£m x√∫c v√† c√°c t·ª´ ch·ªâ m·ª©c ƒë·ªô cao trong ph√¢n t√≠ch AI
            intensity_words = ["r·∫•t", "m·∫°nh", "nghi√™m tr·ªçng", "ƒë√°ng k·ªÉ", "l·ªõn", "quan tr·ªçng"]
            for word in intensity_words:
                if word in ai_text and (
                    "th·ªã tr∆∞·ªùng" in ai_text or "nh√† ƒë·∫ßu t∆∞" in ai_text or "·∫£nh h∆∞·ªüng" in ai_text
                ):
                    logging.info(f"Hot news ph√°t hi·ªán b·ªüi c·∫£m x√∫c v√† m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng: {title}")
                    return True
        
        return False
    except Exception as e:
        logging.warning(f"L·ªói khi ph√°t hi·ªán tin n√≥ng: {e}")
        return False

# --- Parse RSS Feed & News Processing ---
def normalize_text(text):
    if not text:
        return ""
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ l·∫°i ch·ªØ v√† s·ªë
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)
    # Vi·∫øt th∆∞·ªùng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = text.lower().strip()
    return text

def is_recent_news(entry, days=Config.FETCH_LIMIT_DAYS):
    """
    Ch·ªâ l·∫•y tin c√≥ ng√†y ƒëƒÉng l√† h√¥m nay (theo timezone Vi·ªát Nam).
    N·∫øu kh√¥ng parse ƒë∆∞·ª£c ng√†y th√¨ b·ªè qua (kh√¥ng l·∫•y).
    """
    published = getattr(entry, 'published', None) or getattr(entry, 'updated', None)
    if not published:
        return False
    try:
        # Th·ª≠ parse nhi·ªÅu ƒë·ªãnh d·∫°ng ng√†y th√°ng
        try:
            dt = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %Z')
        except ValueError:
            try:
                dt = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
            except ValueError:
                # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, b·ªè qua tin n√†y
                return False
        # ƒê·∫£m b·∫£o c√≥ timezone
        if not dt.tzinfo:
            dt = Config.TIMEZONE.localize(dt)
        now = get_now_with_tz()
        # So s√°nh ng√†y (kh√¥ng so s√°nh gi·ªù)
        return dt.date() == now.date()
    except Exception as e:
        logger.warning(f"L·ªói khi ki·ªÉm tra th·ªùi gian tin: {e}")
        # N·∫øu c√≥ l·ªói, b·ªè qua tin n√†y
        return False

def is_relevant_news_smart(entry):
    """
    L·ªçc tin th√¥ng minh: nhi·ªÅu t·ª´ kh√≥a li√™n quan, lo·∫°i tr·ª´ spam/PR.
    """
    title = normalize_text(getattr(entry, 'title', ''))
    summary = normalize_text(getattr(entry, 'summary', ''))
    content_text = f"{title} {summary}"

    # Lo·∫°i tr·ª´ tin spam/PR
    for ex_kw in EXCLUDE_KEYWORDS:
        if normalize_text(ex_kw) in content_text:
            return False

    # ƒê·∫øm s·ªë t·ª´ kh√≥a li√™n quan xu·∫•t hi·ªán
    all_keywords = [normalize_text(k) for k in Config.RELEVANT_KEYWORDS] + [normalize_text(k) for k in additional_keywords]
    match_count = sum(1 for kw in all_keywords if kw and kw in content_text)
    
    # Ph·∫£i c√≥ √≠t nh·∫•t 1 t·ª´ kh√≥a
    return match_count >= 1

def is_hot_news_simple(entry):
    """
    Ph√°t hi·ªán tin n√≥ng m√† kh√¥ng d√πng AI, ch·ªâ d·ª±a tr√™n t·ª´ kh√≥a.
    """
    title = getattr(entry, 'title', '').lower()
    summary = getattr(entry, 'summary', '').lower()
    content_text = f"{title} {summary}".lower()
    
    for keyword in Config.HOT_NEWS_KEYWORDS:
        if keyword.lower() in content_text:
            logger.info(f"Hot news ƒë∆°n gi·∫£n ph√°t hi·ªán b·ªüi t·ª´ kh√≥a '{keyword}': {title}")
            return True
    
    return False

def is_relevant_news(entry):
    """
    Ki·ªÉm tra xem tin t·ª©c c√≥ li√™n quan ƒë·∫øn c√°c ch·ªß ƒë·ªÅ quan t√¢m kh√¥ng d·ª±a tr√™n t·ª´ kh√≥a (chu·∫©n h√≥a)
    """
    # L·∫•y n·ªôi dung t·ª´ ti√™u ƒë·ªÅ v√† t√≥m t·∫Øt, chu·∫©n h√≥a
    title = normalize_text(getattr(entry, 'title', ''))
    summary = normalize_text(getattr(entry, 'summary', ''))
    content_text = f"{title} {summary}"

    # Chu·∫©n h√≥a t·ª´ kh√≥a m·∫∑c ƒë·ªãnh v√† b·ªï sung
    all_keywords = [normalize_text(k) for k in Config.RELEVANT_KEYWORDS] + [normalize_text(k) for k in additional_keywords]

    # So kh·ªõp t·ª´ kh√≥a
    for keyword in all_keywords:
        if keyword and keyword in content_text:
            return True
    return False

async def parse_feed(url):
    try:
        feed_data = await asyncio.to_thread(feedparser.parse, url)
        if not feed_data.entries:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y tin t·ª©c t·ª´ feed: {url}")
            return []
        return feed_data.entries
    except Exception as e:
        logger.error(f"L·ªói khi parse RSS feed {url}: {e}")
        return []

def extract_image_url(entry):
    """Extract image URL from entry if available"""
    try:
        # ∆Øu ti√™n l·∫•y t·ª´ enclosures (chu·∫©n RSS qu·ªëc t·∫ø)
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if hasattr(enclosure, 'type') and 'image' in enclosure.type:
                    if hasattr(enclosure, 'href'):
                        return enclosure.href
                    elif 'url' in enclosure:
                        return enclosure['url']
        # Th·ª≠ l·∫•y t·ª´ media_thumbnail
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            thumb = entry.media_thumbnail[0]
            if 'url' in thumb:
                return thumb['url']
        # media_content (c≈©)
        if 'media_content' in entry and entry.media_content:
            for media in entry.media_content:
                if 'url' in media and ('type' not in media or 'image' in media.get('type', '')):
                    return media['url']
        # Try finding image in content
        if 'content' in entry and entry.content:
            for content in entry.content:
                if 'value' in content:
                    match = re.search(r'<img[^>]+src="([^">]+)"', content['value'])
                    if match:
                        return match.group(1)
        # Try finding image in summary
        if hasattr(entry, 'summary'):
            match = re.search(r'<img[^>]+src="([^">]+)"', entry.summary)
            if match:
                return match.group(1)
    except Exception as e:
        logger.warning(f"L·ªói khi extract ·∫£nh: {e}")
    return None
    
def extract_video_url(entry):
    """Extract video URL from entry if available"""
    try:
        # ∆Øu ti√™n l·∫•y t·ª´ enclosures (chu·∫©n RSS qu·ªëc t·∫ø)
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if hasattr(enclosure, 'type') and 'video' in enclosure.type:
                    if hasattr(enclosure, 'href'):
                        return enclosure.href
                    elif 'url' in enclosure:
                        return enclosure['url']
        # media_content c√≥ type video
        if 'media_content' in entry and entry.media_content:
            for media in entry.media_content:
                if 'url' in media and 'type' in media and 'video' in media['type']:
                    return media['url']
                if 'url' in media and media['url'].endswith('.mp4'):
                    return media['url']
        # T√¨m video trong content (th∆∞·ªùng l√† <video src=...> ho·∫∑c <source src=... type=...>)
        if 'content' in entry and entry.content:
            for content in entry.content:
                if 'value' in content:
                    match = re.search(r'<video[^>]+src="([^"]+)"', content['value'])
                    if match:
                        return match.group(1)
                    match2 = re.search(r'<source[^>]+src="([^"]+)"[^>]+type="video', content['value'])
                    if match2:
                        return match2.group(1)
        # T√¨m video trong summary
        if hasattr(entry, 'summary'):
            match = re.search(r'<video[^>]+src="([^"]+)"', entry.summary)
            if match:
                return match.group(1)
            match2 = re.search(r'<source[^>]+src="([^"]+)"[^>]+type="video', entry.summary)
            if match2:
                return match2.group(1)
    except Exception as e:
        logger.warning(f"L·ªói khi extract video: {e}")
    return None

# --- Command Handler Functions ---

# Admin only decorator
def admin_only(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        if user_id != Config.ADMIN_ID:
            await update.message.reply_text("‚ùå B·∫°n kh√¥ng c√≥ quy·ªÅn s·ª≠ d·ª•ng l·ªánh n√†y.")
            return
        return await func(update, context, *args, **kwargs)
    return wrapped

# --- Redis-only user approval ---
async def is_user_approved(user_id):
    # Lu√¥n coi ADMIN_ID l√† approved, kh√¥ng c·∫ßn l∆∞u v√†o Redis
    return user_id == Config.ADMIN_ID or await redis_client.sismember("approved_users", user_id)

# --- User approval logic using Redis set ---
async def approve_user_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    action, user_id = query.data.split("_")
    user_id = int(user_id)
    if action == "approve":
        if user_id != Config.ADMIN_ID:
            await redis_client.sadd("approved_users", user_id)
        await query.edit_message_text(f"‚úÖ User {user_id} ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát.")
        await context.bot.send_message(chat_id=user_id, text="‚úÖ B·∫°n ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng Bot News! G√µ /help ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    else:
        await query.edit_message_text(f"‚ùå ƒê√£ t·ª´ ch·ªëi y√™u c·∫ßu t·ª´ user {user_id}.")
        await context.bot.send_message(chat_id=user_id, text="‚ùå Y√™u c·∫ßu s·ª≠ d·ª•ng bot c·ªßa b·∫°n ƒë√£ b·ªã t·ª´ ch·ªëi.")

# --- L∆∞u v√† l·∫•y ti√™u ƒë·ªÅ/tin g·∫ßn ƒë√¢y b·∫±ng Redis list (gi·ªõi h·∫°n 200) ---
async def add_recent_title(normalized_title, limit=200):
    await redis_client.lpush("recent_titles", normalized_title)
    await redis_client.ltrim("recent_titles", 0, limit-1)

async def get_recent_titles(limit=200):
    return [title.decode() if isinstance(title, bytes) else title for title in await redis_client.lrange("recent_titles", 0, limit-1)]

async def add_recent_news_text(news_text, limit=200):
    await redis_client.lpush("recent_news_texts", news_text)
    await redis_client.ltrim("recent_news_texts", 0, limit-1)

async def get_recent_news_texts(limit=200):
    return [txt.decode() if isinstance(txt, bytes) else txt for txt in await redis_client.lrange("recent_news_texts", 0, limit-1)]

# --- Registration commands
async def register_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    user_id = user.id
    
    # Check if already approved
    if await is_user_approved(user_id):
        await update.message.reply_text("‚úÖ B·∫°n ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω s·ª≠ d·ª•ng bot r·ªìi!")
        return
    
    # Notify admin about registration request
    keyboard = [
        [
            InlineKeyboardButton("Approve ‚úÖ", callback_data=f"approve_{user_id}"),
            InlineKeyboardButton("Deny ‚ùå", callback_data=f"deny_{user_id}")
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await context.bot.send_message(
        chat_id=Config.ADMIN_ID,
        text=(
            f"üîî Y√™u c·∫ßu ƒëƒÉng k√Ω m·ªõi:\n"
            f"User ID: {user_id}\n"
            f"Name: {user.first_name} {user.last_name or ''}\n"
            f"Username: @{user.username or 'N/A'}"
        ),
        reply_markup=reply_markup
    )
    
    await update.message.reply_text(
        "üìù Y√™u c·∫ßu ƒëƒÉng k√Ω c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c g·ª≠i t·ªõi admin. "
        "B·∫°n s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o khi y√™u c·∫ßu ƒë∆∞·ª£c x·ª≠ l√Ω."
    )

# --- Timezone Helper Functions ---
def get_now_with_tz():
    """Return current datetime with timezone"""
    return datetime.datetime.now(Config.TIMEZONE)

def format_datetime(dt, format='%Y-%m-%d %H:%M:%S'):
    """Format datetime with timezone if needed"""
    if dt is None:
        return get_now_with_tz().strftime(format)
    
    # Check if datetime has timezone info
    if not dt.tzinfo:
        # Add timezone info if missing
        dt = Config.TIMEZONE.localize(dt)
    
    return dt.strftime(format)

def ensure_timezone_aware(dt):
    """ƒê·∫£m b·∫£o datetime object c√≥ timezone tr∆∞·ªõc khi ƒë∆∞a v√†o DB"""
    if dt is None:
        return get_now_with_tz()
    # N·∫øu ƒë√£ c√≥ tzinfo v√† offset, tr·∫£ v·ªÅ nguy√™n b·∫£n
    if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None:
        return dt
    # N·∫øu ch∆∞a c√≥ timezone, th√™m v√†o
    try:
        return Config.TIMEZONE.localize(dt)
    except (ValueError, AttributeError):
        # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá
        logger.warning(f"Kh√¥ng th·ªÉ th√™m timezone cho datetime: {dt}")
        # T·∫°o m·ªõi datetime v·ªõi timezone
        return datetime.datetime(
            dt.year, dt.month, dt.day, 
            dt.hour, dt.minute, dt.second, 
            dt.microsecond, Config.TIMEZONE
        )

async def normalize_title(title):
    """Chu·∫©n h√≥a ti√™u ƒë·ªÅ tin t·ª©c ƒë·ªÉ so s√°nh"""
    if not title:
        return ""
    
    # Remove HTML tags if any
    title = re.sub(r'<[^>]+>', '', title)
    
    # Normalize unicode
    title = unicodedata.normalize('NFD', title)
    title = ''.join([c for c in title if unicodedata.category(c) != 'Mn'])
    
    # Convert to lowercase and remove extra spaces
    title = re.sub(r'\s+', ' ', title.lower().strip())
    
    # Remove special characters
    title = re.sub(r'[^\w\s]', '', title)
    
    return title

async def is_title_sent(normalized_title):
    """Ki·ªÉm tra xem ti√™u ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c g·ª≠i ch∆∞a (d·ª±a tr√™n ti√™u ƒë·ªÅ chu·∫©n h√≥a)"""
    return await redis_client.sismember("sent_titles", normalized_title)

async def mark_title_sent(normalized_title):
    """ƒê√°nh d·∫•u ti√™u ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c g·ª≠i"""
    await redis_client.sadd("sent_titles", normalized_title)
    await redis_client.expire("sent_titles", Config.REDIS_TTL)

# --- News Deduplication by Hash ---
async def is_hash_sent(content_hash):
    return await redis_client.sismember("sent_hashes", content_hash)

async def mark_hash_sent(content_hash):
    await redis_client.sadd("sent_hashes", content_hash)
    await redis_client.expire("sent_hashes", Config.REDIS_TTL)

def is_similar_title(new_title, recent_titles, threshold=None):
    """So s√°nh ti√™u ƒë·ªÅ m·ªõi v·ªõi c√°c ti√™u ƒë·ªÅ c≈©, n·∫øu similarity > threshold th√¨ coi l√† tr√πng"""
    if not recent_titles:
        return False
        
    # S·ª≠ d·ª•ng ng∆∞·ª°ng t·ª´ Config n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if threshold is None:
        threshold = Config.TITLE_SIMILARITY_THRESHOLD
        
    try:
        titles = [new_title] + recent_titles
        vectorizer = TfidfVectorizer().fit_transform(titles)
        vectors = vectorizer.toarray()
        sim_scores = cosine_similarity([vectors[0]], vectors[1:])[0]
        max_sim = max(sim_scores) if len(sim_scores) > 0 else 0
        
        # Ghi log chi ti·∫øt n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh
        if Config.LOG_SIMILARITY_DETAILS and max_sim > 0.6:
            max_idx = sim_scores.argmax() if len(sim_scores) > 0 else -1
            similar_title = recent_titles[max_idx] if max_idx >= 0 else "N/A"
            logger.info(f"Similarity ti√™u ƒë·ªÅ: {max_sim:.2f} (ng∆∞·ª°ng: {threshold:.2f})")
            logger.debug(f"Ti√™u ƒë·ªÅ m·ªõi: {new_title[:30]}... | Ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {similar_title[:30]}...")
            
        return max_sim > threshold
    except Exception as e:
        logger.error(f"L·ªói khi so s√°nh ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {e}")
        return False

# --- News Duplication Detection ---
def is_duplicate_by_content(new_text, recent_texts, threshold=Config.DUPLICATE_THRESHOLD):
    """
    Ph√°t hi·ªán tin tr√πng l·∫∑p b·∫±ng TF-IDF v√† Cosine Similarity
    N√¢ng c·∫•p:
    - S·ª≠ d·ª•ng ngram t·ª´ 1-3 ƒë·ªÉ b·∫Øt c·ª•m t·ª´ d√†i h∆°n
    - Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát
    - T·ªëi ∆∞u vector h√≥a
    - Ph√°t hi·ªán ch√≠nh x√°c h∆°n c√°c tin c√≥ c√πng n·ªôi dung nh∆∞ng kh√°c ngu·ªìn
    """
    if not recent_texts:
        return False
    
    try:
        # Danh s√°ch stopwords ti·∫øng Vi·ªát c∆° b·∫£n
        vn_stopwords = {
            "v√†", "l√†", "c·ªßa", "c√≥", "ƒë∆∞·ª£c", "trong", "cho", "kh√¥ng", "ƒë√£", "v·ªõi", "ƒë∆∞·ª£c", "n√†y",
            "ƒë·∫øn", "t·ª´", "khi", "nh∆∞", "ng∆∞·ªùi", "nh·ªØng", "s·∫Ω", "v√†o", "v·ªÅ", "c√≤n", "b·ªã", "theo",
            "ƒë·ªÉ", "t·∫°i", "nh∆∞ng", "ra", "n√™n", "m·ªôt", "c√°c", "c≈©ng", "ƒëang", "t·ªõi", "tr√™n", "t√¥i",
            "b·∫°n", "ch√∫ng", "r·∫±ng", "th√¨", "ƒë√≥", "l√†m", "n·∫øu", "n√≥i", "b·ªüi", "l√™n", "kh√°c", "h·ªç"
        }
        
        # Th√™m tin m·ªõi v√†o ƒë·∫ßu danh s√°ch ƒë·ªÉ vector h√≥a
        texts = [new_text] + recent_texts
        
        # Ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu v√† gi·ªØ l·∫°i n·ªôi dung quan tr·ªçng
        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ ph√°t hi·ªán tin c√πng n·ªôi dung t·ª´ c√°c ngu·ªìn kh√°c nhau
        processed_texts = []
        for text in texts:
            # Chu·∫©n h√≥a tin, ch·ªâ gi·ªØ t·ª´ kh√≥a ch√≠nh
            words = text.lower().split()
            words = [w for w in words if w not in vn_stopwords and len(w) > 1]
            processed_texts.append(' '.join(words))
        
        # T√≠nh vector TF-IDF, d√πng ngram t·ª´ 1-3 ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c c·ª•m t·ª´ c√≥ √Ω nghƒ©a
        vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words="english", # V·∫´n gi·ªØ cho c√°c t·ª´ ti·∫øng Anh
            ngram_range=(1, 3),   # N√¢ng l√™n (1, 3) ƒë·ªÉ b·∫Øt c·ª•m t·ª´ d√†i h∆°n
            min_df=1,
            max_features=10000    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·ªÉ tƒÉng hi·ªáu su·∫•t
        ).fit_transform(processed_texts)
        
        # Chuy·ªÉn sang m·∫£ng ƒë·ªÉ so s√°nh
        vectors = vectorizer.toarray()
        
        # T√≠nh cosine similarity gi·ªØa tin m·ªõi v√† c√°c tin c≈©
        sim_scores = cosine_similarity([vectors[0]], vectors[1:])[0]
        
        # Ki·ªÉm tra c√≥ tr√πng l·∫∑p kh√¥ng (similarity > threshold)
        max_similarity = max(sim_scores) if len(sim_scores) > 0 else 0
        is_duplicate = max_similarity > threshold
        
        if is_duplicate:
            max_idx = sim_scores.argmax() if len(sim_scores) > 0 else -1
            similar_text = recent_texts[max_idx] if max_idx >= 0 else "N/A"
            logger.info(f"Ph√°t hi·ªán tin tr√πng l·∫∑p! Similarity: {max_similarity:.2f}, Threshold: {threshold}")
            logger.debug(f"Tin m·ªõi: {new_text[:50]}... | Tin tr√πng: {similar_text[:50]}...")
        
        return is_duplicate
    except Exception as e:
        logger.error(f"L·ªói khi ph√°t hi·ªán tin tr√πng l·∫∑p: {e}")
        return False  # N·∫øu l·ªói, coi nh∆∞ kh√¥ng tr√πng ƒë·ªÉ x·ª≠ l√Ω tin

async def init_redis():
    """Initialize Redis connection"""
    global redis_client
    try:
        redis_client = aioredis.from_url(Config.REDIS_URL)
        
        # Load additional keywords from Redis if they exist
        global additional_keywords
        keywords_data = await redis_client.get("additional_keywords")
        if keywords_data:
            additional_keywords = pickle.loads(keywords_data)
            logger.info(f"Loaded {len(additional_keywords)} additional keywords from Redis")
        
        # Load duplicate detection settings
        dup_threshold = await redis_client.get("duplicate_threshold")
        if dup_threshold:
            try:
                Config.DUPLICATE_THRESHOLD = float(dup_threshold.decode())
                logger.info(f"Loaded duplicate threshold: {Config.DUPLICATE_THRESHOLD}")
            except (ValueError, AttributeError) as e:
                logger.warning(f"Failed to load duplicate threshold: {e}")
                
        title_threshold = await redis_client.get("title_threshold")
        if title_threshold:
            try:
                Config.TITLE_SIMILARITY_THRESHOLD = float(title_threshold.decode())
                logger.info(f"Loaded title similarity threshold: {Config.TITLE_SIMILARITY_THRESHOLD}")
            except (ValueError, AttributeError) as e:
                logger.warning(f"Failed to load title threshold: {e}")
                
        log_similarity = await redis_client.get("log_similarity_details")
        if log_similarity:
            try:
                Config.LOG_SIMILARITY_DETAILS = log_similarity.decode().lower() == "true"
                logger.info(f"Loaded similarity logging setting: {Config.LOG_SIMILARITY_DETAILS}")
            except AttributeError as e:
                logger.warning(f"Failed to load similarity logging setting: {e}")
        
        logger.info("Redis initialized successfully")
        return True
    except Exception as e:
        logger.error(f"Error initializing Redis: {e}")
        return False

async def clear_news_queues():
    """X√≥a t·∫•t c·∫£ tin t·ª©c trong queue khi kh·ªüi ƒë·ªông l·∫°i bot ƒë·ªÉ tr√°nh g·ª≠i l·∫°i tin c≈©"""
    try:
        # X√≥a c√°c queue tin t·ª©c
        await redis_client.delete("hot_news_queue")
        await redis_client.delete("news_queue")
        # Gi·ªØ l·∫°i ids ƒë√£ g·ª≠i ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        logger.info("ƒê√£ x√≥a t·∫•t c·∫£ tin trong queue ƒë·ªÉ chu·∫©n b·ªã cho qu√©t m·ªõi")
    except Exception as e:
        logger.error(f"L·ªói khi x√≥a queue tin t·ª©c: {e}")

# --- Helper for rotating RSS feed index ---
async def get_current_feed_index():
    idx = await redis_client.get("current_feed_index")
    if idx is not None:
        return int(idx)
    return 0

async def set_current_feed_index(idx):
    await redis_client.set("current_feed_index", str(idx))

# --- Main Function ---

# Global application variable to store our bot instance
application = None

async def send_message_to_user(user_id, message, entry=None, is_hot_news=False):
    """Send a news message to a user, k√®m ·∫£nh ho·∫∑c video n·∫øu c√≥"""
    try:
        # Chu·∫©n b·ªã n·ªôi dung tin nh·∫Øn
        title = getattr(entry, 'title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        link = getattr(entry, 'link', '#')
        published = getattr(entry, 'published', None)
        if isinstance(published, str):
            try:
                published = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %Z')
                published = ensure_timezone_aware(published)
            except ValueError:
                try:
                    published = datetime.datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
                except ValueError:
                    published = None
        date = format_datetime(published) if published else format_datetime(None)
        domain = urlparse(link).netloc
        prefix = "üî• TIN N√ìNG: " if is_hot_news else "üì∞ TIN M·ªöI: "
        formatted_message = (
            f"{prefix}<b>{title}</b>\n\n"
            f"<pre>{message}</pre>\n\n"
            f"<i>Ngu·ªìn: {domain} ‚Ä¢ {date}</i>\n"
            f"<a href='{link}'>ƒê·ªçc chi ti·∫øt</a>"
        )
        keyboard = [[InlineKeyboardButton("ƒê·ªçc chi ti·∫øt", url=link)]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        global application
        if application and application.bot:
            bot = application.bot
        else:
            from telegram import Bot
            bot = Bot(token=Config.BOT_TOKEN)
        # --- G·ª≠i media n·∫øu c√≥ ---
        image_url = extract_image_url(entry)
        video_url = extract_video_url(entry)
        if video_url:
            await bot.send_video(
                chat_id=user_id,
                video=video_url,
                caption=formatted_message,
                reply_markup=reply_markup,
                parse_mode='HTML',
                supports_streaming=True
            )
        elif image_url:
            await bot.send_photo(
                chat_id=user_id,
                photo=image_url,
                caption=formatted_message,
                reply_markup=reply_markup,
                parse_mode='HTML'
            )
        else:
            await bot.send_message(
                chat_id=user_id,
                text=formatted_message,
                reply_markup=reply_markup,
                parse_mode='HTML',
                disable_web_page_preview=False
            )
    except Exception as e:
        logger.error(f"L·ªói khi g·ª≠i tin t·ª©c cho user {user_id}: {e}")

# --- C√°c job ƒë·ªãnh k·ª≥ m·ªõi ---

async def process_and_send_news(news_data):
    """
    X·ª≠ l√Ω (AI, d·ªãch, ƒë√°nh d·∫•u ƒë√£ g·ª≠i) v√† g·ª≠i tin cho t·∫•t c·∫£ user ƒë√£ duy·ªát. D√πng cho c·∫£ g·ª≠i ƒë·ªãnh k·ª≥ v√† g·ª≠i ngay l·∫≠p t·ª©c.
    """
    try:
        # L·∫•y danh s√°ch ng∆∞·ªùi d√πng ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát
        approved_users_list = [int(uid) for uid in await redis_client.smembers("approved_users")]
        if Config.ADMIN_ID not in approved_users_list:
            approved_users_list.append(Config.ADMIN_ID)
        if not approved_users_list:
            logger.warning("Kh√¥ng c√≥ ng∆∞·ªùi d√πng n√†o ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ g·ª≠i tin.")
            return

        # Ph√¢n t√≠ch tin t·ª©c b·∫±ng AI
        domain = urlparse(news_data['link']).netloc if 'link' in news_data else 'N/A'
        prompt = f"""
        T√≥m t·∫Øt v√† ph√¢n t√≠ch tin t·ª©c sau cho nh√† ƒë·∫ßu t∆∞ ch·ª©ng kho√°n Vi·ªát Nam.
        \nTi√™u ƒë·ªÅ: {news_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}
        T√≥m t·∫Øt: {news_data.get('summary', 'Kh√¥ng c√≥ t√≥m t·∫Øt')}
        Ngu·ªìn: {domain}
        \n1. T√≥m t·∫Øt ng·∫Øn g·ªçn n·ªôi dung 
        2. ƒê√°nh gi√° t√°c ƒë·ªông ( 2-3 c√¢u ). C·∫£m x√∫c (T√≠ch c·ª±c/Ti√™u c·ª±c/Trung l·∫≠p)
        3. M√£/ng√†nh li√™n quan
        """
        try:
            ai_summary = await analyze_news(prompt)
        except Exception as e:
            logger.error(f"L·ªói khi ph√¢n t√≠ch tin b·∫±ng AI: {e}")
            ai_summary = news_data.get('summary', 'Kh√¥ng c√≥ ph√¢n t√≠ch n√†o.')

        # N·∫øu l√† ti·∫øng Anh th√¨ d·ªãch sang ti·∫øng Vi·ªát
        lang = detect_language(news_data.get('title', '') + ' ' + news_data.get('summary', ''))
        if lang == 'en':
            ai_summary = await translate_to_vietnamese(ai_summary)

        # T·∫°o ƒë·ªëi t∆∞·ª£ng entry t·ª´ news_data ƒë·ªÉ truy·ªÅn v√†o h√†m g·ª≠i
        class EntryObject:
            pass
        entry = EntryObject()
        for key, value in news_data.items():
            setattr(entry, key, value)

        # X·ª≠ l√Ω sentiment n·∫øu c·∫ßn
        is_hot = news_data.get('is_hot', False)
        sentiment = await extract_sentiment(ai_summary) if is_hot else 'Trung l·∫≠p'

        # ƒê√°nh d·∫•u tin ƒë√£ ƒë∆∞·ª£c g·ª≠i
        await mark_sent(news_data.get('id', '') or news_data.get('link', ''))

        # G·ª≠i tin cho t·∫•t c·∫£ ng∆∞·ªùi d√πng
        sent_count = 0
        for user_id in approved_users_list:
            try:
                await send_message_to_user(user_id, ai_summary, entry, news_data.get('is_hot', False))
                sent_count += 1
            except Exception as e:
                logger.error(f"L·ªói khi g·ª≠i tin cho user {user_id}: {e}")
        logger.info(f"ƒê√£ g·ª≠i tin '{news_data.get('title', '')[:30]}...' cho {sent_count}/{len(approved_users_list)} ng∆∞·ªùi d√πng.")
    except Exception as e:
        logger.error(f"L·ªói trong process_and_send_news: {e}")

async def fetch_and_cache_news(context: ContextTypes.DEFAULT_TYPE):
    """
    Qu√©t t·∫•t c·∫£ RSS trong FEED_URLS m·ªói 5 ph√∫t, l·ªçc v√† cache tin m·ªõi. Tin n√≥ng ch·ªâ g·ª≠i n·∫øu l√† m·ªõi nh·∫•t v√† ch∆∞a t·ª´ng g·ª≠i.
    """
    try:
        logger.info("ƒêang qu√©t t·∫•t c·∫£ RSS v√† cache tin m·ªõi...")
        recent_news_texts_raw = await get_recent_news_texts()
        recent_news_texts = [normalize_text(txt) for txt in recent_news_texts_raw]
        recent_titles = await get_recent_titles(limit=200)
        queued_count = 0
        skipped_count = 0
        duplicate_content_count = 0
        hot_news_count = 0
        feeds = Config.FEED_URLS
        # L·∫•y published c·ªßa tin n√≥ng m·ªõi nh·∫•t ƒë√£ g·ª≠i (l∆∞u trong Redis, key: latest_hot_news_published)
        latest_hot_news_published = await redis_client.get("latest_hot_news_published")
        if latest_hot_news_published:
            try:
                latest_hot_news_published = datetime.datetime.fromisoformat(latest_hot_news_published.decode())
            except Exception:
                latest_hot_news_published = None
        else:
            latest_hot_news_published = None
        for feed_url in feeds:
            logger.info(f"Qu√©t RSS: {feed_url}")
            entries = await parse_feed(feed_url)
            for entry in entries:
                try:
                    if not is_recent_news(entry, days=Config.FETCH_LIMIT_DAYS):
                        continue
                    if not is_relevant_news_smart(entry):
                        continue
                    entry_id = getattr(entry, 'id', '') or getattr(entry, 'link', '')
                    normalized_title = await normalize_title(getattr(entry, 'title', ''))
                    entry_text = f"{getattr(entry, 'title', '')} {getattr(entry, 'summary', '')}"
                    entry_text_norm = normalize_text(entry_text)
                    content_hash = hashlib.sha256(entry_text_norm.encode('utf-8')).hexdigest()
                    if await is_hash_sent(content_hash):
                        skipped_count += 1
                        continue
                    if is_similar_title(normalized_title, recent_titles, threshold=Config.TITLE_SIMILARITY_THRESHOLD):
                        skipped_count += 1
                        continue
                    prepared_content = normalize_text(f"{getattr(entry, 'title', '')} {getattr(entry, 'summary', '')}")
                    if is_duplicate_by_content(prepared_content, recent_news_texts, threshold=Config.DUPLICATE_THRESHOLD):
                        logger.info(f"Ph√°t hi·ªán tin tr√πng l·∫∑p n·ªôi dung t·ª´ ngu·ªìn kh√°c nhau: {getattr(entry, 'title', '')[:50]}...")
                        duplicate_content_count += 1
                        continue
                    recent_titles.append(normalized_title)
                    if await is_title_sent(normalized_title):
                        skipped_count += 1
                        continue
                    if await redis_client.sismember("news_queue_ids", entry_id):
                        skipped_count += 1
                        continue
                    if await is_sent(entry_id):
                        skipped_count += 1
                        continue
                    recent_news_texts.append(entry_text_norm)
                    is_hot = is_hot_news_simple(entry)
                    # Parse published datetime
                    published_str = getattr(entry, "published", "")
                    published_dt = None
                    if published_str:
                        try:
                            published_dt = datetime.datetime.strptime(published_str, '%a, %d %b %Y %H:%M:%S %Z')
                        except ValueError:
                            try:
                                published_dt = datetime.datetime.strptime(published_str, '%a, %d %b %Y %H:%M:%S %z')
                            except ValueError:
                                published_dt = None
                    news_data = {
                        "id": entry_id,
                        "title": getattr(entry, "title", ""),
                        "link": getattr(entry, "link", ""),
                        "summary": getattr(entry, "summary", ""),
                        "published": published_str,
                        "is_hot": is_hot,
                    }
                    if is_hot:
                        # Ch·ªâ g·ª≠i n·∫øu published m·ªõi h∆°n latest_hot_news_published V√Ä ch∆∞a t·ª´ng g·ª≠i (id/hash/title)
                        already_sent = await is_sent(entry_id) or await is_hash_sent(content_hash) or await is_title_sent(normalized_title)
                        if published_dt and (not latest_hot_news_published or published_dt > latest_hot_news_published) and not already_sent:
                            await redis_client.rpush("hot_news_queue", json.dumps(news_data))
                            hot_news_count += 1
                            await process_and_send_news(news_data)
                            # C·∫≠p nh·∫≠t latest_hot_news_published
                            await redis_client.set("latest_hot_news_published", published_dt.isoformat())
                        else:
                            logger.info(f"B·ªè qua tin n√≥ng c≈© ho·∫∑c ƒë√£ g·ª≠i: {getattr(entry, 'title', '')[:50]}...")
                    else:
                        await redis_client.rpush("news_queue", json.dumps(news_data))
                    await redis_client.sadd("news_queue_ids", entry_id)
                    await redis_client.expire("news_queue_ids", Config.REDIS_TTL)
                    await mark_title_sent(normalized_title)
                    await mark_hash_sent(content_hash)
                    queued_count += 1
                    await add_recent_title(normalized_title)
                    await add_recent_news_text(entry_text_norm)
                except Exception as e:
                    logger.warning(f"L·ªói khi x·ª≠ l√Ω tin t·ª´ feed {feed_url}: {e}")
        hot_queue_len = await redis_client.llen("hot_news_queue")
        normal_queue_len = await redis_client.llen("news_queue")
        logger.info(f"Qu√©t RSS ho√†n t·∫•t: ƒê√£ cache {queued_count} tin m·ªõi ({hot_news_count} tin n√≥ng), "
                   f"b·ªè qua {skipped_count} tin tr√πng l·∫∑p th√¥ng th∆∞·ªùng, {duplicate_content_count} tin tr√πng n·ªôi dung. "
                   f"S·ªë tin trong queue: {hot_queue_len} tin n√≥ng, {normal_queue_len} tin th∆∞·ªùng.")
    except Exception as e:
        logger.error(f"L·ªói trong job fetch_and_cache_news: {e}")

async def send_news_from_queue(context: ContextTypes.DEFAULT_TYPE):
    """
    Job ch·∫°y m·ªói 800s ƒë·ªÉ l·∫•y 1 tin t·ª´ queue v√† g·ª≠i cho user.
    ∆Øu ti√™n tin n√≥ng tr∆∞·ªõc.
    """
    try:
        # ∆Øu ti√™n l·∫•y tin n√≥ng tr∆∞·ªõc
        news_json = await redis_client.lpop("hot_news_queue")
        if not news_json:
            # N·∫øu kh√¥ng c√≥ tin n√≥ng, l·∫•y tin th∆∞·ªùng
            news_json = await redis_client.lpop("news_queue")
        if not news_json:
            logger.info("Kh√¥ng c√≤n tin trong c·∫£ hai queue. ƒê·ª£i chu k·ª≥ fetch ti·∫øp theo.")
            return
        # Parse JSON th√†nh dict
        news_data = json.loads(news_json)
        await process_and_send_news(news_data)
    except Exception as e:
        logger.error(f"L·ªói trong job send_news_from_queue: {e}")

# H√†m ph√°t hi·ªán ng√¥n ng·ªØ
def detect_language(text):
    try:
        return detect(text)
    except Exception:
        return "unknown"

# H√†m d·ªãch sang ti·∫øng Vi·ªát
async def translate_to_vietnamese(text):
    try:
        # deep-translator kh√¥ng async, n√™n d√πng to_thread
        result = await asyncio.to_thread(GoogleTranslator(source='auto', target='vi').translate, text)
        return result
    except Exception as e:
        logging.error(f"L·ªói khi d·ªãch sang ti·∫øng Vi·ªát: {e}")
        return text

# Function to handle callback queries
async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    # Handle different callback data
    if query.data == "view_keywords":
        # Reuse view_keywords_command logic
        user_id = update.effective_user.id
        
        if not await is_user_approved(user_id):
            await query.message.reply_text(
                "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
            )
            return
        
        global additional_keywords
        default_keywords = Config.RELEVANT_KEYWORDS
        
        message = (
            f"üìã *Danh s√°ch t·ª´ kh√≥a hi·ªán t·∫°i*\n\n"
            f"*T·ª´ kh√≥a m·∫∑c ƒë·ªãnh ({len(default_keywords)})*: Bao g·ªìm c√°c t·ª´ kh√≥a v·ªÅ ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh...\n\n"
        )
        
        if additional_keywords:
            message += f"*T·ª´ kh√≥a b·ªï sung ({len(additional_keywords)})*:\n{', '.join(additional_keywords)}\n\n"
        else:
            message += "*T·ª´ kh√≥a b·ªï sung*: Ch∆∞a c√≥\n\n"
        
        message += (
            "S·ª≠ d·ª•ng /set_keywords ƒë·ªÉ th√™m t·ª´ kh√≥a b·ªï sung.\n"
            "S·ª≠ d·ª•ng /clear_keywords ƒë·ªÉ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung."
        )
        
        await query.message.reply_text(message, parse_mode='Markdown')
    
    elif query.data == "help":
        # Show help message
        await help_command(update, context)
    
    # Handle other callbacks
    elif query.data.startswith("approve_") or query.data.startswith("deny_"):
        await approve_user_callback(update, context)

async def job_ping(context: ContextTypes.DEFAULT_TYPE):
    try:
        url = "https://anvt.onrender.com/health"  # N·∫øu endpoint n√†y kh√¥ng c√≥, ƒë·ªïi th√†nh "/"
        async with httpx.AsyncClient(timeout=10) as client:
            await client.get(url)
        logger.info(f"Ping gi·ªØ bot awake t·ªõi {url}")
    except Exception as e:
        logger.error(f"L·ªói khi ping gi·ªØ awake: {e}")

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    await update.message.reply_text(
        f"üëã Xin ch√†o {user.first_name or 'b·∫°n'}! ƒê√¢y l√† bot t·ªïng h·ª£p tin t·ª©c ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh.\n"
        "G√µ /help ƒë·ªÉ xem h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    is_admin = user_id == Config.ADMIN_ID
    
    basic_commands = (
        "C√°c l·ªánh h·ªó tr·ª£:\n"
        "/start - B·∫Øt ƒë·∫ßu\n"
        "/help - H∆∞·ªõng d·∫´n\n"
        "/register - ƒêƒÉng k√Ω s·ª≠ d·ª•ng bot\n"
        "/keywords - Xem t·ª´ kh√≥a l·ªçc tin\n"
        "/set_keywords - Th√™m t·ª´ kh√≥a l·ªçc tin\n"
        "/clear_keywords - X√≥a t·ª´ kh√≥a b·ªï sung"
    )
    
    if is_admin:
        admin_commands = (
            "\n\nL·ªánh d√†nh ri√™ng cho admin:\n"
            "/check_dup_settings - Ki·ªÉm tra c√†i ƒë·∫∑t l·ªçc tin tr√πng\n"
            "/set_dup_threshold - ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán n·ªôi dung tr√πng\n"
            "/set_title_threshold - ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±\n"
            "/toggle_sim_log - B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity"
        )
        await update.message.reply_text(basic_commands + admin_commands)
    else:
        await update.message.reply_text(basic_commands)

async def view_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    default_keywords = Config.RELEVANT_KEYWORDS

    message = (
        f"üìã *Danh s√°ch t·ª´ kh√≥a hi·ªán t·∫°i*\n\n"
        f"*T·ª´ kh√≥a m·∫∑c ƒë·ªãnh ({len(default_keywords)})*: Bao g·ªìm c√°c t·ª´ kh√≥a v·ªÅ ch·ª©ng kho√°n, kinh t·∫ø, t√†i ch√≠nh...\n\n"
    )

    if additional_keywords:
        message += f"*T·ª´ kh√≥a b·ªï sung ({len(additional_keywords)})*:\n{', '.join(additional_keywords)}\n\n"
    else:
        message += "*T·ª´ kh√≥a b·ªï sung*: Ch∆∞a c√≥\n\n"

    message += (
        "S·ª≠ d·ª•ng /set_keywords ƒë·ªÉ th√™m t·ª´ kh√≥a b·ªï sung.\n"
        "S·ª≠ d·ª•ng /clear_keywords ƒë·ªÉ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung."
    )

    await update.message.reply_text(message, parse_mode='Markdown')

async def set_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    if not context.args:
        await update.message.reply_text(
            "‚úèÔ∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a b·∫°n mu·ªën th√™m. V√≠ d·ª•: `/set_keywords bitcoin, eth`"
        )
        return

    new_keywords = [normalize_text(kw.strip()) for kw in ' '.join(context.args).split(',') if kw.strip()]
    
    if not new_keywords:
        await update.message.reply_text("‚ö†Ô∏è Kh√¥ng c√≥ t·ª´ kh√≥a h·ª£p l·ªá n√†o ƒë∆∞·ª£c cung c·∫•p.")
        return

    added_count = 0
    for kw in new_keywords:
        if kw not in additional_keywords:
            additional_keywords.append(kw)
            added_count += 1
    
    if added_count > 0:
        # Save to Redis
        await redis_client.set("additional_keywords", pickle.dumps(additional_keywords))
        await update.message.reply_text(
            f"‚úÖ ƒê√£ th√™m {added_count} t·ª´ kh√≥a m·ªõi. Hi·ªán c√≥ {len(additional_keywords)} t·ª´ kh√≥a b·ªï sung.\n"
            "G√µ /keywords ƒë·ªÉ xem danh s√°ch ƒë·∫ßy ƒë·ªß."
        )
    else:
        await update.message.reply_text(
            "‚ÑπÔ∏è C√°c t·ª´ kh√≥a b·∫°n nh·∫≠p ƒë√£ c√≥ s·∫µn ho·∫∑c kh√¥ng h·ª£p l·ªá."
        )

async def clear_keywords_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not await is_user_approved(user_id):
        await update.message.reply_text(
            "‚ùå B·∫°n ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát ƒë·ªÉ s·ª≠ d·ª•ng bot. G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω."
        )
        return

    global additional_keywords
    if not additional_keywords:
        await update.message.reply_text("‚ÑπÔ∏è Hi·ªán kh√¥ng c√≥ t·ª´ kh√≥a b·ªï sung n√†o ƒë·ªÉ x√≥a.")
        return
        
    additional_keywords.clear()
    # Save to Redis
    await redis_client.delete("additional_keywords")
    await update.message.reply_text(
        "üóëÔ∏è ƒê√£ x√≥a t·∫•t c·∫£ t·ª´ kh√≥a b·ªï sung. Bot s·∫Ω ch·ªâ s·ª≠ d·ª•ng danh s√°ch t·ª´ kh√≥a m·∫∑c ƒë·ªãnh.\n"
        "G√µ /keywords ƒë·ªÉ xem l·∫°i."
    )

@admin_only
async def set_duplicate_threshold(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to set the duplicate detection threshold without restarting the bot
    Usage: /set_dup_threshold 0.65
    """
    if not context.args or len(context.args) != 1:
        await update.message.reply_text(
            "‚ùå C√∫ ph√°p sai. S·ª≠ d·ª•ng: `/set_dup_threshold 0.65`\n"
            "Gi√° tr·ªã t·ª´ 0.0 ƒë·∫øn 1.0, c√†ng th·∫•p c√†ng nh·∫°y v·ªõi vi·ªác ph√°t hi·ªán tr√πng l·∫∑p."
        )
        return
        
    try:
        threshold = float(context.args[0])
        if threshold < 0.0 or threshold > 1.0:
            await update.message.reply_text("‚ùå Gi√° tr·ªã ph·∫£i t·ª´ 0.0 ƒë·∫øn 1.0")
            return
            
        # C·∫≠p nh·∫≠t gi√° tr·ªã trong Config
        Config.DUPLICATE_THRESHOLD = threshold
        
        # L∆∞u v√†o Redis ƒë·ªÉ gi·ªØ gi√° tr·ªã khi kh·ªüi ƒë·ªông l·∫°i
        await redis_client.set("duplicate_threshold", str(threshold))
        
        await update.message.reply_text(
            f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t ng∆∞·ª°ng ph√°t hi·ªán tin tr√πng l·∫∑p n·ªôi dung: {threshold}\n"
            f"√Åp d·ª•ng ngay cho c√°c l·∫ßn qu√©t RSS ti·∫øp theo."
        )
    except ValueError:
        await update.message.reply_text("‚ùå Gi√° tr·ªã kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p s·ªë t·ª´ 0.0 ƒë·∫øn 1.0")

@admin_only
async def toggle_similarity_logging(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to toggle detailed similarity logging
    Usage: /toggle_sim_log
    """
    # Toggle gi√° tr·ªã
    Config.LOG_SIMILARITY_DETAILS = not Config.LOG_SIMILARITY_DETAILS
    
    # L∆∞u v√†o Redis
    await redis_client.set("log_similarity_details", str(Config.LOG_SIMILARITY_DETAILS).lower())
    
    status = "b·∫≠t" if Config.LOG_SIMILARITY_DETAILS else "t·∫Øt"
    await update.message.reply_text(
        f"‚úÖ ƒê√£ {status} ghi log chi ti·∫øt v·ªÅ ph√°t hi·ªán tin tr√πng l·∫∑p.\n"
        "Xem log h·ªá th·ªëng ƒë·ªÉ theo d√µi th√¥ng tin chi ti·∫øt v·ªÅ similarity."
    )

@admin_only
async def check_dup_settings(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to check current duplicate detection settings
    Usage: /check_dup_settings
    """
    settings = (
        f"üìä *C√†i ƒë·∫∑t ph√°t hi·ªán tin tr√πng l·∫∑p*\n\n"
        f"‚Ä¢ Ng∆∞·ª°ng tin tr√πng n·ªôi dung (DUPLICATE_THRESHOLD): {Config.DUPLICATE_THRESHOLD}\n"
        f"‚Ä¢ Ng∆∞·ª°ng ti√™u ƒë·ªÅ t∆∞∆°ng t·ª± (TITLE_SIMILARITY_THRESHOLD): {Config.TITLE_SIMILARITY_THRESHOLD}\n"
        f"‚Ä¢ Ghi log chi ti·∫øt: {'B·∫≠t' if Config.LOG_SIMILARITY_DETAILS else 'T·∫Øt'}\n\n"
        f"L·ªánh ƒëi·ªÅu ch·ªânh:\n"
        f"‚Ä¢ /set_dup_threshold [0.0-1.0] - ƒê·∫∑t ng∆∞·ª°ng ph√°t hi·ªán n·ªôi dung tr√πng l·∫∑p\n"
        f"‚Ä¢ /set_title_threshold [0.0-1.0] - ƒê·∫∑t ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±\n"
        f"‚Ä¢ /toggle_sim_log - B·∫≠t/t·∫Øt log chi ti·∫øt v·ªÅ similarity"
    )
    await update.message.reply_text(settings, parse_mode='Markdown')

@admin_only
async def set_title_threshold(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Admin command to set the title similarity threshold
    Usage: /set_title_threshold 0.92
    """
    if not context.args or len(context.args) != 1:
        await update.message.reply_text(
            "‚ùå C√∫ ph√°p sai. S·ª≠ d·ª•ng: `/set_title_threshold 0.92`\n"
            "Gi√° tr·ªã t·ª´ 0.0 ƒë·∫øn 1.0, c√†ng cao c√†ng y√™u c·∫ßu ti√™u ƒë·ªÅ gi·ªëng nhau m·ªõi coi l√† tr√πng."
        )
        return
        
    try:
        threshold = float(context.args[0])
        if threshold < 0.0 or threshold > 1.0:
            await update.message.reply_text("‚ùå Gi√° tr·ªã ph·∫£i t·ª´ 0.0 ƒë·∫øn 1.0")
            return
            
        # C·∫≠p nh·∫≠t gi√° tr·ªã trong Config
        Config.TITLE_SIMILARITY_THRESHOLD = threshold
        
        # L∆∞u v√†o Redis ƒë·ªÉ gi·ªØ gi√° tr·ªã khi kh·ªüi ƒë·ªông l·∫°i
        await redis_client.set("title_threshold", str(threshold))
        
        await update.message.reply_text(
            f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t ng∆∞·ª°ng ph√°t hi·ªán ti√™u ƒë·ªÅ t∆∞∆°ng t·ª±: {threshold}\n"
            f"√Åp d·ª•ng ngay cho c√°c l·∫ßn qu√©t RSS ti·∫øp theo."
        )
    except ValueError:
        await update.message.reply_text("‚ùå Gi√° tr·ªã kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p s·ªë t·ª´ 0.0 ƒë·∫øn 1.0")

def main():
    global application, shutdown_flag
    
    # Reset shutdown flag
    shutdown_flag = False
    
    # Thi·∫øt l·∫≠p signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        application = Application.builder().token(Config.BOT_TOKEN).build()

        # Initialize database and Redis
        loop = asyncio.get_event_loop()
        redis_ok = loop.run_until_complete(init_redis())
        
        # T·∫£i c√†i ƒë·∫∑t ng∆∞·ª°ng t·ª´ Redis n·∫øu c√≥
        if redis_ok:
            dup_threshold = loop.run_until_complete(redis_client.get("duplicate_threshold"))
            if dup_threshold:
                try:
                    Config.DUPLICATE_THRESHOLD = float(dup_threshold.decode())
                except (ValueError, AttributeError):
                    pass
                    
            title_threshold = loop.run_until_complete(redis_client.get("title_threshold"))
            if title_threshold:
                try:
                    Config.TITLE_SIMILARITY_THRESHOLD = float(title_threshold.decode())
                except (ValueError, AttributeError):
                    pass
                    
            log_similarity = loop.run_until_complete(redis_client.get("log_similarity_details"))
            if log_similarity:
                try:
                    Config.LOG_SIMILARITY_DETAILS = log_similarity.decode().lower() == "true"
                except (AttributeError):
                    pass
                    
            logger.info(f"C√†i ƒë·∫∑t l·ªçc tin tr√πng: DUPLICATE_THRESHOLD={Config.DUPLICATE_THRESHOLD}, " 
                      f"TITLE_SIMILARITY_THRESHOLD={Config.TITLE_SIMILARITY_THRESHOLD}, "
                      f"LOG_SIMILARITY_DETAILS={Config.LOG_SIMILARITY_DETAILS}")

        if not redis_ok:
            logger.error("Failed to initialize Redis. Exiting.")
            return

        # X√≥a queue c≈© khi kh·ªüi ƒë·ªông l·∫°i
        loop.run_until_complete(clear_news_queues())

        # Add command handlers
        application.add_handler(CommandHandler("start", start_command))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("register", register_user))
        application.add_handler(CommandHandler("keywords", view_keywords_command))
        application.add_handler(CommandHandler("set_keywords", set_keywords_command))
        application.add_handler(CommandHandler("clear_keywords", clear_keywords_command))
        
        # Th√™m l·ªánh qu·∫£n l√Ω c√†i ƒë·∫∑t l·ªçc tin tr√πng
        application.add_handler(CommandHandler("check_dup_settings", check_dup_settings))
        application.add_handler(CommandHandler("set_dup_threshold", set_duplicate_threshold))
        application.add_handler(CommandHandler("set_title_threshold", set_title_threshold))
        application.add_handler(CommandHandler("toggle_sim_log", toggle_similarity_logging))

        # Add callback query handler
        application.add_handler(CallbackQueryHandler(button_callback))

        # Set up the job queue
        job_queue = application.job_queue
        
        # C·∫•u h√¨nh c√°c job ƒë·ªãnh k·ª≥ m·ªõi:
        # 1. Job qu√©t RSS t·∫•t c·∫£ ngu·ªìn m·ªói 5 ph√∫t
        job_queue.run_repeating(fetch_and_cache_news, interval=300, first=10)
        
        # 2. Job g·ª≠i tin t·ª´ queue m·ªói 800s
        job_queue.run_repeating(send_news_from_queue, interval=Config.NEWS_JOB_INTERVAL, first=30)
        
        # 3. Job ping gi·ªØ awake m·ªói 5 ph√∫t
        job_queue.run_repeating(job_ping, interval=300, first=60)
        
        # In th√¥ng tin job
        logger.info(f"ƒê√£ thi·∫øt l·∫≠p 3 job ƒë·ªãnh k·ª≥:\n"
                    f"- Qu√©t RSS & cache: {Config.HOURLY_JOB_INTERVAL}s/l·∫ßn\n"
                    f"- G·ª≠i tin t·ª´ queue: {Config.NEWS_JOB_INTERVAL}s/l·∫ßn\n"
                    f"- Ping gi·ªØ awake: 300s/l·∫ßn")

        if Config.WEBHOOK_URL:
            webhook_port = int(os.environ.get("PORT", 8443))
            logger.info(f"Starting webhook on port {webhook_port} with URL: {Config.WEBHOOK_URL}")
            application.run_webhook(
                listen="0.0.0.0",
                port=webhook_port,
                url_path=Config.BOT_TOKEN,
                webhook_url=f"{Config.WEBHOOK_URL}/{Config.BOT_TOKEN}"
            )
        else:
            logger.info("Starting bot in polling mode")
            application.run_polling(allowed_updates=Update.ALL_TYPES)
            
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c trong h√†m main: {e}")
        # D·ªçn d·∫πp t√†i nguy√™n khi c√≥ l·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(cleanup_resources())
        else:
            loop.run_until_complete(cleanup_resources())
        sys.exit(1)

if __name__ == "__main__":
    main()
