import logging
import os
import asyncio
# Nh√≥m c√°c import th∆∞ vi·ªán b√™n ngo√†i
import feedparser
import httpx
import asyncpg
import redis.asyncio as aioredis
import google.generativeai as genai
# Nh√≥m c√°c import aiogram 
from aiogram import Bot, Dispatcher, types, F
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.webhook.aiohttp_server import SimpleRequestHandler, setup_application
from aiogram.filters import Command
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery
# Nh√≥m c√°c import kh√°c
from aiohttp import web
import re

# --- 1. Config & setup ---
class Config:
    BOT_TOKEN = os.getenv("BOT_TOKEN")
    WEBHOOK_URL = os.getenv("WEBHOOK_URL")
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost")
    DB_URL = os.getenv("DATABASE_URL")
    ADMIN_ID = int(os.getenv("ADMIN_ID", "1225226589"))
    GOOGLE_GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
    OPENROUTER_FALLBACK_MODEL = os.getenv("OPENROUTER_FALLBACK_MODEL", "deepseek/deepseek-chat-v3-0324:free")
    FEED_URLS = [
        "https://news.google.com/rss/search?q=kinh+t%E1%BA%BF&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=ch%E1%BB%A9ng+kho%C3%A1n&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=v%C4%A9+m%C3%B4&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=chi%E1%BA%BFn+tranh&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=l%C3%A3i+su%E1%BA%A5t&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss/search?q=fed&hl=vi&gl=VN&ceid=VN:vi",
        "https://news.google.com/rss?hl=vi&gl=VN&ceid=VN:vi",
    ]
    REDIS_TTL = int(os.getenv("REDIS_TTL", "21600"))  # 6h
    NEWS_JOB_INTERVAL = int(os.getenv("NEWS_JOB_INTERVAL", "600"))  # 10 ph√∫t (gi√¢y)
    DELETE_OLD_NEWS_DAYS = int(os.getenv("DELETE_OLD_NEWS_DAYS", "7"))
    MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))  # S·ªë l·∫ßn th·ª≠ l·∫°i khi feed l·ªói
    MAX_NEWS_PER_CYCLE = int(os.getenv("MAX_NEWS_PER_CYCLE", "3"))  # T·ªëi ƒëa 3 tin m·ªói l·∫ßn

# --- Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng b·∫Øt bu·ªôc ---
REQUIRED_ENV_VARS = ["BOT_TOKEN", "OPENROUTER_API_KEY"]
for var in REQUIRED_ENV_VARS:
    if not os.getenv(var):
        raise RuntimeError(f"Missing required environment variable: {var}")

# --- Logging ---
logging.basicConfig(level=logging.INFO)
bot = Bot(token=Config.BOT_TOKEN)
dp = Dispatcher(storage=MemoryStorage())

# --- Redis ---
redis = None

async def is_sent(entry_id):
    return await redis.sismember("sent_news", entry_id)

async def mark_sent(entry_id):
    await redis.sadd("sent_news", entry_id)
    await redis.expire("sent_news", Config.REDIS_TTL)

# --- PostgreSQL ---
pool = None

async def save_news(entry, ai_summary, sentiment):
    try:
        async with pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO news_insights (title, link, summary, sentiment, ai_opinion)
                VALUES ($1, $2, $3, $4, $5)
                ON CONFLICT (link) DO NOTHING
            """, entry.title, entry.link, entry.summary, sentiment, ai_summary)
    except Exception as e:
        logging.warning(f"L·ªói khi l∆∞u tin t·ª©c v√†o DB (link={entry.link}): {e}")

# --- AI Analysis (Gemini) ---
GEMINI_MODEL = Config.GEMINI_MODEL
OPENROUTER_FALLBACK_MODEL = Config.OPENROUTER_FALLBACK_MODEL
GOOGLE_GEMINI_API_KEY = Config.GOOGLE_GEMINI_API_KEY

async def analyze_news(prompt, model=None):
    try:
        # G·ªçi Google Gemini API ch√≠nh th·ª©c
        genai.configure(api_key=GOOGLE_GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-1.5-pro")
        response = await asyncio.to_thread(model.generate_content, prompt)
        return response.text
    except Exception as e:
        logging.error(f"Gemini API l·ªói: {e}, fallback sang OpenRouter {OPENROUTER_FALLBACK_MODEL}")
        # Fallback sang OpenRouter
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}",},
                    json={
                        "model": OPENROUTER_FALLBACK_MODEL,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7
                    }
                )
                result = response.json()
                if "choices" not in result:
                    logging.error(f"OpenRouter API error (model={OPENROUTER_FALLBACK_MODEL}): {result}")
                    raise RuntimeError(f"OpenRouter API error: {result}")
                return result["choices"][0]["message"]["content"]
        except Exception as e2:
            logging.error(f"OpenRouter fallback c≈©ng l·ªói: {e2}")
            raise e2

# --- Extract sentiment from AI result ---
def extract_sentiment(ai_summary):
    """Extract sentiment from AI summary"""
    sentiment = "Trung l·∫≠p"  # Default
    try:
        for line in ai_summary.splitlines():
            if "C·∫£m x√∫c:" in line:
                sentiment_text = line.split(":")[-1].strip().lower()
                if "t√≠ch c·ª±c" in sentiment_text:
                    return "T√≠ch c·ª±c"
                elif "ti√™u c·ª±c" in sentiment_text:
                    return "Ti√™u c·ª±c"
                else:
                    return "Trung l·∫≠p"
    except Exception as e:
        logging.warning(f"L·ªói khi parse sentiment: {e}")
    return sentiment

# --- Parse RSS Feed ---
async def parse_feed(url):
    """Parse RSS feed with error handling and retries"""
    for attempt in range(Config.MAX_RETRIES):
        try:
            feed = feedparser.parse(url)
            if not feed.entries and not hasattr(feed, 'status'):
                raise Exception("Empty feed without status")
            return feed
        except Exception as e:
            logging.warning(f"Error parsing feed {url}, attempt {attempt+1}/{Config.MAX_RETRIES}: {e}")
            if attempt < Config.MAX_RETRIES - 1:
                await asyncio.sleep(1)  # Short delay before retry
            else:
                logging.error(f"Failed to parse feed after {Config.MAX_RETRIES} attempts: {url}")
                return feedparser.FeedParserDict(entries=[])  # Return empty feed
    
# --- L·ªánh ƒëƒÉng k√Ω user ---
@dp.message(Command("register"))
async def register_user(msg: types.Message):
    user_id = msg.from_user.id
    username = msg.from_user.username or ""
    async with pool.acquire() as conn:
        user = await conn.fetchrow("SELECT * FROM subscribed_users WHERE user_id=$1", user_id)
        if user:
            if user["is_approved"]:
                await msg.answer("B·∫°n ƒë√£ ƒë∆∞·ª£c duy·ªát v√† s·∫Ω nh·∫≠n tin t·ª©c!")
            else:
                await msg.answer("B·∫°n ƒë√£ ƒëƒÉng k√Ω, vui l√≤ng ch·ªù admin duy·ªát!")
            return
        await conn.execute(
            "INSERT INTO subscribed_users (user_id, username, is_approved) VALUES ($1, $2, FALSE) ON CONFLICT (user_id) DO NOTHING",
            user_id, username
        )
    # G·ª≠i th√¥ng b√°o cho admin
    kb = InlineKeyboardMarkup(
        inline_keyboard=[[InlineKeyboardButton(text="Duy·ªát user n√†y", callback_data=f"approve_{user_id}")]]
    )
    await bot.send_message(
        Config.ADMIN_ID,
        f"Y√™u c·∫ßu duy·ªát user m·ªõi: @{username} (ID: {user_id})",
        reply_markup=kb
    )
    await msg.answer("ƒê√£ g·ª≠i y√™u c·∫ßu ƒëƒÉng k√Ω, vui l√≤ng ch·ªù admin duy·ªát!")

# --- X·ª≠ l√Ω callback admin duy·ªát user ---
@dp.callback_query(F.data.startswith("approve_"))
async def approve_user_callback(cb: CallbackQuery):
    if cb.from_user.id != Config.ADMIN_ID:
        await cb.answer("Ch·ªâ admin m·ªõi ƒë∆∞·ª£c duy·ªát!", show_alert=True)
        return
    user_id = int(cb.data.split("_")[1])
    async with pool.acquire() as conn:
        await conn.execute("UPDATE subscribed_users SET is_approved=TRUE WHERE user_id=$1", user_id)
        user = await conn.fetchrow("SELECT username FROM subscribed_users WHERE user_id=$1", user_id)
    await bot.send_message(user_id, "B·∫°n ƒë√£ ƒë∆∞·ª£c admin duy·ªát, s·∫Ω nh·∫≠n tin t·ª©c t·ª´ bot!")
    await cb.answer(f"ƒê√£ duy·ªát user @{user['username']} ({user_id})")

# --- Tin t·ª©c theo chu k·ª≥ ---
async def is_in_db(entry):
    async with pool.acquire() as conn:
        row = await conn.fetchrow("SELECT 1 FROM news_insights WHERE link=$1", entry.link)
        return row is not None

async def news_job():
    while True:
        await delete_old_news(days=Config.DELETE_OLD_NEWS_DAYS)
        new_entries = []
        cached_results = {}
        entries_to_analyze = []

        try:
            # Collect entries from feeds
            for url in Config.FEED_URLS:
                feed = await parse_feed(url)
                for entry in feed.entries:
                    if await is_sent(entry.id) or await is_in_db(entry):
                        continue
                    await mark_sent(entry.id)
                    cache_key = f"ai_summary:{entry.id}"
                    cached = await redis.get(cache_key)
                    if cached:
                        cached_results[entry.id] = cached.decode()
                    else:
                        entries_to_analyze.append(entry)
                    new_entries.append(entry)
                    if len(new_entries) >= Config.MAX_NEWS_PER_CYCLE:
                        break
                if len(new_entries) >= Config.MAX_NEWS_PER_CYCLE:
                    break

            # G·ªçi Gemini cho c√°c entry ch∆∞a c√≥ cache
            ai_results = {}
            if entries_to_analyze:
                prompt = "ƒê√¢y l√† c√°c tin t·ª©c t√†i ch√≠nh:\n"
                for idx, entry in enumerate(entries_to_analyze, 1):
                    prompt += f"\n---\nTin s·ªë {idx}:\n{entry.title}\n{entry.summary}\n"
                prompt += '''
H√£y v·ªõi m·ªói tin:
1. T√≥m t·∫Øt ng·∫Øn g·ªçn (d∆∞·ªõi 2 c√¢u)
2. ƒê∆∞a ra nh·∫≠n ƒë·ªãnh th·ªã tr∆∞·ªùng ng·∫Øn g·ªçn (d∆∞·ªõi 2 c√¢u)
3. Ph√¢n t√≠ch c·∫£m x√∫c: t√≠ch c·ª±c / ti√™u c·ª±c / trung l·∫≠p.
Tr·∫£ v·ªÅ k·∫øt qu·∫£ cho t·ª´ng tin theo ƒë·ªãnh d·∫°ng:
- Tin s·ªë X:
  - T√≥m t·∫Øt:
  - Nh·∫≠n ƒë·ªãnh:
  - C·∫£m x√∫c:
'''
                ai_result = await analyze_news(prompt)
                
                try:
                    # Parse k·∫øt qu·∫£ t·ª´ Gemini
                    results = re.split(r"- Tin s·ªë \d+:", ai_result)[1:]  # S·ª≠a regex
                    
                    # Check if we have enough results for all entries
                    if len(results) >= len(entries_to_analyze):
                        for entry, idx, result in zip(entries_to_analyze, range(1, len(entries_to_analyze)+1), results):
                            ai_summary = f"- Tin s·ªë {idx}:{result.strip()}"
                            ai_results[entry.id] = ai_summary
                            await redis.set(f"ai_summary:{entry.id}", ai_summary, ex=Config.REDIS_TTL)
                    else:
                        logging.warning(f"Gemini tr·∫£ v·ªÅ kh√¥ng ƒë·ªß k·∫øt qu·∫£: {len(results)} results for {len(entries_to_analyze)} entries")
                        # Backup: t·∫°o k·∫øt qu·∫£ tr·ªëng cho m·ªçi entry ch∆∞a ph√¢n t√≠ch
                        for entry in entries_to_analyze:
                            if entry.id not in ai_results:
                                ai_results[entry.id] = f"- Tin s·ªë 0:\n  - T√≥m t·∫Øt: {entry.title}\n  - Nh·∫≠n ƒë·ªãnh: Kh√¥ng c√≥ ƒë·ªß th√¥ng tin\n  - C·∫£m x√∫c: Trung l·∫≠p"
                except Exception as e:
                    logging.error(f"L·ªói khi parse k·∫øt qu·∫£ Gemini: {e}, k·∫øt qu·∫£: {ai_result}")
                    # T·∫°o k·∫øt qu·∫£ tr·ªëng cho m·ªçi entry ch∆∞a ph√¢n t√≠ch
                    for entry in entries_to_analyze:
                        ai_results[entry.id] = f"- Tin s·ªë 0:\n  - T√≥m t·∫Øt: {entry.title}\n  - Nh·∫≠n ƒë·ªãnh: L·ªói ph√¢n t√≠ch\n  - C·∫£m x√∫c: Trung l·∫≠p"

            # G·ª≠i v√† l∆∞u DB cho t·∫•t c·∫£ entry
            users_to_notify = []
            async with pool.acquire() as conn:
                rows = await conn.fetch("SELECT user_id FROM subscribed_users WHERE is_approved=TRUE")
                users_to_notify = [row["user_id"] for row in rows]
            
            for entry in new_entries:
                if entry.id in cached_results:
                    ai_summary = cached_results[entry.id]
                elif entry.id in ai_results:
                    ai_summary = ai_results[entry.id]
                else:
                    continue  # Kh√¥ng c√≥ k·∫øt qu·∫£ AI

                sentiment = extract_sentiment(ai_summary)
                await save_news(entry, ai_summary, sentiment)
                
                message = f"üì∞ *{entry.title}*\n{entry.link}\n\nü§ñ *Gemini AI ph√¢n t√≠ch:*\n{ai_summary}"
                
                # Send to all users (in parallel using gather)
                sending_tasks = []
                for user_id in users_to_notify:
                    sending_tasks.append(send_message_to_user(user_id, message, entry=entry))
                if sending_tasks:
                    await asyncio.gather(*sending_tasks, return_exceptions=True)
                
        except Exception as e:
            logging.error(f"L·ªói trong chu k·ª≥ news_job: {e}")
            
        await asyncio.sleep(Config.NEWS_JOB_INTERVAL)

async def send_message_to_user(user_id, message, entry=None):
    """Send message to user with error handling, k√®m ·∫£nh n·∫øu c√≥"""
    try:
        image_url = extract_image_url(entry) if entry else None
        if image_url:
            await bot.send_photo(user_id, image_url, caption=message, parse_mode="Markdown")
        else:
            await bot.send_message(user_id, message, parse_mode="Markdown")
    except Exception as e:
        logging.warning(f"Kh√¥ng g·ª≠i ƒë∆∞·ª£c tin cho user {user_id}: {e}")

# --- Webhook setup ---
# (Kh√¥ng ƒëƒÉng k√Ω b·∫•t k·ª≥ handler n√†o cho l·ªánh t·ª´ user)
async def init_db():
    async with pool.acquire() as conn:
        # T·∫°o b·∫£ng news_insights n·∫øu ch∆∞a c√≥
        await conn.execute('''
        CREATE TABLE IF NOT EXISTS news_insights (
            id SERIAL PRIMARY KEY,
            title TEXT,
            link TEXT UNIQUE,
            summary TEXT,
            sentiment TEXT,
            ai_opinion TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        );
        ''')
        # ƒê·∫£m b·∫£o c·ªôt created_at t·ªìn t·∫°i (n·∫øu migrate t·ª´ b·∫£n c≈©)
        await conn.execute('''
        ALTER TABLE news_insights ADD COLUMN IF NOT EXISTS created_at TIMESTAMP DEFAULT NOW();
        ''')
        # T·∫°o b·∫£ng subscribed_users n·∫øu ch∆∞a c√≥
        await conn.execute('''
        CREATE TABLE IF NOT EXISTS subscribed_users (
            user_id BIGINT PRIMARY KEY,
            username TEXT,
            is_approved BOOLEAN DEFAULT FALSE
        );
        ''')

# H√†m x√≥a tin c≈© h∆°n n ng√†y
async def delete_old_news(days=Config.DELETE_OLD_NEWS_DAYS):
    try:
        async with pool.acquire() as conn:
            await conn.execute(
                f"DELETE FROM news_insights WHERE created_at < NOW() - INTERVAL '{days} days';"
            )
    except Exception as e:
        logging.error(f"L·ªói khi x√≥a tin c≈©: {e}")

# --- 8. Webhook & main ---
async def on_startup(app):
    try:
        global redis, pool
        logging.info("Bot kh·ªüi ƒë·ªông, thi·∫øt l·∫≠p k·∫øt n·ªëi Redis...")
        redis = await aioredis.from_url(Config.REDIS_URL)
        logging.info("Thi·∫øt l·∫≠p k·∫øt n·ªëi PostgreSQL...")
        pool = await asyncpg.create_pool(dsn=Config.DB_URL)
        logging.info("Kh·ªüi t·∫°o database...")
        await init_db()
        logging.info(f"Thi·∫øt l·∫≠p webhook: {Config.WEBHOOK_URL}")
        await bot.delete_webhook() # X√≥a webhook c≈© n·∫øu c√≥
        result = await bot.set_webhook(Config.WEBHOOK_URL)
        logging.info(f"K·∫øt qu·∫£ thi·∫øt l·∫≠p webhook: {result}")
        
        # Ki·ªÉm tra webhook ƒë√£ set ƒë√∫ng ch∆∞a
        webhook_info = await bot.get_webhook_info()
        logging.info(f"WebhookInfo: URL={webhook_info.url}, pending_updates={webhook_info.pending_update_count}")
        
        logging.info("Kh·ªüi ƒë·ªông task g·ª≠i tin...")
        asyncio.create_task(news_job())
        logging.info("Bot ƒë√£ s·∫µn s√†ng ho·∫°t ƒë·ªông!")
    except Exception as e:
        logging.error(f"L·ªói trong on_startup: {e}")
        raise e

async def on_shutdown(app):
    logging.info("Bot ƒëang t·∫Øt...")
    await bot.delete_webhook()
    if pool:
        await pool.close()
    if redis:
        await redis.close()
    logging.info("Bot ƒë√£ t·∫Øt ho√†n to√†n.")

# Route cho healthcheck
async def healthcheck(request):
    return web.Response(text="Bot ƒëang ho·∫°t ƒë·ªông!", status=200)

app = web.Application()
app.on_startup.append(on_startup)
app.on_shutdown.append(on_shutdown)
app.router.add_get("/", healthcheck)  # Th√™m route / ƒë·ªÉ ki·ªÉm tra bot s·ªëng
SimpleRequestHandler(dispatcher=dp, bot=bot).register(app, path="/webhook")
setup_application(app, dp, bot=bot)

if __name__ == "__main__":
    logging.info("Kh·ªüi ƒë·ªông web server...")
    web.run_app(app, host="0.0.0.0", port=8000)

def extract_image_url(entry):
    # 1. RSS chu·∫©n c√≥ th·ªÉ c√≥ media_content
    if hasattr(entry, 'media_content') and entry.media_content:
        return entry.media_content[0].get('url')
    # 2. RSS c√≥ th·ªÉ c√≥ media_thumbnail
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        return entry.media_thumbnail[0].get('url')
    # 3. T√¨m ·∫£nh trong summary (n·∫øu c√≥ th·∫ª <img>)
    match = re.search(r'<img[^>]+src=["\"]([^"\"]+)["\"]', getattr(entry, 'summary', ''))
    if match:
        return match.group(1)
    return None

@dp.message(Command("start"))
async def start_command(msg: types.Message):
    await msg.answer(
        "Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi bot tin t·ª©c t√†i ch√≠nh!\n" \
        "- G√µ /register ƒë·ªÉ ƒëƒÉng k√Ω nh·∫≠n tin t·ª©c.\n" \
        "- Sau khi ƒë∆∞·ª£c admin duy·ªát, b·∫°n s·∫Ω nh·∫≠n tin t·ª©c t·ª± ƒë·ªông.\n" \
        "- G√µ /help ƒë·ªÉ xem h∆∞·ªõng d·∫´n."
    )
