#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Bot Ch·ª©ng Kho√°n To√†n Di·ªán Phi√™n B·∫£n V18.8.1T (N√¢ng c·∫•p t·∫£i d·ªØ li·ªáu):
- T·ªëi ∆∞u h√≥a t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu, t·ª± ƒë·ªông l√†m s·∫°ch v√† s·ª≠a l·ªói d·ªØ li·ªáu
- H·ªá th·ªëng ki·ªÉm so√°t ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu t·ª± ƒë·ªông v·ªõi nhi·ªÅu ti√™u ch√≠
- C·∫≠p nh·∫≠t d·ªØ li·ªáu gia tƒÉng gi·∫£m t·∫£i h·ªá th·ªëng v√† bƒÉng th√¥ng
- T·ª± ƒë·ªông ph√°t hi·ªán v√† x·ª≠ l√Ω ngo·∫°i lai, d·ªØ li·ªáu b·ªã thi·∫øu
- H·ªá th·ªëng t·∫°o ƒë·∫∑c tr∆∞ng ph√°i sinh t·ª± ƒë·ªông cho ph√¢n t√≠ch k·ªπ thu·∫≠t
- S·ª≠ d·ª•ng m√¥ h√¨nh deepseek/deepseek-chat-v3-0324:free
- ƒê·∫£m b·∫£o c√°c ch·ª©c nƒÉng v√† c√¥ng ngh·ªá hi·ªán c√≥ kh√¥ng b·ªã ·∫£nh h∆∞·ªüng
"""

import os
import sys
import io
import logging
import pickle
from datetime import datetime, timedelta
import asyncio
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import numpy as np
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

import yfinance as yf
from vnstock import Vnstock
import google.generativeai as genai

from ta import trend, momentum, volatility
from ta.volume import MFIIndicator
import feedparser

import redis.asyncio as redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import Column, Integer, String, Float, Text, DateTime, select, LargeBinary

import xgboost as xgb
from sklearn.metrics import accuracy_score
from prophet import Prophet

import matplotlib.pyplot as plt
import holidays
import html

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from tenacity import retry, stop_after_attempt, wait_exponential

import aiohttp
import json
from timestamp_aligner import TimestampAligner

# ---------- C·∫§U H√åNH & LOGGING ----------
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
ADMIN_ID = os.getenv("ADMIN_ID", "1225226589")
PORT = int(os.environ.get("PORT", 10000))
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
RENDER_SERVICE_NAME = os.getenv("RENDER_SERVICE_NAME", "")

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

CACHE_EXPIRE_SHORT = 1800
CACHE_EXPIRE_MEDIUM = 3600
CACHE_EXPIRE_LONG = 86400
NEWS_CACHE_EXPIRE = 900
DEFAULT_CANDLES = 100
DEFAULT_TIMEFRAME = '1D'

executor = ThreadPoolExecutor(max_workers=5)

async def run_in_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, lambda: func(*args, **kwargs))

# ---------- K·∫æT N·ªêI REDIS (Async) ----------
class RedisManager:
    def __init__(self):
        try:
            self.redis_client = redis.from_url(REDIS_URL)
            logger.info("K·∫øt n·ªëi Redis th√†nh c√¥ng.")
        except Exception as e:
            logger.error(f"L·ªói k·∫øt n·ªëi Redis: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def set(self, key, value, expire):
        try:
            serialized_value = pickle.dumps(value)
            await self.redis_client.set(key, serialized_value, ex=expire)
            return True
        except Exception as e:
            logger.error(f"L·ªói Redis set: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def get(self, key):
        try:
            data = await self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            logger.error(f"L·ªói Redis get: {str(e)}")
            return None

redis_manager = RedisManager()

# ---------- K·∫æT N·ªêI POSTGRESQL (Async) ----------
Base = declarative_base()

class ApprovedUser(Base):
    __tablename__ = 'approved_users'
    id = Column(Integer, primary_key=True)
    user_id = Column(String, unique=True, nullable=False)
    approved_at = Column(DateTime, default=datetime.now)

class ReportHistory(Base):
    __tablename__ = 'report_history'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    date = Column(String, nullable=False)
    report = Column(Text, nullable=False)
    close_today = Column(Float, nullable=False)
    close_yesterday = Column(Float, nullable=False)
    timestamp = Column(DateTime, default=datetime.now)

class TrainedModel(Base):
    __tablename__ = 'trained_models'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    model_type = Column(String, nullable=False)
    model_blob = Column(LargeBinary, nullable=False)
    created_at = Column(DateTime, default=datetime.now)
    performance = Column(Float, nullable=True)

engine = create_async_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine, class_=AsyncSession, expire_on_commit=False)

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

class DBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def is_user_approved(self, user_id) -> bool:
        try:
            async with self.Session() as session:
                result = await session.execute(select(ApprovedUser).filter_by(user_id=str(user_id)))
                return result.scalar_one_or_none() is not None or str(user_id) == ADMIN_ID
        except Exception as e:
            logger.error(f"L·ªói ki·ªÉm tra ng∆∞·ªùi d√πng: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def add_approved_user(self, user_id, approved_at=None) -> None:
        try:
            async with self.Session() as session:
                if not await self.is_user_approved(user_id) and str(user_id) != ADMIN_ID:
                    new_user = ApprovedUser(user_id=str(user_id), approved_at=approved_at or datetime.now())
                    session.add(new_user)
                    await session.commit()
                    logger.info(f"Th√™m ng∆∞·ªùi d√πng ƒë∆∞·ª£c ph√™ duy·ªát: {user_id}")
        except Exception as e:
            logger.error(f"L·ªói th√™m ng∆∞·ªùi d√πng: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_report_history(self, symbol: str) -> list:
        try:
            async with self.Session() as session:
                reports = await session.execute(select(ReportHistory).filter_by(symbol=symbol).order_by(ReportHistory.id.asc()))
                reports = reports.scalars().all()
                return [
                    {
                        "id": report.id,
                        "symbol": report.symbol,
                        "date": report.date,
                        "report": report.report,
                        "close_today": report.close_today,
                        "close_yesterday": report.close_yesterday,
                        "timestamp": report.timestamp.isoformat()
                    }
                    for report in reports
                ]
        except Exception as e:
            logger.error(f"L·ªói t·∫£i l·ªãch s·ª≠ b√°o c√°o: {str(e)}")
            return []

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def save_report_history(self, symbol: str, report: str, close_today: float, close_yesterday: float) -> None:
        try:
            async with self.Session() as session:
                date_str = datetime.now().strftime('%Y-%m-%d')
                standardized_data = standardize_data_for_db({
                    'symbol': symbol,
                    'date': date_str,
                    'report': report,
                    'close_today': close_today,
                    'close_yesterday': close_yesterday
                })
                new_report = ReportHistory(**standardized_data)
                session.add(new_report)
                await session.commit()
                logger.info(f"L∆∞u b√°o c√°o m·ªõi cho {symbol}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u b√°o c√°o: {str(e)}")
            raise

db = DBManager()

# ---------- QU·∫¢N L√ù M√î H√åNH (Prophet & XGBoost) ----------
class ModelDBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def store_trained_model(self, symbol: str, model_type: str, model, performance: float = None):
        try:
            model_blob = pickle.dumps(model)
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                existing = result.scalar_one_or_none()
                if existing:
                    existing.model_blob = model_blob
                    existing.created_at = datetime.now()
                    existing.performance = performance
                else:
                    new_model = TrainedModel(symbol=symbol, model_type=model_type, model_blob=model_blob, performance=performance)
                    session.add(new_model)
                await session.commit()
            logger.info(f"L∆∞u m√¥ h√¨nh {model_type} cho {symbol} th√†nh c√¥ng v·ªõi hi·ªáu su·∫•t: {performance}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_trained_model(self, symbol: str, model_type: str):
        try:
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                model_record = result.scalar_one_or_none()
                if model_record:
                    model = pickle.loads(model_record.model_blob)
                    return model, model_record.performance
            return None, None
        except Exception as e:
            logger.error(f"L·ªói t·∫£i m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")
            return None, None

model_db_manager = ModelDBManager()

# ---------- H√ÄM H·ªñ TR·ª¢ ----------
def is_index(symbol: str) -> bool:
    indices = ['VNINDEX', 'VN30', 'HNX30', 'HNXINDEX', 'UPCOM']
    return symbol.upper() in indices

async def is_user_approved(user_id) -> bool:
    return await db.is_user_approved(user_id)

def standardize_data_for_db(data: dict) -> dict:
    standardized_data = {}
    for key, value in data.items():
        if isinstance(value, np.float64):
            standardized_data[key] = float(value)
        elif isinstance(value, np.int64):
            standardized_data[key] = int(value)
        elif isinstance(value, pd.Timestamp):
            standardized_data[key] = value.to_pydatetime()
        else:
            standardized_data[key] = value
    return standardized_data

# ---------- H√ÄM H·ªñ TR·ª¢: L·ªåC NG√ÄY GIAO D·ªäCH -----------
def filter_trading_days(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    df = df[df.index.weekday < 5]
    years = df.index.year.unique()
    vn_holidays = holidays.Vietnam(years=years)
    holiday_dates = set(vn_holidays.keys())
    df = df[~pd.to_datetime(df.index.date).isin(holiday_dates)]
    return df

# ---------- T·∫¢I D·ªÆ LI·ªÜU (N√ÇNG C·∫§P V18.8.1T) ----------
class DataLoader:
    def __init__(self, primary_source: str = 'vnstock', backup_sources: list = None):
        self.primary_source = primary_source
        self.backup_sources = backup_sources or ['yahoo']
        self.data_quality_metrics = {}
        self.source_reliability = {
            'vnstock': 1.0,
            'yahoo': 0.8
        }
        # Kh·ªüi t·∫°o b·ªô cƒÉn ch·ªânh timestamp
        self.timestamp_aligner = TimestampAligner(exchange_timezone='Asia/Bangkok')
        
    def _get_data_source_priorities(self):
        """Tr·∫£ v·ªÅ danh s√°ch c√°c ngu·ªìn d·ªØ li·ªáu theo th·ª© t·ª± ∆∞u ti√™n."""
        sources = [self.primary_source] + [s for s in self.backup_sources if s != self.primary_source]
        return sources
        
    def detect_outliers(self, df: pd.DataFrame, method: str = 'zscore', threshold: float = 3.0) -> (pd.DataFrame, str):
        if 'close' not in df.columns:
            return df, "Kh√¥ng c√≥ c·ªôt 'close' ƒë·ªÉ ph√°t hi·ªán outlier"
            
        if method == 'zscore':
            z_scores = np.abs((df['close'] - df['close'].mean()) / df['close'].std())
            df['is_outlier'] = z_scores > threshold
            outliers = df[df['is_outlier']]
            
            # Ghi l·∫°i b√°o c√°o chi ti·∫øt
            outlier_report = f"Ph√°t hi·ªán {len(outliers)} gi√° tr·ªã b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu:\n"
            for idx, row in outliers.iterrows():
                outlier_report += f"- {idx.strftime('%Y-%m-%d')}: {row['close']:.2f}\n"
                
            return df, outlier_report if not outliers.empty else "Kh√¥ng c√≥ gi√° tr·ªã b·∫•t th∆∞·ªùng"
        
        elif method == 'iqr':
            # Ph∆∞∆°ng ph√°p ph√°t hi·ªán ngo·∫°i lai d·ª±a tr√™n IQR
            Q1 = df['close'].quantile(0.25)
            Q3 = df['close'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            df['is_outlier'] = (df['close'] < lower_bound) | (df['close'] > upper_bound)
            outliers = df[df['is_outlier']]
            
            outlier_report = f"Ph√°t hi·ªán {len(outliers)} gi√° tr·ªã b·∫•t th∆∞·ªùng (IQR) trong d·ªØ li·ªáu:\n"
            for idx, row in outliers.iterrows():
                outlier_report += f"- {idx.strftime('%Y-%m-%d')}: {row['close']:.2f}\n"
                
            return df, outlier_report if not outliers.empty else "Kh√¥ng c√≥ gi√° tr·ªã b·∫•t th∆∞·ªùng (IQR)"
        
        return df, "Ph∆∞∆°ng ph√°p ph√°t hi·ªán ngo·∫°i lai kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£"

    def handle_missing_values(self, df: pd.DataFrame, method: str = 'linear') -> pd.DataFrame:
        """X·ª≠ l√Ω c√°c gi√° tr·ªã c√≤n thi·∫øu trong d·ªØ li·ªáu chu·ªói th·ªùi gian."""
        if df.empty:
            return df
            
        # Ki·ªÉm tra gi√° tr·ªã c√≤n thi·∫øu
        missing_count = df.isna().sum().sum()
        if missing_count == 0:
            return df
            
        # Th√™m c·ªù ƒë√°nh d·∫•u d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅn
        df['is_imputed'] = False
        
        # X·ª≠ l√Ω t·ª´ng c·ªôt
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns and df[col].isna().any():
                missing_indices = df[col].isna()
                
                if method == 'linear':
                    df.loc[missing_indices, col] = df[col].interpolate(method='linear')
                elif method == 'ffill':
                    df.loc[missing_indices, col] = df[col].ffill()
                elif method == 'bfill':
                    df.loc[missing_indices, col] = df[col].bfill()
                elif method == 'mean':
                    df.loc[missing_indices, col] = df[col].fillna(df[col].mean())
                    
                # ƒê√°nh d·∫•u c√°c d√≤ng ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅn
                df.loc[missing_indices, 'is_imputed'] = True
                
        # Ghi log k·∫øt qu·∫£ x·ª≠ l√Ω
        if missing_count > 0:
            logger.info(f"ƒê√£ x·ª≠ l√Ω {missing_count} gi√° tr·ªã c√≤n thi·∫øu b·∫±ng ph∆∞∆°ng ph√°p {method}")
            
        return df
    
    def standardize_dataframe(self, df: pd.DataFrame, required_columns: list = None) -> pd.DataFrame:
        """Chu·∫©n h√≥a DataFrame ƒë·∫£m b·∫£o c·∫•u tr√∫c nh·∫•t qu√°n."""
        required_columns = required_columns or ['open', 'high', 'low', 'close', 'volume']
        
        # Chu·∫©n h√≥a t√™n c·ªôt
        column_mapping = {
            'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume',
            'time': 'date', 'Time': 'date', 'Date': 'date', 'Datetime': 'date'
        }
        
        df = df.rename(columns={col: column_mapping.get(col, col) for col in df.columns})
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß c·ªôt c·∫ßn thi·∫øt
        for col in required_columns:
            if col not in df.columns:
                if col == 'volume':
                    df[col] = 0  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh cho volume
                else:
                    raise ValueError(f"D·ªØ li·ªáu thi·∫øu c·ªôt b·∫Øt bu·ªôc: {col}")
        
        # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu n·∫øu c·∫ßn
        for col in ['open', 'high', 'low', 'close']:
            if col in df.columns:
                df[col] = df[col].astype('float32')
                
        if 'volume' in df.columns:
            df['volume'] = df['volume'].astype('float32')
            
        # Chu·∫©n h√≥a index
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df = df.set_index('date')
                
        # S·∫Øp x·∫øp theo th·ªùi gian
        df = df.sort_index()
        
        return df
    
    def validate_price_data(self, df: pd.DataFrame) -> (bool, str):
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu gi√°."""
        if df.empty:
            return False, "DataFrame r·ªóng"
            
        validation_errors = []
        
        # Ki·ªÉm tra gi√° high >= gi√° low
        if not (df['high'] >= df['low']).all():
            invalid_rows = df[df['high'] < df['low']]
            validation_errors.append(f"Ph√°t hi·ªán {len(invalid_rows)} d√≤ng c√≥ gi√° high < gi√° low")
            
        # Ki·ªÉm tra gi√° close n·∫±m trong kho·∫£ng high-low
        if not ((df['close'] >= df['low']) & (df['close'] <= df['high'])).all():
            invalid_rows = df[~((df['close'] >= df['low']) & (df['close'] <= df['high']))]
            validation_errors.append(f"Ph√°t hi·ªán {len(invalid_rows)} d√≤ng c√≥ gi√° close n·∫±m ngo√†i kho·∫£ng high-low")
            
        # Ki·ªÉm tra volume kh√¥ng √¢m
        if 'volume' in df.columns and (df['volume'] < 0).any():
            invalid_rows = df[df['volume'] < 0]
            validation_errors.append(f"Ph√°t hi·ªán {len(invalid_rows)} d√≤ng c√≥ volume √¢m")
            
        if validation_errors:
            return False, "\n".join(validation_errors)
            
        return True, "D·ªØ li·ªáu gi√° h·ª£p l·ªá"

    async def load_data(self, symbol: str, timeframe: str, num_candles: int) -> (pd.DataFrame, str):
        """T·∫£i d·ªØ li·ªáu t·ª´ ngu·ªìn ch√≠nh, n·∫øu th·∫•t b·∫°i s·∫Ω d√πng ngu·ªìn d·ª± ph√≤ng."""
        timeframe_map = {'1d': '1D', '1w': '1W', '1mo': '1M'}
        timeframe = timeframe_map.get(timeframe.lower(), timeframe).upper()
        
        expire = CACHE_EXPIRE_SHORT if timeframe == '1D' else CACHE_EXPIRE_MEDIUM if timeframe == '1W' else CACHE_EXPIRE_LONG
        
        # Ki·ªÉm tra cache
        cache_key = f"data_{self.primary_source}_{symbol}_{timeframe}_{num_candles}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data, "D·ªØ li·ªáu t·ª´ cache, kh√¥ng ki·ªÉm tra outlier"

        # Th·ª≠ t·∫£i d·ªØ li·ªáu l·∫ßn l∆∞·ª£t t·ª´ c√°c ngu·ªìn theo th·ª© t·ª± ∆∞u ti√™n
        sources = self._get_data_source_priorities()
        last_error = None
        
        for source in sources:
            try:
                logger.info(f"ƒêang t·∫£i d·ªØ li·ªáu cho {symbol} t·ª´ ngu·ªìn {source}...")
                
                if source == 'vnstock':
                    df = await self._load_from_vnstock(symbol, timeframe, num_candles)
                elif source == 'yahoo':
                    df = await self._load_from_yahoo(symbol, timeframe, num_candles)
                else:
                    logger.warning(f"Ngu·ªìn d·ªØ li·ªáu kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {source}")
                    continue
                
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                df = self.standardize_dataframe(df)
                
                # CƒÉn ch·ªânh timestamp ch√≠nh x√°c
                df = self.timestamp_aligner.fix_timestamp_issues(df)
                df = self.timestamp_aligner.standardize_timeframe(df, freq=timeframe)
                
                # Ki·ªÉm tra t√≠nh h·ª£p l·ªá
                is_valid, validation_msg = self.validate_price_data(df)
                if not is_valid:
                    logger.warning(f"D·ªØ li·ªáu t·ª´ {source} kh√¥ng h·ª£p l·ªá: {validation_msg}")
                    continue
                
                # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu 
                df = self.handle_missing_values(df)
                
                # L·ªçc ng√†y giao d·ªãch 
                df = self.timestamp_aligner.filter_trading_days(df)
                
                # Ph√°t hi·ªán ngo·∫°i lai
                df, outlier_report = self.detect_outliers(df)
                
                # L∆∞u v√†o cache
                await redis_manager.set(cache_key, df, expire=expire)
                
                # T·ªëi ∆∞u b·ªô nh·ªõ b·∫±ng c√°ch chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
                for col in df.select_dtypes(include=['float64']).columns:
                    df[col] = df[col].astype('float32')
                
                # C·∫≠p nh·∫≠t ƒë·ªô tin c·∫≠y c·ªßa ngu·ªìn
                self.source_reliability[source] = min(1.0, self.source_reliability.get(source, 0.5) + 0.1)
                
                return df, outlier_report
                
            except Exception as e:
                last_error = str(e)
                logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu t·ª´ {source} cho {symbol}: {last_error}")
                # Gi·∫£m ƒë·ªô tin c·∫≠y c·ªßa ngu·ªìn n√†y 
                self.source_reliability[source] = max(0.1, self.source_reliability.get(source, 0.5) - 0.1)
        
        # N·∫øu t·∫•t c·∫£ c√°c ngu·ªìn ƒë·ªÅu th·∫•t b·∫°i
        raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho {symbol} t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o: {last_error}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def _load_from_vnstock(self, symbol: str, timeframe: str, num_candles: int) -> pd.DataFrame:
        """T·∫£i d·ªØ li·ªáu t·ª´ VNStock."""
        def fetch_vnstock():
            stock = Vnstock().stock(symbol=symbol, source='TCBS')
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=(num_candles + 1) * 3)).strftime('%Y-%m-%d')
            df = stock.quote.history(start=start_date, end=end_date, interval=timeframe)
            
            if df is None or df.empty or len(df) < 20:
                raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {'ch·ªâ s·ªë' if is_index(symbol) else 'm√£'} {symbol}")
                
            df = df.rename(columns={'time': 'date', 'open': 'open', 'high': 'high',
                                     'low': 'low', 'close': 'close', 'volume': 'volume'})
            df['date'] = pd.to_datetime(df['date'])
            df = df.set_index('date')
            
            # Th√™m m√∫i gi·ªù cho index n·∫øu ch∆∞a c√≥
            if df.index.tz is None:
                df.index = df.index.tz_localize('Asia/Bangkok')
                
            df = df[['open', 'high', 'low', 'close', 'volume']].dropna()
            
            return df.tail(num_candles + 1)
            
        return await run_in_thread(fetch_vnstock)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8), reraise=True)
    async def _load_from_yahoo(self, symbol: str, timeframe: str, num_candles: int) -> pd.DataFrame:
        """T·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance."""
        period_map = {'1D': 'd', '1W': 'wk', '1M': 'mo'}
        period = period_map.get(timeframe, 'd')
        
        try:
            async with aiohttp.ClientSession() as session:
                start_ts = int((datetime.now() - timedelta(days=num_candles * 3)).timestamp())
                end_ts = int(datetime.now().timestamp())
                url = (f"https://query1.finance.yahoo.com/v7/finance/download/{symbol}"
                       f"?period1={start_ts}&period2={end_ts}&interval=1{period}&events=history")
                async with asyncio.wait_for(session.get(url), timeout=15) as response:
                    if response.status != 200:
                        raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ Yahoo, HTTP {response.status}")
                    text = await response.text()
                    df = pd.read_csv(io.StringIO(text))
                    if df.empty:
                        raise ValueError("D·ªØ li·ªáu Yahoo r·ªóng")
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.set_index('Date')
                    
                    # Chu·∫©n h√≥a t√™n c·ªôt
                    df = df.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low',
                                            'Close': 'close', 'Volume': 'volume'})
                    
                    # Th√™m m√∫i gi·ªù
                    if df.index.tz is None:
                        df.index = df.index.tz_localize('Asia/Bangkok')
                    
                    return df.tail(num_candles)
        except asyncio.TimeoutError:
            logger.error("Timeout khi t·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance.")
            raise
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu Yahoo: {str(e)}")
            raise

    async def get_incremental_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """Ch·ªâ t·∫£i d·ªØ li·ªáu m·ªõi t·ª´ l·∫ßn c·∫≠p nh·∫≠t cu·ªëi."""
        cache_key = f"last_update_{symbol}_{timeframe}"
        last_update = await redis_manager.get(cache_key)
        
        if not last_update:
            # T·∫£i to√†n b·ªô d·ªØ li·ªáu n·∫øu ch∆∞a c√≥
            df, _ = await self.load_data(symbol, timeframe, DEFAULT_CANDLES)
            await redis_manager.set(cache_key, datetime.now(), expire=CACHE_EXPIRE_LONG)
            return df
            
        # T√≠nh to√°n kho·∫£ng th·ªùi gian c·∫ßn t·∫£i
        from_date = last_update + timedelta(days=1)
        to_date = datetime.now()
        
        # Kh√¥ng c·∫ßn t·∫£i n·∫øu th·ªùi gian ch∆∞a ƒë·ªß 1 ng√†y
        if (to_date - from_date).days < 1:
            df_old, _ = await self.load_data(symbol, timeframe, DEFAULT_CANDLES)
            return df_old
            
        try:
            # T·∫£i d·ªØ li·ªáu m·ªõi
            if self.primary_source == 'vnstock':
                def fetch_incremental():
                    stock = Vnstock().stock(symbol=symbol, source='TCBS')
                    df = stock.quote.history(start=from_date.strftime('%Y-%m-%d'), 
                                          end=to_date.strftime('%Y-%m-%d'), 
                                          interval=timeframe)
                    if df is not None and not df.empty:
                        df = df.rename(columns={'time': 'date', 'open': 'open', 'high': 'high',
                                              'low': 'low', 'close': 'close', 'volume': 'volume'})
                        df['date'] = pd.to_datetime(df['date'])
                        df = df.set_index('date')
                        if df.index.tz is None:
                            df.index = df.index.tz_localize('Asia/Bangkok')
                        return df[['open', 'high', 'low', 'close', 'volume']]
                    return pd.DataFrame()
                
                df_new = await run_in_thread(fetch_incremental)
                
                if df_new is None or df_new.empty:
                    logger.info(f"Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi cho {symbol} t·ª´ {from_date} ƒë·∫øn {to_date}")
                    df_old, _ = await self.load_data(symbol, timeframe, DEFAULT_CANDLES)
                    return df_old
                    
                # Merge v·ªõi d·ªØ li·ªáu c≈©
                df_old, _ = await self.load_data(symbol, timeframe, DEFAULT_CANDLES)
                df = pd.concat([df_old, df_new]).drop_duplicates()
                
                # Chu·∫©n h√≥a, x·ª≠ l√Ω v√† l∆∞u
                df = self.standardize_dataframe(df)
                df = self.handle_missing_values(df)
                df = filter_trading_days(df)
                
                # C·∫≠p nh·∫≠t cache
                cache_key_data = f"data_{self.primary_source}_{symbol}_{timeframe}_{DEFAULT_CANDLES}"
                await redis_manager.set(cache_key_data, df, expire=CACHE_EXPIRE_MEDIUM)
                await redis_manager.set(cache_key, datetime.now(), expire=CACHE_EXPIRE_LONG)
                
                return df
                
            else:
                # Fallback to full load for other sources
                return await self.load_data(symbol, timeframe, DEFAULT_CANDLES)[0]
                
        except Exception as e:
            logger.error(f"L·ªói c·∫≠p nh·∫≠t gia tƒÉng cho {symbol}: {str(e)}")
            # Fallback to cached data
            return await self.load_data(symbol, timeframe, DEFAULT_CANDLES)[0]

    async def fetch_fundamental_data_vnstock(self, symbol: str) -> dict:
        cache_key = f"fundamental_vnstock_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = Vnstock().stock(symbol=symbol, source='TCBS')
            fundamental_data = {}
            ratios = stock.finance.ratio()
            if ratios is not None and not ratios.empty:
                fundamental_data.update(ratios.iloc[-1].to_dict())
            if hasattr(stock.finance, 'valuation'):
                valuation = stock.finance.valuation()
                if valuation is not None and not valuation.empty:
                    fundamental_data.update(valuation.iloc[-1].to_dict())
            if not fundamental_data:
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock")
            return fundamental_data

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock: {str(e)}")
            return {}

    async def fetch_fundamental_data_yahoo(self, symbol: str) -> dict:
        cache_key = f"fundamental_yahoo_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = yf.Ticker(f"{symbol}.VN")
            info = stock.info
            fundamental_data = {
                'EPS': info.get('trailingEps', None),
                'P/E': info.get('trailingPE', None),
                'P/B': info.get('priceToBook', None),
                'ROE': info.get('returnOnEquity', None),
                'Dividend Yield': info.get('dividendYield', None),
                'Market Cap': info.get('marketCap', None),
                'BVPS': info.get('bookValue', None)
            }
            if all(v is None for v in fundamental_data.values()):
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo Finance")
            return fundamental_data

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo: {str(e)}")
            return {}

    async def get_fundamental_data(self, symbol: str) -> dict:
        if is_index(symbol):
            return {"error": f"{symbol} l√† ch·ªâ s·ªë, kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n"}
        fundamental_data = await self.fetch_fundamental_data_vnstock(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        fundamental_data = await self.fetch_fundamental_data_yahoo(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        return {"error": f"Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n cho {symbol}"}

    async def merge_data_sources(self, symbol: str, timeframe: str, num_candles: int) -> pd.DataFrame:
        """
        T·∫£i d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn v√† h·ª£p nh·∫•t l·∫°i v·ªõi cƒÉn ch·ªânh timestamp.
        
        Args:
            symbol: M√£ ch·ª©ng kho√°n c·∫ßn t·∫£i
            timeframe: Khung th·ªùi gian ('1D', '1W', '1M')
            num_candles: S·ªë n·∫øn c·∫ßn t·∫£i
            
        Returns:
            DataFrame h·ª£p nh·∫•t t·ª´ nhi·ªÅu ngu·ªìn
        """
        dataframes = []
        sources = self._get_data_source_priorities()
        
        # T·∫£i d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn
        for source in sources:
            try:
                if source == 'vnstock':
                    df = await self._load_from_vnstock(symbol, timeframe, num_candles)
                elif source == 'yahoo':
                    df = await self._load_from_yahoo(symbol, timeframe, num_candles)
                else:
                    continue
                    
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                df = self.standardize_dataframe(df)
                
                # T·∫°o c·ªôt ƒë·ªÉ ƒë√°nh d·∫•u ngu·ªìn d·ªØ li·ªáu
                df['data_source'] = source
                
                dataframes.append(df)
            except Exception as e:
                logger.warning(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ ngu·ªìn {source}: {str(e)}")
        
        if not dataframes:
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho {symbol} t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o")
            
        # S·ª≠ d·ª•ng TimestampAligner ƒë·ªÉ h·ª£p nh·∫•t c√°c DataFrame
        merged_df = self.timestamp_aligner.merge_dataframes_with_alignment(dataframes, freq=timeframe)
        
        # X·ª≠ l√Ω tr√πng l·∫∑p v√† l·ªçc d·ªØ li·ªáu
        merged_df = self.handle_missing_values(merged_df)
        merged_df = self.timestamp_aligner.filter_trading_days(merged_df)
        
        return merged_df
        
    async def get_precise_timestamp_data(self, symbol: str, timeframe: str, num_candles: int) -> pd.DataFrame:
        """
        T·∫£i d·ªØ li·ªáu v·ªõi timestamp ƒë∆∞·ª£c cƒÉn ch·ªânh ch√≠nh x√°c.
        
        Args:
            symbol: M√£ ch·ª©ng kho√°n c·∫ßn t·∫£i
            timeframe: Khung th·ªùi gian ('1D', '1W', '1M')
            num_candles: S·ªë n·∫øn c·∫ßn t·∫£i
            
        Returns:
            DataFrame v·ªõi timestamp ƒë√£ ƒë∆∞·ª£c cƒÉn ch·ªânh ch√≠nh x√°c
        """
        cache_key = f"precise_ts_{symbol}_{timeframe}_{num_candles}"
        cached_data = await redis_manager.get(cache_key)
        
        if cached_data is not None:
            return cached_data
            
        try:
            # T·∫£i d·ªØ li·ªáu t·ª´ ngu·ªìn ch√≠nh
            df, _ = await self.load_data(symbol, timeframe, num_candles)
            
            # CƒÉn ch·ªânh timestamp
            fixed_df = self.timestamp_aligner.fix_timestamp_issues(df)
            aligned_df = self.timestamp_aligner.standardize_timeframe(fixed_df, freq=timeframe)
            
            # Th√™m c√°c ƒë·∫∑c tr∆∞ng timestamp
            enhanced_df = self.timestamp_aligner.extract_timestamp_features(aligned_df)
            
            # L∆∞u v√†o cache
            await redis_manager.set(cache_key, enhanced_df, expire=CACHE_EXPIRE_MEDIUM)
            
            return enhanced_df
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu timestamp ch√≠nh x√°c cho {symbol}: {str(e)}")
            raise

# ---------- QU·∫¢N L√ù CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU ----------
class DataQualityControl:
    def __init__(self, db_manager=None):
        self.quality_metrics = {}
        self.db_manager = db_manager
        self.quality_threshold = 0.7
        
    def evaluate_data_quality(self, df: pd.DataFrame, symbol: str) -> dict:
        """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu theo nhi·ªÅu ti√™u ch√≠."""
        if df is None or df.empty:
            return {
                "symbol": symbol,
                "completeness": 0.0,
                "consistency": 0.0,
                "timeliness": 0.0,
                "validity": 0.0,
                "accuracy": 0.0,
                "overall_score": 0.0,
                "recommendation": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°",
                "timestamp": datetime.now().isoformat()
            }
            
        metrics = {
            "symbol": symbol,
            "timestamp": datetime.now().isoformat()
        }
        
        # 1. T√≠nh to√°n ƒëi·ªÉm ƒë·∫ßy ƒë·ªß (completeness)
        missing_values = df.isnull().mean().mean()
        metrics["completeness"] = float(1.0 - missing_values)
        
        # 2. T√≠nh to√°n ƒëi·ªÉm nh·∫•t qu√°n (consistency)
        # Ki·ªÉm tra c√°c r√†ng bu·ªôc gi·ªØa c√°c c·ªôt
        if 'high' in df.columns and 'low' in df.columns:
            valid_hl = (df['high'] >= df['low']).mean()
            metrics["consistency"] = float(valid_hl)
        else:
            metrics["consistency"] = 0.5  # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ki·ªÉm tra
            
        # 3. T√≠nh to√°n ƒëi·ªÉm k·ªãp th·ªùi (timeliness)
        # Ki·ªÉm tra d·ªØ li·ªáu c√≥ c·∫≠p nh·∫≠t m·ªõi kh√¥ng
        if isinstance(df.index, pd.DatetimeIndex):
            latest_date = df.index.max()
            days_since_update = (datetime.now() - latest_date.to_pydatetime()).days
            metrics["timeliness"] = float(max(0, 1.0 - days_since_update/30.0))  # Gi·∫£m 1/30 m·ªói ng√†y kh√¥ng c·∫≠p nh·∫≠t
        else:
            metrics["timeliness"] = 0.0
            
        # 4. T√≠nh to√°n ƒëi·ªÉm h·ª£p l·ªá (validity)
        # Ki·ªÉm tra c√°c gi√° tr·ªã c√≥ n·∫±m trong kho·∫£ng h·ª£p l·ªá kh√¥ng
        if 'close' in df.columns and 'low' in df.columns and 'high' in df.columns:
            valid_close = ((df['close'] >= df['low']) & (df['close'] <= df['high'])).mean()
            metrics["validity"] = float(valid_close)
        else:
            metrics["validity"] = 0.5
            
        # 5. ∆Ø·ªõc l∆∞·ª£ng ƒë·ªô ch√≠nh x√°c (accuracy)
        # Ph√°t hi·ªán outliers b·∫±ng Z-score
        if 'close' in df.columns:
            z_scores = np.abs((df['close'] - df['close'].mean()) / df['close'].std())
            outlier_ratio = (z_scores > 3).mean()
            metrics["accuracy"] = float(1.0 - outlier_ratio)
        else:
            metrics["accuracy"] = 0.5
            
        # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p
        weights = {
            "completeness": 0.25,
            "consistency": 0.2,
            "timeliness": 0.2,
            "validity": 0.2,
            "accuracy": 0.15
        }
        
        weighted_scores = [metrics[key] * weights[key] for key in weights.keys()]
        metrics["overall_score"] = float(sum(weighted_scores))
        
        # X√°c ƒë·ªãnh khuy·∫øn ngh·ªã d·ª±a tr√™n ch·∫•t l∆∞·ª£ng
        if metrics["overall_score"] < 0.5:
            metrics["recommendation"] = "D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng th·∫•p, n√™n thu th·∫≠p l·∫°i"
        elif metrics["overall_score"] < 0.7:
            metrics["recommendation"] = "D·ªØ li·ªáu c·∫ßn ƒë∆∞·ª£c l√†m s·∫°ch th√™m"
        elif metrics["overall_score"] < 0.9:
            metrics["recommendation"] = "D·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng kh√° t·ªët"
        else:
            metrics["recommendation"] = "D·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng r·∫•t t·ªët"
            
        # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°
        self.quality_metrics[symbol] = metrics
        
        return metrics
        
    async def save_quality_metrics(self, metrics: dict):
        """L∆∞u tr·ªØ c√°c ch·ªâ s·ªë ch·∫•t l∆∞·ª£ng v√†o DB n·∫øu c√≥."""
        if self.db_manager:
            # Implementation would depend on your database schema
            pass
            
    def is_data_usable(self, metrics: dict) -> bool:
        """Ki·ªÉm tra d·ªØ li·ªáu c√≥ ƒë·ªß ch·∫•t l∆∞·ª£ng ƒë·ªÉ s·ª≠ d·ª•ng kh√¥ng."""
        return metrics["overall_score"] >= self.quality_threshold
        
    async def generate_quality_report(self, symbol: str, timeframe: str) -> str:
        """T·∫°o b√°o c√°o v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu."""
        if symbol not in self.quality_metrics:
            return f"Ch∆∞a c√≥ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho {symbol}"
            
        metrics = self.quality_metrics[symbol]
        
        report = f"üìä B√ÅO C√ÅO CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU: {symbol} ({timeframe})\n\n"
        report += f"‚è±Ô∏è Th·ªùi ƒëi·ªÉm ƒë√°nh gi√°: {metrics['timestamp']}\n"
        report += f"‚úÖ ƒêi·ªÉm t·ªïng h·ª£p: {metrics['overall_score']:.2f}/1.0\n\n"
        report += "CHI TI·∫æT:\n"
        report += f"- ƒê·∫ßy ƒë·ªß: {metrics['completeness']:.2f}/1.0\n"
        report += f"- Nh·∫•t qu√°n: {metrics['consistency']:.2f}/1.0\n"
        report += f"- K·ªãp th·ªùi: {metrics['timeliness']:.2f}/1.0\n"
        report += f"- H·ª£p l·ªá: {metrics['validity']:.2f}/1.0\n"
        report += f"- Ch√≠nh x√°c: {metrics['accuracy']:.2f}/1.0\n\n"
        report += f"üìå Khuy·∫øn ngh·ªã: {metrics['recommendation']}"
        
        return report

# ---------- PH√ÇN T√çCH K·ª∏ THU·∫¨T ----------
class TechnicalAnalyzer:
    @staticmethod
    def _calculate_common_indicators(df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty:
            raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ t√≠nh to√°n ch·ªâ b√°o")
        if 'close' not in df.columns:
            raise ValueError("D·ªØ li·ªáu kh√¥ng c√≥ c·ªôt 'close' c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o")
        if len(df) < 20:
            raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh to√°n SMA20 (c·∫ßn √≠t nh·∫•t 20 n·∫øn)")
        try:
            df['sma20'] = trend.SMAIndicator(df['close'], window=20).sma_indicator()
            df['sma50'] = trend.SMAIndicator(df['close'], window=50).sma_indicator()
            df['sma200'] = trend.SMAIndicator(df['close'], window=200).sma_indicator() if len(df) >= 200 else np.nan
            df['rsi'] = momentum.RSIIndicator(df['close'], window=14).rsi() if len(df) >= 14 else np.nan
            macd = trend.MACD(df['close'])
            df['macd'] = macd.macd()
            df['signal'] = macd.macd_signal()
            bb = volatility.BollingerBands(df['close'])
            df['bb_high'] = bb.bollinger_hband()
            df['bb_low'] = bb.bollinger_lband()
            ichimoku = trend.IchimokuIndicator(df['high'], df['low'])
            df['ichimoku_a'] = ichimoku.ichimoku_a()
            df['ichimoku_b'] = ichimoku.ichimoku_b()
            df['vwap'] = (df['volume'] * (df['high'] + df['low'] + df['close']) / 3).cumsum() / df['volume'].cumsum()
            df['mfi'] = MFIIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume'], window=14).money_flow_index()
            ichimoku_short = trend.IchimokuIndicator(df['high'], df['low'], window1=9, window2=26)
            df['tenkan_sen'] = ichimoku_short.ichimoku_a()
            ichimoku_medium = trend.IchimokuIndicator(df['high'], df['low'], window1=26, window2=52)
            df['kijun_sen'] = ichimoku_medium.ichimoku_b()
            df['chikou_span'] = df['close'].shift(-26) if len(df) > 26 else np.nan
            high_price = df['high'].max()
            low_price = df['low'].min()
            diff = high_price - low_price
            df['fib_0.0'] = high_price
            df['fib_23.6'] = high_price - 0.236 * diff
            df['fib_38.2'] = high_price - 0.382 * diff
            df['fib_50.0'] = high_price - 0.5 * diff
            df['fib_61.8'] = high_price - 0.618 * diff
            df['fib_100.0'] = low_price
            return df
        except Exception as e:
            logger.error(f"L·ªói t√≠nh to√°n ch·ªâ s·ªë k·ªπ thu·∫≠t: {str(e)}")
            raise

    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        return self._calculate_common_indicators(df)

    def calculate_multi_timeframe_indicators(self, dfs: dict) -> dict:
        indicators = {}
        for timeframe, df in dfs.items():
            try:
                df_processed = self._calculate_common_indicators(df)
                indicators[timeframe] = df_processed.tail(1).to_dict(orient='records')[0]
            except Exception as e:
                logger.error(f"L·ªói t√≠nh to√°n cho khung {timeframe}: {str(e)}")
                indicators[timeframe] = {}
        return indicators

# ---------- THU TH·∫¨P TIN T·ª®C (S·ª¨A L·ªñI) ----------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
async def fetch_rss_feed(url: str) -> str:
    import aiohttp
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    raise ValueError(f"HTTP {response.status} khi l·∫•y RSS t·ª´ {url}")
    except asyncio.TimeoutError:
        logger.error(f"Timeout khi l·∫•y RSS t·ª´ {url}")
    except Exception as e:
        logger.error(f"L·ªói l·∫•y RSS t·ª´ {url}: {str(e)}")
    return None

async def get_news(symbol: str = None, limit: int = 3) -> list:
    cache_key = f"news_{symbol}_{limit}" if symbol else f"news_market_{limit}"
    cached_news = await redis_manager.get(cache_key)
    if cached_news is not None:
        return cached_news

    rss_urls = [
        "https://cafef.vn/thi-truong-chung-khoan.rss",
        "https://cafef.vn/smart-money.rss",
        "https://cafef.vn/tai-chinh-ngan-hang.rss",
        "https://cafef.vn/doanh-nghiep.rss",
        "https://vnexpress.net/rss/kinh-doanh.rss",
        "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
        "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml"
    ]

    market_keywords = ["market", "stock", "index", "economy", "th·ªã tr∆∞·ªùng", "ch·ª©ng kho√°n", "l√£i su·∫•t", "vnindex"]
    symbol_keywords = [symbol.lower(), f"{symbol.lower()} ", f"m√£ {symbol.lower()}"] if symbol else []

    news_list = []
    tasks = [fetch_rss_feed(url) for url in rss_urls]
    rss_results = await asyncio.gather(*tasks)
    for rss_text in rss_results:
        if rss_text is None:
            continue
        feed = parse_rss_content(rss_text)
        if not feed or not feed.entries:
            continue
        for entry in feed.entries[:2]:
            title = entry.get("title", "").strip()
            link = entry.get("link", "")
            summary = entry.get("summary", entry.get("description", "")).strip()
            pub_date = entry.get("published", datetime.now().isoformat())
            content = f"{title} {summary}".lower()
            match = False
            if symbol and any(kw in content for kw in symbol_keywords):
                match = True
            elif not symbol and any(kw in content for kw in market_keywords):
                match = True
            if match:
                news_list.append({
                    "title": title,
                    "link": link,
                    "summary": summary[:200] + "..." if len(summary) > 200 else summary,
                    "published": pub_date
                })
    unique_news = sorted(news_list, key=lambda x: x.get("published", ""), reverse=True)[:limit]
    result = unique_news if unique_news else [{"title": "‚ö†Ô∏è Kh√¥ng c√≥ tin t·ª©c", "link": "#", "summary": ""}]
    await redis_manager.set(cache_key, result, expire=NEWS_CACHE_EXPIRE)
    return result

def parse_rss_content(rss_text: str):
    try:
        return feedparser.parse(rss_text) if rss_text else None
    except Exception as e:
        logger.error(f"L·ªói ph√¢n t√≠ch RSS: {str(e)}")
        return None

# ---------- PH√ÇN T√çCH C∆† B·∫¢N ----------
def deep_fundamental_analysis(fundamental_data: dict) -> str:
    report = "üìä **Ph√¢n t√≠ch c∆° b·∫£n**:\n"
    if not fundamental_data or 'error' in fundamental_data:
        return report + f"‚ùå {fundamental_data.get('error', 'Kh√¥ng c√≥ d·ªØ li·ªáu')}\n"

    keys_mapping = {
        'EPS': ['EPS', 'eps', 'Earning Per Share'],
        'P/E': ['P/E', 'PE', 'pe', 'Price to Earning', 'trailingPE'],
        'P/B': ['P/B', 'PB', 'pb', 'Price to Book', 'priceToBook'],
        'ROE': ['ROE', 'roe', 'Return on Equity', 'returnOnEquity'],
        'Dividend Yield': ['Dividend Yield', 'DY', 'dividend_yield', 'dividendYield'],
        'BVPS': ['BVPS', 'Book Value Per Share', 'bookValue'],
        'Market Cap': ['Market Cap', 'market_cap', 'marketCap']
    }

    extracted = {}
    for standard_key, possible_keys in keys_mapping.items():
        for key in possible_keys:
            if key in fundamental_data and fundamental_data[key] is not None:
                extracted[standard_key] = fundamental_data[key]
                break

    for key, value in extracted.items():
        report += f"- **{key}**: {value:.2f}\n" if isinstance(value, (int, float)) else f"- **{key}**: {value}\n"

    if 'P/E' in extracted and isinstance(extracted['P/E'], (int, float)):
        pe = extracted['P/E']
        report += "- **P/E**: C·ªï phi·∫øu c√≥ th·ªÉ ƒë·ªãnh gi√° th·∫•p\n" if pe < 10 else "- **P/E**: ƒê·ªãnh gi√° cao\n" if pe > 20 else "- **P/E**: ƒê·ªãnh gi√° h·ª£p l√Ω\n"

    if 'ROE' in extracted and isinstance(extracted['ROE'], (int, float)):
        report += "- **ROE**: Hi·ªáu qu·∫£ s·ª≠ d·ª•ng v·ªën t·ªët\n" if extracted['ROE'] > 15 else "- **ROE**: C·∫ßn c·∫£i thi·ªán\n"

    if 'Dividend Yield' in extracted and isinstance(extracted['Dividend Yield'], (int, float)):
        report += "- **Dividend**: H·∫•p d·∫´n\n" if extracted['Dividend Yield'] > 5 else "- **Dividend**: Trung b√¨nh\n"

    return report

# ---------- HU·∫§N LUY·ªÜN V√Ä L∆ØU M√î H√åNH ----------
def prepare_data_for_prophet(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ d·ª± b√°o")
    df_reset = df.reset_index()
    df_reset['date'] = df_reset['date'].dt.tz_localize(None)
    df_reset = df_reset.rename(columns={'date': 'ds', 'close': 'y'})
    return df_reset[['ds', 'y']]

def get_vietnam_holidays(years) -> pd.DataFrame:
    holiday_list = []
    for year in years:
        vn_holidays = holidays.Vietnam(years=year)
        for date, name in vn_holidays.items():
            holiday_list.append({'ds': pd.to_datetime(date), 'holiday': name})
    holiday_df = pd.DataFrame(holiday_list)
    holiday_df['ds'] = holiday_df['ds'].dt.tz_localize(None)
    return holiday_df

def forecast_with_prophet(df: pd.DataFrame, periods: int = 7) -> (pd.DataFrame, Prophet):
    try:
        data = prepare_data_for_prophet(df)
        if data.empty:
            raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o Prophet.")
        current_year = datetime.now().year
        holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
        model = Prophet(holidays=holiday_df)
        model.fit(data)
        future = model.make_future_dataframe(periods=periods)
        forecast = model.predict(future)
        return forecast, model
    except Exception as e:
        logger.error(f"L·ªói d·ª± b√°o Prophet: {str(e)}")
        raise

def evaluate_prophet_performance(df: pd.DataFrame, forecast: pd.DataFrame) -> float:
    actual = df['close'].iloc[-len(forecast):].values
    predicted = forecast['yhat'].values[:len(actual)]
    if len(actual) < 1 or len(predicted) < 1:
        return 0.0
    mse = np.mean((actual - predicted) ** 2)
    return 1 / (1 + mse)

def predict_xgboost_signal(df: pd.DataFrame, features: list) -> (int, float):
    if df is None or df.empty:
        return "D·ªØ li·ªáu r·ªóng", 0.0
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu", 0.0
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    X_pred = X.iloc[-1:]
    y_test = y.iloc[-1:]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    pred = model.predict(X_pred)[0]
    accuracy = accuracy_score(y_test, [pred])
    return pred, accuracy

def train_prophet_model(df: pd.DataFrame) -> (Prophet, float):
    data = prepare_data_for_prophet(df)
    if data.empty:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán Prophet")
    current_year = datetime.now().year
    holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
    model = Prophet(holidays=holiday_df)
    model.fit(data)
    future = model.make_future_dataframe(periods=0)
    forecast = model.predict(future)
    performance = evaluate_prophet_performance(df, forecast)
    return model, performance

def train_xgboost_model(df: pd.DataFrame, features: list) -> (xgb.XGBClassifier, float):
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ hu·∫•n luy·ªán XGBoost")
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán XGBoost")
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_train)
    performance = accuracy_score(y_train, y_pred)
    return model, performance

# ---------- AUTO TRAINING ----------
async def get_training_symbols() -> list:
    try:
        async with SessionLocal() as session:
            result = await session.execute(select(ReportHistory.symbol).distinct())
            symbols = [row[0] for row in result.fetchall()]
            logger.info(f"C√°c m√£ ƒë∆∞·ª£c l·∫•y t·ª´ l·ªãch s·ª≠ b√°o c√°o: {symbols}")
            return symbols
    except Exception as e:
        logger.error(f"L·ªói truy v·∫•n symbols t·ª´ ReportHistory: {str(e)}")
        return []

async def train_models_for_symbol(symbol: str):
    try:
        logger.info(f"B·∫Øt ƒë·∫ßu auto training cho m√£: {symbol}")
        loader = DataLoader()
        tech_analyzer = TechnicalAnalyzer()
        df, _ = await loader.load_data(symbol, '1D', 500)
        df = tech_analyzer.calculate_indicators(df)
        features = ['sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal',
                    'bb_high', 'bb_low', 'ichimoku_a', 'ichimoku_b', 'vwap', 'mfi']
        task_prophet = asyncio.to_thread(train_prophet_model, df)
        task_xgb = asyncio.to_thread(train_xgboost_model, df, features)
        (prophet_model, prophet_perf), (xgb_model, xgb_perf) = await asyncio.gather(task_prophet, task_xgb)
        await model_db_manager.store_trained_model(symbol, 'prophet', prophet_model, prophet_perf)
        await model_db_manager.store_trained_model(symbol, 'xgboost', xgb_model, xgb_perf)
        logger.info(f"Auto training cho {symbol} ho√†n t·∫•t.")
    except Exception as e:
        logger.error(f"L·ªói auto training cho {symbol}: {str(e)}")

async def auto_train_models():
    try:
        symbols = await get_training_symbols()
        if not symbols:
            logger.info("Kh√¥ng c√≥ m√£ n√†o trong ReportHistory, b·ªè qua auto training.")
            return
        tasks = [train_models_for_symbol(symbol) for symbol in symbols]
        await asyncio.gather(*tasks)
    except Exception as e:
        logger.error(f"L·ªói auto training: {str(e)}")

# ---------- AI V√Ä B√ÅO C√ÅO ----------
class AIAnalyzer:
    def __init__(self):
        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-1.5-pro')

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def generate_content(self, prompt):
        return await self.model.generate_content_async(prompt)

    async def load_report_history(self, symbol: str) -> list:
        return await db.load_report_history(symbol)

    async def save_report_history(self, symbol: str, report: str, close_today: float, close_yesterday: float) -> None:
        await db.save_report_history(symbol, report, close_today, close_yesterday)

    def analyze_price_action(self, df: pd.DataFrame) -> str:
        if len(df) < 6:
            return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch."
        last_5_days = df['close'].tail(5)
        changes = last_5_days.pct_change().dropna()
        trend_summary = []
        for i, change in enumerate(changes):
            date = last_5_days.index[i+1].strftime('%Y-%m-%d')
            if df.loc[last_5_days.index[i+1], 'is_outlier']:
                outlier_note = " (‚ö†Ô∏è outlier)"
            else:
                outlier_note = ""
            if change > 0:
                trend_summary.append(f"{date}: TƒÉng {change*100:.2f}%{outlier_note}")
            elif change < 0:
                trend_summary.append(f"{date}: Gi·∫£m {-change*100:.2f}%{outlier_note}")
            else:
                trend_summary.append(f"{date}: Kh√¥ng ƒë·ªïi{outlier_note}")
        consecutive_up = consecutive_down = 0
        for change in changes[::-1]:
            if change > 0:
                consecutive_up += 1
                consecutive_down = 0
            elif change < 0:
                consecutive_down += 1
                consecutive_up = 0
            else:
                break
        if consecutive_up >= 3:
            summary = f"‚úÖ Gi√° tƒÉng {consecutive_up} phi√™n li√™n ti·∫øp.\n"
        elif consecutive_down >= 3:
            summary = f"‚ö†Ô∏è Gi√° gi·∫£m {consecutive_down} phi√™n li√™n ti·∫øp.\n"
        else:
            summary = "üîç Xu h∆∞·ªõng ch∆∞a r√µ.\n"
        summary += "\n".join(trend_summary)
        return summary

    async def analyze_with_openrouter(self, technical_data):
        if not OPENROUTER_API_KEY:
            raise Exception("Ch∆∞a c√≥ OPENROUTER_API_KEY")

        prompt = (
            "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t ch·ª©ng kho√°n."
            " D·ª±a tr√™n d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y, h√£y nh·∫≠n di·ªán c√°c m·∫´u h√¨nh n·∫øn nh∆∞ Doji, Hammer, Shooting Star, Engulfing,"
            " s√≥ng Elliott, m√¥ h√¨nh Wyckoff, v√† c√°c v√πng h·ªó tr·ª£/kh√°ng c·ª±."
            "\n\nCh·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ ·ªü d·∫°ng JSON nh∆∞ sau, kh√¥ng th√™m gi·∫£i th√≠ch n√†o kh√°c:\n"
            "{\n"
            "  \"support_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"resistance_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"patterns\": [\n"
            "    {\"name\": \"t√™n m·∫´u h√¨nh\", \"description\": \"gi·∫£i th√≠ch ng·∫Øn\"},\n"
            "    ...\n"
            "  ]\n"
            "}\n\n"
            f"D·ªØ li·ªáu:\n{json.dumps(technical_data, ensure_ascii=False, indent=2)}"
        )

        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://yourapp.com",
            "X-Title": "Stock Analysis Bot"
        }
        payload = {
            "model": "deepseek/deepseek-chat-v3-0324:free",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 1024,
            "temperature": 0.2
        }

        async with aiohttp.ClientSession() as session:
            async with session.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload) as resp:
                text = await resp.text()
                try:
                    result = json.loads(text)
                    content = result['choices'][0]['message']['content']
                    return json.loads(content)
                except json.JSONDecodeError:
                    logger.error(f"Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ OpenRouter: {text}")
                    return {}
                except KeyError:
                    logger.error(f"Ph·∫£n h·ªìi thi·∫øu tr∆∞·ªùng c·∫ßn thi·∫øt: {text}")
                    return {}

    async def generate_report(self, dfs: dict, symbol: str, fundamental_data: dict, outlier_reports: dict) -> str:
        try:
            tech_analyzer = TechnicalAnalyzer()
            indicators = tech_analyzer.calculate_multi_timeframe_indicators(dfs)
            news = await get_news(symbol=symbol)
            news_text = "\n".join([f"üì∞ **{n['title']}**\nüîó {n['link']}\nüìù {n['summary']}" for n in news])
            df_1d = dfs.get('1D')
            close_today = df_1d['close'].iloc[-1]
            close_yesterday = df_1d['close'].iloc[-2]
            price_action = self.analyze_price_action(df_1d)
            history = await self.load_report_history(symbol)
            past_report = ""
            if history:
                last = history[-1]
                past_result = "ƒë√∫ng" if (close_today > last["close_today"] and "mua" in last["report"].lower()) else "sai"
                past_report = f"üìú **B√°o c√°o tr∆∞·ªõc** ({last['date']}): {last['close_today']} ‚Üí {close_today} ({past_result})\n"
            fundamental_report = deep_fundamental_analysis(fundamental_data)

            # Ph√¢n t√≠ch v·ªõi OpenRouter
            technical_data = {
                "candlestick_data": df_1d.tail(50).to_dict(orient="records"),
                "technical_indicators": indicators['1D']
            }
            openrouter_result = await self.analyze_with_openrouter(technical_data)
            support_levels = openrouter_result.get('support_levels', [])
            resistance_levels = openrouter_result.get('resistance_levels', [])
            patterns = openrouter_result.get('patterns', [])

            forecast, prophet_model = forecast_with_prophet(df_1d, periods=7)
            prophet_perf = evaluate_prophet_performance(df_1d, forecast)
            future_forecast = forecast[forecast['ds'] > df_1d.index[-1].tz_localize(None)]
            if not future_forecast.empty:
                next_day_pred = future_forecast.iloc[0]
                day7_pred = future_forecast.iloc[6] if len(future_forecast) >= 7 else future_forecast.iloc[-1]
            else:
                next_day_pred = forecast.iloc[-1]
                day7_pred = forecast.iloc[-1]

            forecast_summary = f"**D·ª± b√°o gi√° (Prophet)** (Hi·ªáu su·∫•t: {prophet_perf:.2f}):\n"
            forecast_summary += f"- Ng√†y ti·∫øp theo ({next_day_pred['ds'].strftime('%d/%m/%Y')}): {next_day_pred['yhat']:.2f}\n"
            forecast_summary += f"- Sau 7 ng√†y ({day7_pred['ds'].strftime('%d/%m/%Y')}): {day7_pred['yhat']:.2f}\n"

            features = ['sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal', 'bb_high', 'bb_low', 'ichimoku_a', 'ichimoku_b', 'vwap', 'mfi']
            xgb_signal, xgb_perf = predict_xgboost_signal(df_1d.copy(), features)
            if isinstance(xgb_signal, int):
                xgb_text = "TƒÉng" if xgb_signal == 1 else "Gi·∫£m"
            else:
                xgb_text = xgb_signal
            xgb_summary = f"**XGBoost d·ª± ƒëo√°n t√≠n hi·ªáu giao d·ªãch** (Hi·ªáu su·∫•t: {xgb_perf:.2f}): {xgb_text}\n"

            outlier_text = "\n".join([f"**{tf}**: {report}" for tf, report in outlier_reports.items()])

            prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n, trader chuy√™n nghi·ªáp, chuy√™n gia b·∫Øt ƒë√°y 30 nƒÉm kinh nghi·ªám ·ªü ch·ª©ng kho√°n Vi·ªát Nam. H√£y vi·∫øt b√°o c√°o chi ti·∫øt cho {symbol}:

**Th√¥ng tin c∆° b·∫£n:**
- Ng√†y: {datetime.now().strftime('%d/%m/%Y')}
- Gi√° h√¥m qua: {close_yesterday:.2f}
- Gi√° h√¥m nay: {close_today:.2f}

**H√†nh ƒë·ªông gi√°:**
{price_action}

**L·ªãch s·ª≠ d·ª± ƒëo√°n:**
{past_report}

**Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu:**
{outlier_text}

**Ch·ªâ s·ªë k·ªπ thu·∫≠t:**
"""
            for tf, ind in indicators.items():
                prompt += f"\n--- {tf} ---\n"
                prompt += f"- Close: {ind.get('close', 0):.2f}\n"
                prompt += f"- SMA20: {ind.get('sma20', 0):.2f}, SMA50: {ind.get('sma50', 0):.2f}, SMA200: {ind.get('sma200', 0):.2f}\n"
                prompt += f"- RSI: {ind.get('rsi', 0):.2f}\n"
                prompt += f"- MACD: {ind.get('macd', 0):.2f} (Signal: {ind.get('signal', 0):.2f})\n"
                prompt += f"- Bollinger: {ind.get('bb_low', 0):.2f} - {ind.get('bb_high', 0):.2f}\n"
                prompt += f"- Ichimoku: A: {ind.get('ichimoku_a', 0):.2f}, B: {ind.get('ichimoku_b', 0):.2f}\n"
                prompt += f"- Fibonacci: 0.0: {ind.get('fib_0.0', 0):.2f}, 61.8: {ind.get('fib_61.8', 0):.2f}\n"
            prompt += f"\n**C∆° b·∫£n:**\n{fundamental_report}\n"
            prompt += f"\n**Tin t·ª©c:**\n{news_text}\n"
            prompt += f"\n**Ph√¢n t√≠ch t·ª´ OpenRouter:**\n"
            prompt += f"- H·ªó tr·ª£: {', '.join(map(str, support_levels))}\n"
            prompt += f"- Kh√°ng c·ª±: {', '.join(map(str, resistance_levels))}\n"
            prompt += f"- M·∫´u h√¨nh: {', '.join([p['name'] for p in patterns])}\n"
            prompt += f"\n{xgb_summary}\n"
            prompt += f"{forecast_summary}\n"
            prompt += """
**Y√™u c·∫ßu:**
1. So s√°nh gi√°/ ch·ªâ s·ªë phi√™n hi·ªán t·∫°i v√† phi√™n tr∆∞·ªõc ƒë√≥.
2. Ph√¢n t√≠ch ƒëa khung th·ªùi gian, xu h∆∞·ªõng ng·∫Øn h·∫°n, trung h·∫°n, d√†i h·∫°n.
3. ƒê√°nh gi√° c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t, ƒë·ªông l·ª±c th·ªã tr∆∞·ªùng.
4. X√°c ƒë·ªãnh h·ªó tr·ª£/kh√°ng c·ª± t·ª´ OpenRouter. ƒê∆∞a ra k·ªãch b·∫£n v√† x√°c su·∫•t % (tƒÉng, gi·∫£m, sideway).
5. ƒê·ªÅ xu·∫•t MUA/B√ÅN/N·∫ÆM GI·ªÆ v·ªõi % tin c·∫≠y, ƒëi·ªÉm v√†o, c·∫Øt l·ªó, ch·ªët l·ªùi. Ph∆∞∆°ng √°n ƒëi v·ªën, ph√¢n b·ªï t·ª∑ tr·ªçng c·ª• th·ªÉ.
6. ƒê√°nh gi√° r·ªßi ro v√† t·ª∑ l·ªá risk/reward.
7. K·∫øt h·ª£p tin t·ª©c, ph√¢n t√≠ch k·ªπ thu·∫≠t, c∆° b·∫£n v√† k·∫øt qu·∫£ t·ª´ OpenRouter ƒë·ªÉ ƒë∆∞a ra nh·∫≠n ƒë·ªãnh.
8. Kh√¥ng c·∫ßn theo form c·ªë ƒë·ªãnh, tr√¨nh b√†y logic, s√∫c t√≠ch nh∆∞ng ƒë·ªß th√¥ng tin ƒë·ªÉ h√†nh ƒë·ªông v√† s√°ng t·∫°o v·ªõi emoji.

**H∆∞·ªõng d·∫´n b·ªï sung:**
- D·ª±a v√†o h√†nh ƒë·ªông gi√° g·∫ßn ƒë√¢y ƒë·ªÉ x√°c ƒë·ªãnh qu√°n t√≠nh (momentum) hi·ªán t·∫°i.
- S·ª≠ d·ª•ng d·ªØ li·ªáu, s·ªë li·ªáu ƒë∆∞·ª£c cung c·∫•p, KH√îNG t·ª± suy di·ªÖn th√™m.
- Ch√∫ √Ω: VNINDEX, VN30 l√† ch·ªâ s·ªë, kh√¥ng ph·∫£i c·ªï phi·∫øu.
"""
            response = await self.generate_content(prompt)
            report = response.text
            await self.save_report_history(symbol, report, close_today, close_yesterday)
            return report
        except Exception as e:
            logger.error(f"L·ªói t·∫°o b√°o c√°o: {str(e)}")
            return f"‚ùå L·ªói t·∫°o b√°o c√°o: {str(e)}"

# ---------- TELEGRAM COMMANDS ----------
async def notify_admin_new_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.message.from_user
    user_id = user.id
    if not await is_user_approved(user_id):
        message = f"üîî Ng∆∞·ªùi d√πng m·ªõi:\nID: {user_id}\nUsername: {user.username}\nT√™n: {user.full_name}\nDuy·ªát: /approve {user_id}"
        await context.bot.send_message(chat_id=ADMIN_ID, text=message)
        await update.message.reply_text("‚è≥ Ch·ªù admin duy·ªát!")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    logger.info(f"Start called: user_id={user_id}, ADMIN_ID={ADMIN_ID}")

    if str(user_id) == ADMIN_ID and not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        logger.info(f"Admin {user_id} t·ª± ƒë·ªông duy·ªát.")

    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return

    await update.message.reply_text(
        "üöÄ **V18.8.1T - N√¢ng c·∫•p t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu!**\n"
        "üìä **L·ªánh**:\n"
        "- /analyze [M√£] [S·ªë n·∫øn] - Ph√¢n t√≠ch ƒëa khung.\n"
        "- /refresh [M√£] - L√†m m·ªõi d·ªØ li·ªáu cho m√£.\n"
        "- /getid - L·∫•y ID.\n"
        "- /approve [user_id] - Duy·ªát ng∆∞·ªùi d√πng (admin).\n"
        "- /datastats - Xem th·ªëng k√™ d·ªØ li·ªáu (admin).\n"
        "üí° **B·∫Øt ƒë·∫ßu n√†o!**"
    )

async def analyze_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return
    try:
        args = context.args
        if not args:
            raise ValueError("Nh·∫≠p m√£ ch·ª©ng kho√°n (e.g., VNINDEX, SSI).")
        symbol = args[0].upper()
        num_candles = int(args[1]) if len(args) > 1 else DEFAULT_CANDLES
        if num_candles < 20:
            raise ValueError("S·ªë n·∫øn ph·∫£i l·ªõn h∆°n ho·∫∑c b·∫±ng 20 ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o!")
        if num_candles > 500:
            raise ValueError("T·ªëi ƒëa 500 n·∫øn!")
            
        # Kh·ªüi t·∫°o c√°c l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu n√¢ng cao (n·∫øu ch∆∞a t·ªìn t·∫°i)
        data_loader = DataLoader(primary_source='vnstock', backup_sources=['yahoo'])
        data_quality_control = DataQualityControl()
        data_processor = AdvancedDataProcessor()
        tech_analyzer = TechnicalAnalyzer()
        ai_analyzer = AIAnalyzer()
        
        # Th√¥ng b√°o cho ng∆∞·ªùi d√πng
        processing_msg = await update.message.reply_text("‚è≥ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu v√† ph√¢n t√≠ch... Vui l√≤ng ƒë·ª£i.")
        
        # T·∫£i d·ªØ li·ªáu ƒëa khung th·ªùi gian
        timeframes = ['1D', '1W', '1M']
        dfs = {}
        outlier_reports = {}
        quality_reports = {}
        
        for tf in timeframes:
            # T·∫£i d·ªØ li·ªáu
            df, outlier_report = await data_loader.load_data(symbol, tf, num_candles)
            
            # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
            quality_metrics = data_quality_control.evaluate_data_quality(df, symbol)
            quality_report = f"Ch·∫•t l∆∞·ª£ng: {quality_metrics['overall_score']:.2f}/1.0"
            
            # X·ª≠ l√Ω d·ªØ li·ªáu n√¢ng cao
            if data_quality_control.is_data_usable(quality_metrics):
                df = data_processor.preprocess_data(df)
                
            # T√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
            df = tech_analyzer.calculate_indicators(df)
            
            # L∆∞u k·∫øt qu·∫£
            dfs[tf] = df
            outlier_reports[tf] = outlier_report
            quality_reports[tf] = quality_report
            
        # L·∫•y d·ªØ li·ªáu c∆° b·∫£n
        fundamental_data = await data_loader.get_fundamental_data(symbol)
        
        # T·∫°o b√°o c√°o
        report = await ai_analyzer.generate_report(dfs, symbol, fundamental_data, outlier_reports)
        
        # L∆∞u v√†o cache
        await redis_manager.set(f"report_{symbol}_{num_candles}", report, expire=CACHE_EXPIRE_SHORT)
        
        # Th√™m th√¥ng tin ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu v√†o b√°o c√°o
        quality_info = "\n".join([f"üîç {tf}: {report}" for tf, report in quality_reports.items()])
        formatted_report = f"<b>üìà B√°o c√°o ph√¢n t√≠ch cho {symbol}</b>\n<i>{quality_info}</i>\n\n<pre>{html.escape(report)}</pre>"
        
        # C·∫≠p nh·∫≠t ho·∫∑c g·ª≠i b√°o c√°o m·ªõi
        try:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=processing_msg.message_id,
                text=formatted_report,
                parse_mode='HTML'
            )
        except Exception:
            # N·∫øu kh√¥ng th·ªÉ ch·ªânh s·ª≠a tin nh·∫Øn (c√≥ th·ªÉ qu√° d√†i), g·ª≠i tin nh·∫Øn m·ªõi
            await update.message.reply_text(formatted_report, parse_mode='HTML')
            
    except ValueError as e:
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")
    except Exception as e:
        logger.error(f"L·ªói trong analyze_command: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")

async def get_id(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    await update.message.reply_text(f"ID c·ªßa b·∫°n: {user_id}")

async def approve_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if str(update.message.from_user.id) != ADMIN_ID:
        await update.message.reply_text("‚ùå Ch·ªâ admin d√πng ƒë∆∞·ª£c l·ªánh n√†y!")
        return
    if len(context.args) != 1:
        await update.message.reply_text("‚ùå Nh·∫≠p user_id: /approve 123456789")
        return
    user_id = context.args[0]
    if not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        await update.message.reply_text(f"‚úÖ ƒê√£ duy·ªát {user_id}")
    else:
        await update.message.reply_text(f"‚ÑπÔ∏è {user_id} ƒë√£ ƒë∆∞·ª£c duy·ªát")

async def data_stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh ƒë·ªÉ admin xem th·ªëng k√™ d·ªØ li·ªáu h·ªá th·ªëng."""
    user_id = update.message.from_user.id
    if str(user_id) != ADMIN_ID:
        await update.message.reply_text("‚ùå Ch·ªâ admin d√πng ƒë∆∞·ª£c l·ªánh n√†y!")
        return
        
    await update.message.reply_text("‚è≥ ƒêang t·ªïng h·ª£p th·ªëng k√™ d·ªØ li·ªáu...")
    
    try:
        # Kh·ªüi t·∫°o c√°c l·ªõp c·∫ßn thi·∫øt
        data_loader = DataLoader()
        data_quality = DataQualityControl()
        data_processor = AdvancedDataProcessor()
        
        data_manager = DataAutomationManager(data_loader, data_quality, data_processor)
        stats = await data_manager.get_data_statistics()
        
        # T·∫°o b√°o c√°o
        report = "üìä <b>TH·ªêNG K√ä D·ªÆ LI·ªÜU H·ªÜ TH·ªêNG</b>\n\n"
        report += f"üî¢ T·ªïng s·ªë m√£: {stats['total_symbols']}\n"
        report += f"üìà T·ªïng s·ªë ƒëi·ªÉm d·ªØ li·ªáu: {stats['total_datapoints']:,}\n"
        report += f"üóÑÔ∏è T·ªïng s·ªë kh√≥a cache: {stats['total_cache_keys']}\n\n"
        
        if stats['problem_symbols']:
            report += "‚ö†Ô∏è <b>M√É C√ì V·∫§N ƒê·ªÄ CH·∫§T L∆Ø·ª¢NG:</b>\n"
            for symbol_info in stats['problem_symbols'][:10]:  # Ch·ªâ hi·ªÉn th·ªã 10 m√£ ƒë·∫ßu ti√™n
                report += f"- {symbol_info['symbol']}: {symbol_info['score']:.2f}/1.0\n"
                
            if len(stats['problem_symbols']) > 10:
                report += f"... v√† {len(stats['problem_symbols']) - 10} m√£ kh√°c\n"
        else:
            report += "‚úÖ T·∫•t c·∫£ m√£ ƒë·ªÅu c√≥ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu t·ªët\n"
            
        # Th√¥ng tin b·ªô nh·ªõ
        if stats['memory_usage']:
            report += "\nüíæ <b>S·ª¨ D·ª§NG B·ªò NH·ªö REDIS:</b>\n"
            report += f"- ƒê√£ d√πng: {stats['memory_usage'].get('used_memory', 'N/A')}\n"
            report += f"- ƒê·ªânh: {stats['memory_usage'].get('used_memory_peak', 'N/A')}\n"
            report += f"- T·ªïng b·ªô nh·ªõ h·ªá th·ªëng: {stats['memory_usage'].get('total_system_memory', 'N/A')}\n"
            
        await update.message.reply_text(report, parse_mode='HTML')
        
    except Exception as e:
        logger.error(f"L·ªói l·∫•y th·ªëng k√™ d·ªØ li·ªáu: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói khi l·∫•y th·ªëng k√™ d·ªØ li·ªáu: {str(e)}")

async def refresh_data_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh ƒë·ªÉ l√†m m·ªõi d·ªØ li·ªáu cho m·ªôt m√£ c·ª• th·ªÉ."""
    user_id = update.message.from_user.id
    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return
        
    args = context.args
    if not args:
        await update.message.reply_text("‚ùå Vui l√≤ng nh·∫≠p m√£ ch·ª©ng kho√°n c·∫ßn l√†m m·ªõi d·ªØ li·ªáu.")
        return
        
    symbol = args[0].upper()
    await update.message.reply_text(f"‚è≥ ƒêang l√†m m·ªõi d·ªØ li·ªáu cho {symbol}...")
    
    try:
        # Kh·ªüi t·∫°o c√°c l·ªõp c·∫ßn thi·∫øt
        data_loader = DataLoader()
        timeframes = ['1D', '1W', '1M']
        
        # X√≥a cache hi·ªán t·∫°i
        for tf in timeframes:
            cache_key = f"data_vnstock_{symbol}_{tf}_{DEFAULT_CANDLES}"
            await redis_manager.redis_client.delete(cache_key)
            cache_key = f"data_yahoo_{symbol}_{tf}_{DEFAULT_CANDLES}"
            await redis_manager.redis_client.delete(cache_key)
            
        # T·∫£i d·ªØ li·ªáu m·ªõi
        results = []
        for tf in timeframes:
            try:
                df, report = await data_loader.load_data(symbol, tf, DEFAULT_CANDLES)
                results.append(f"‚úÖ {tf}: {len(df)} n·∫øn")
            except Exception as e:
                results.append(f"‚ùå {tf}: {str(e)}")
                
        # B√°o c√°o k·∫øt qu·∫£
        report = f"üîÑ <b>L√ÄM M·ªöI D·ªÆ LI·ªÜU: {symbol}</b>\n\n" + "\n".join(results)
        await update.message.reply_text(report, parse_mode='HTML')
        
    except Exception as e:
        logger.error(f"L·ªói l√†m m·ªõi d·ªØ li·ªáu cho {symbol}: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói l√†m m·ªõi d·ªØ li·ªáu: {str(e)}")

async def check_timestamp_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh ƒë·ªÉ ki·ªÉm tra v√† s·ª≠a timestamp cho m·ªôt m√£ c·ª• th·ªÉ."""
    user_id = update.message.from_user.id
    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return
        
    args = context.args
    if not args or len(args) < 1:
        await update.message.reply_text("‚ùå Vui l√≤ng nh·∫≠p: /checkts [M√£] [Khung th·ªùi gian: 1D, 1W, 1M (m·∫∑c ƒë·ªãnh 1D)]")
        return
        
    symbol = args[0].upper()
    timeframe = args[1].upper() if len(args) > 1 else '1D'
    
    if timeframe not in ['1D', '1W', '1M']:
        await update.message.reply_text("‚ùå Khung th·ªùi gian kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng: 1D, 1W, ho·∫∑c 1M")
        return
        
    await update.message.reply_text(f"‚è≥ ƒêang ki·ªÉm tra timestamp cho {symbol} ({timeframe})...")
    
    try:
        # Kh·ªüi t·∫°o loader v√† timestamp aligner
        data_loader = DataLoader(primary_source='vnstock', backup_sources=['yahoo'])
        
        # T·∫£i d·ªØ li·ªáu
        regular_df, _ = await data_loader.load_data(symbol, timeframe, 30)
        precise_df = await data_loader.get_precise_timestamp_data(symbol, timeframe, 30)
        
        # T·∫°o b√°o c√°o
        report = f"üïí <b>KI·ªÇM TRA TIMESTAMP CHO {symbol} ({timeframe})</b>\n\n"
        
        # So s√°nh s·ªë l∆∞·ª£ng n·∫øn
        regular_count = len(regular_df) if regular_df is not None else 0
        precise_count = len(precise_df) if precise_df is not None else 0
        
        report += f"üìä <b>S·ªê L∆Ø·ª¢NG N·∫æN:</b>\n"
        report += f"- D·ªØ li·ªáu th√¥ng th∆∞·ªùng: {regular_count} n·∫øn\n"
        report += f"- D·ªØ li·ªáu ƒë√£ cƒÉn ch·ªânh: {precise_count} n·∫øn\n\n"
        
        # Th√¥ng tin timestamp
        if precise_df is not None and not precise_df.empty:
            first_date = precise_df.index[0].strftime('%Y-%m-%d %H:%M')
            last_date = precise_df.index[-1].strftime('%Y-%m-%d %H:%M')
            
            report += f"üóìÔ∏è <b>PH·∫†M VI TH·ªúI GIAN:</b>\n"
            report += f"- T·ª´: {first_date}\n"
            report += f"- ƒê·∫øn: {last_date}\n\n"
            
            # Ki·ªÉm tra timezone
            timezone = str(precise_df.index[0].tz)
            report += f"üåê <b>TIMEZONE:</b> {timezone}\n\n"
            
            # Ki·ªÉm tra th·ªùi gian trong ng√†y
            hours = [idx.hour for idx in precise_df.index]
            minutes = [idx.minute for idx in precise_df.index]
            
            if len(set(hours)) == 1 and len(set(minutes)) == 1:
                report += f"‚úÖ <b>CHU·∫®N H√ìA TH·ªúI GIAN:</b> T·∫•t c·∫£ timestamp ƒë·ªÅu v√†o {hours[0]}:{minutes[0]}\n\n"
            else:
                report += f"‚ö†Ô∏è <b>CHU·∫®N H√ìA TH·ªúI GIAN:</b> Timestamp kh√¥ng ƒë·ªìng nh·∫•t!\n"
                report += f"- Gi·ªù kh√°c nhau: {set(hours)}\n"
                report += f"- Ph√∫t kh√°c nhau: {set(minutes)}\n\n"
                
            # Ki·ªÉm tra ng√†y giao d·ªãch
            weekdays = [idx.weekday() for idx in precise_df.index]
            weekday_counts = {
                0: "Th·ª© 2", 1: "Th·ª© 3", 2: "Th·ª© 4", 
                3: "Th·ª© 5", 4: "Th·ª© 6", 5: "Th·ª© 7", 6: "Ch·ªß nh·∫≠t"
            }
            
            if any(wd >= 5 for wd in weekdays):
                report += "‚ö†Ô∏è <b>NG√ÄY GIAO D·ªäCH:</b> Ph√°t hi·ªán ng√†y cu·ªëi tu·∫ßn trong d·ªØ li·ªáu!\n"
                for wd, count in sorted({wd: weekdays.count(wd) for wd in set(weekdays)}.items()):
                    report += f"- {weekday_counts[wd]}: {count} n·∫øn\n"
            else:
                report += "‚úÖ <b>NG√ÄY GIAO D·ªäCH:</b> T·∫•t c·∫£ ƒë·ªÅu l√† ng√†y trong tu·∫ßn (Th·ª© 2-6)\n"
                for wd, count in sorted({wd: weekdays.count(wd) for wd in set(weekdays)}.items()):
                    report += f"- {weekday_counts[wd]}: {count} n·∫øn\n"
        else:
            report += "‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu sau khi cƒÉn ch·ªânh timestamp"
            
        await update.message.reply_text(report, parse_mode='HTML')
        
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra timestamp cho {symbol}: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")

# Th√™m l·ªánh m·ªõi v√†o main
def add_timestamp_commands(app):
    app.add_handler(CommandHandler("checkts", check_timestamp_command))
    logger.info("ƒê√£ ƒëƒÉng k√Ω l·ªánh ki·ªÉm tra timestamp")

# ---------- MAIN & DEPLOY ----------
async def main():
    await init_db()

    # Kh·ªüi t·∫°o c√°c l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu n√¢ng cao
    data_loader = DataLoader(primary_source='vnstock', backup_sources=['yahoo'])
    data_quality_control = DataQualityControl()
    data_processor = AdvancedDataProcessor()
    
    # Thi·∫øt l·∫≠p scheduler
    scheduler = AsyncIOScheduler()
    
    # T·ª± ƒë·ªông h√≥a qu·∫£n l√Ω d·ªØ li·ªáu
    data_automation = DataAutomationManager(
        data_loader=data_loader,
        quality_control=data_quality_control,
        data_processor=data_processor,
        scheduler=scheduler
    )
    
    # Th√™m t√°c v·ª• auto training
    scheduler.add_job(auto_train_models, 'cron', hour=2, minute=0, id='auto_train_models', replace_existing=True)
    
    # Thi·∫øt l·∫≠p t·ª± ƒë·ªông h√≥a d·ªØ li·ªáu
    data_automation.setup_data_automation()
    
    # N·∫øu ƒë√£ c√≥ d·ªØ li·ªáu l·ªãch s·ª≠, thi·∫øt l·∫≠p c√°c m√£ ∆∞u ti√™n
    training_symbols = await get_training_symbols()
    if training_symbols:
        data_automation.set_priority_symbols(training_symbols)
        logger.info(f"ƒê√£ thi·∫øt l·∫≠p {len(training_symbols)} m√£ ∆∞u ti√™n t·ª´ l·ªãch s·ª≠")
    
    # Kh·ªüi ƒë·ªông scheduler
    scheduler.start()
    logger.info("C√°c t√°c v·ª• t·ª± ƒë·ªông ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông")

    # Kh·ªüi t·∫°o ·ª©ng d·ª•ng Telegram
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("analyze", analyze_command))
    app.add_handler(CommandHandler("getid", get_id))
    app.add_handler(CommandHandler("approve", approve_user))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, notify_admin_new_user))
    logger.info("ü§ñ Bot kh·ªüi ƒë·ªông! Phi√™n b·∫£n V18.8.1T (N√¢ng c·∫•p t·∫£i d·ªØ li·ªáu)")

    BASE_URL = os.getenv("RENDER_EXTERNAL_URL", f"https://{os.getenv('RENDER_SERVICE_NAME')}.onrender.com")
    WEBHOOK_URL = f"{BASE_URL}/{TELEGRAM_TOKEN}"
    await app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        webhook_url=WEBHOOK_URL,
        url_path=TELEGRAM_TOKEN
    )

if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        import unittest

        class TestTechnicalAnalysis(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=50, freq='D')
                data = {
                    'open': np.random.random(50) * 100,
                    'high': np.random.random(50) * 100,
                    'low': np.random.random(50) * 100,
                    'close': np.linspace(100, 150, 50),
                    'volume': np.random.randint(1000, 5000, 50)
                }
                self.df = pd.DataFrame(data, index=dates)

            def test_calculate_indicators(self):
                analyzer = TechnicalAnalyzer()
                df_processed = analyzer.calculate_indicators(self.df)
                self.assertIn('sma20', df_processed.columns)
                self.assertIn('rsi', df_processed.columns)

        class TestForecastProphet(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=100, freq='D')
                self.df = pd.DataFrame({
                    'close': np.linspace(100, 200, 100),
                    'open': np.linspace(100, 200, 100),
                    'high': np.linspace(100, 200, 100),
                    'low': np.linspace(100, 200, 100),
                    'volume': np.random.randint(1000, 5000, 100)
                }, index=dates)

            def test_forecast_with_prophet(self):
                forecast, model = forecast_with_prophet(self.df, periods=7)
                self.assertFalse(forecast.empty)
                self.assertIn('yhat', forecast.columns)

        class TestOutlierDetection(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=7, freq='D')
                self.df = pd.DataFrame({
                    'close': [100, 101, 102, 103, 500, 104, 105]
                }, index=dates)

            def test_detect_outliers(self):
                loader = DataLoader()
                df_with_outliers, report = loader.detect_outliers(self.df)
                self.assertIn('is_outlier', df_with_outliers.columns)
                self.assertEqual(df_with_outliers['is_outlier'].sum(), 1)
                self.assertIn('500', report)

        unittest.main(argv=[sys.argv[0]])
    else:
        import nest_asyncio
        nest_asyncio.apply()
        asyncio.run(main())