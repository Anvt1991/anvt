#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Bot Ch·ª©ng Kho√°n To√†n Di·ªán Phi√™n B·∫£n V18.8 (N√¢ng c·∫•p):
- T√≠ch h·ª£p AI OpenRouter cho ph√¢n t√≠ch m·∫´u h√¨nh, s√≥ng, v√† n·∫øn nh·∫≠t.
- S·ª≠ d·ª•ng m√¥ h√¨nh anthropic/claude-3-haiku.
- ƒê·∫£m b·∫£o c√°c ch·ª©c nƒÉng v√† c√¥ng ngh·ªá hi·ªán c√≥ kh√¥ng b·ªã ·∫£nh h∆∞·ªüng.
"""

import os
import sys
import io
import logging
import pickle
from datetime import datetime, timedelta
import asyncio
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import numpy as np
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

import yfinance as yf
from vnstock import Vnstock
import google.generativeai as genai

from ta import trend, momentum, volatility
from ta.volume import MFIIndicator
import feedparser

import redis.asyncio as redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import Column, Integer, String, Float, Text, DateTime, select, LargeBinary

import xgboost as xgb
from sklearn.metrics import accuracy_score
from prophet import Prophet

import matplotlib.pyplot as plt
import holidays
import html

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from tenacity import retry, stop_after_attempt, wait_exponential

import aiohttp
import json

# ---------- C·∫§U H√åNH & LOGGING ----------
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
ADMIN_ID = os.getenv("ADMIN_ID", "1225226589")
PORT = int(os.environ.get("PORT", 10000))
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
RENDER_SERVICE_NAME = os.getenv("RENDER_SERVICE_NAME", "")

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

CACHE_EXPIRE_SHORT = 1800
CACHE_EXPIRE_MEDIUM = 3600
CACHE_EXPIRE_LONG = 86400
NEWS_CACHE_EXPIRE = 900
DEFAULT_CANDLES = 100
DEFAULT_TIMEFRAME = '1D'

executor = ThreadPoolExecutor(max_workers=5)

async def run_in_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, lambda: func(*args, **kwargs))

# ---------- K·∫æT N·ªêI REDIS (Async) ----------
class RedisManager:
    def __init__(self):
        try:
            self.redis_client = redis.from_url(REDIS_URL)
            logger.info("K·∫øt n·ªëi Redis th√†nh c√¥ng.")
        except Exception as e:
            logger.error(f"L·ªói k·∫øt n·ªëi Redis: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def set(self, key, value, expire):
        try:
            serialized_value = pickle.dumps(value)
            await self.redis_client.set(key, serialized_value, ex=expire)
            return True
        except Exception as e:
            logger.error(f"L·ªói Redis set: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def get(self, key):
        try:
            data = await self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            logger.error(f"L·ªói Redis get: {str(e)}")
            return None

redis_manager = RedisManager()

# ---------- K·∫æT N·ªêI POSTGRESQL (Async) ----------
Base = declarative_base()

class ApprovedUser(Base):
    __tablename__ = 'approved_users'
    id = Column(Integer, primary_key=True)
    user_id = Column(String, unique=True, nullable=False)
    approved_at = Column(DateTime, default=datetime.now)

class ReportHistory(Base):
    __tablename__ = 'report_history'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    date = Column(String, nullable=False)
    report = Column(Text, nullable=False)
    close_today = Column(Float, nullable=False)
    close_yesterday = Column(Float, nullable=False)
    timestamp = Column(DateTime, default=datetime.now)

class TrainedModel(Base):
    __tablename__ = 'trained_models'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    model_type = Column(String, nullable=False)
    model_blob = Column(LargeBinary, nullable=False)
    created_at = Column(DateTime, default=datetime.now)
    performance = Column(Float, nullable=True)

engine = create_async_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine, class_=AsyncSession, expire_on_commit=False)

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

class DBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def is_user_approved(self, user_id) -> bool:
        try:
            async with self.Session() as session:
                result = await session.execute(select(ApprovedUser).filter_by(user_id=str(user_id)))
                return result.scalar_one_or_none() is not None or str(user_id) == ADMIN_ID
        except Exception as e:
            logger.error(f"L·ªói ki·ªÉm tra ng∆∞·ªùi d√πng: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def add_approved_user(self, user_id, approved_at=None) -> None:
        try:
            async with self.Session() as session:
                if not await self.is_user_approved(user_id) and str(user_id) != ADMIN_ID:
                    new_user = ApprovedUser(user_id=str(user_id), approved_at=approved_at or datetime.now())
                    session.add(new_user)
                    await session.commit()
                    logger.info(f"Th√™m ng∆∞·ªùi d√πng ƒë∆∞·ª£c ph√™ duy·ªát: {user_id}")
        except Exception as e:
            logger.error(f"L·ªói th√™m ng∆∞·ªùi d√πng: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_report_history(self, symbol: str) -> list:
        try:
            async with self.Session() as session:
                reports = await session.execute(select(ReportHistory).filter_by(symbol=symbol).order_by(ReportHistory.id.asc()))
                reports = reports.scalars().all()
                return [
                    {
                        "id": report.id,
                        "symbol": report.symbol,
                        "date": report.date,
                        "report": report.report,
                        "close_today": report.close_today,
                        "close_yesterday": report.close_yesterday,
                        "timestamp": report.timestamp.isoformat()
                    }
                    for report in reports
                ]
        except Exception as e:
            logger.error(f"L·ªói t·∫£i l·ªãch s·ª≠ b√°o c√°o: {str(e)}")
            return []

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def save_report_history(self, symbol: str, report: str, close_today: float, close_yesterday: float) -> None:
        try:
            async with self.Session() as session:
                date_str = datetime.now().strftime('%Y-%m-%d')
                standardized_data = standardize_data_for_db({
                    'symbol': symbol,
                    'date': date_str,
                    'report': report,
                    'close_today': close_today,
                    'close_yesterday': close_yesterday
                })
                new_report = ReportHistory(**standardized_data)
                session.add(new_report)
                await session.commit()
                logger.info(f"L∆∞u b√°o c√°o m·ªõi cho {symbol}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u b√°o c√°o: {str(e)}")
            raise

db = DBManager()

# ---------- QU·∫¢N L√ù M√î H√åNH (Prophet & XGBoost) ----------
class ModelDBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def store_trained_model(self, symbol: str, model_type: str, model, performance: float = None):
        try:
            model_blob = pickle.dumps(model)
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                existing = result.scalar_one_or_none()
                if existing:
                    existing.model_blob = model_blob
                    existing.created_at = datetime.now()
                    existing.performance = performance
                else:
                    new_model = TrainedModel(symbol=symbol, model_type=model_type, model_blob=model_blob, performance=performance)
                    session.add(new_model)
                await session.commit()
            logger.info(f"L∆∞u m√¥ h√¨nh {model_type} cho {symbol} th√†nh c√¥ng v·ªõi hi·ªáu su·∫•t: {performance}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_trained_model(self, symbol: str, model_type: str):
        try:
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                model_record = result.scalar_one_or_none()
                if model_record:
                    model = pickle.loads(model_record.model_blob)
                    return model, model_record.performance
            return None, None
        except Exception as e:
            logger.error(f"L·ªói t·∫£i m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")
            return None, None

model_db_manager = ModelDBManager()

# ---------- H√ÄM H·ªñ TR·ª¢ ----------
def is_index(symbol: str) -> bool:
    indices = ['VNINDEX', 'VN30', 'HNX30', 'HNXINDEX', 'UPCOM']
    return symbol.upper() in indices

async def is_user_approved(user_id) -> bool:
    return await db.is_user_approved(user_id)

def standardize_data_for_db(data: dict) -> dict:
    standardized_data = {}
    for key, value in data.items():
        if isinstance(value, np.float64):
            standardized_data[key] = float(value)
        elif isinstance(value, np.int64):
            standardized_data[key] = int(value)
        elif isinstance(value, pd.Timestamp):
            standardized_data[key] = value.to_pydatetime()
        else:
            standardized_data[key] = value
    return standardized_data

# ---------- H√ÄM H·ªñ TR·ª¢: L·ªåC NG√ÄY GIAO D·ªäCH -----------
def filter_trading_days(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    df = df[df.index.weekday < 5]
    years = df.index.year.unique()
    vn_holidays = holidays.Vietnam(years=years)
    holiday_dates = set(vn_holidays.keys())
    df = df[~pd.to_datetime(df.index.date).isin(holiday_dates)]
    return df

# ---------- T·∫¢I D·ªÆ LI·ªÜU (N√ÇNG C·∫§P) ----------
class DataLoader:
    def __init__(self, source: str = 'vnstock'):
        self.source = source

    def detect_outliers(self, df: pd.DataFrame) -> (pd.DataFrame, str):
        if 'close' not in df.columns:
            return df, "Kh√¥ng c√≥ c·ªôt 'close' ƒë·ªÉ ph√°t hi·ªán outlier"
        z_scores = np.abs((df['close'] - df['close'].mean()) / df['close'].std())
        threshold = 3
        df['is_outlier'] = z_scores > threshold
        outliers = df[df['is_outlier']]
        outlier_report = f"Ph√°t hi·ªán {len(outliers)} gi√° tr·ªã b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu:\n"
        for idx, row in outliers.iterrows():
            outlier_report += f"- {idx.strftime('%Y-%m-%d')}: {row['close']:.2f}\n"
        return df, outlier_report if not outliers.empty else "Kh√¥ng c√≥ gi√° tr·ªã b·∫•t th∆∞·ªùng"

    async def load_data(self, symbol: str, timeframe: str, num_candles: int) -> (pd.DataFrame, str):
        timeframe_map = {'1d': '1D', '1w': '1W', '1mo': '1M'}
        timeframe = timeframe_map.get(timeframe.lower(), timeframe).upper()
        
        expire = CACHE_EXPIRE_SHORT if timeframe == '1D' else CACHE_EXPIRE_MEDIUM if timeframe == '1W' else CACHE_EXPIRE_LONG
        
        cache_key = f"data_{self.source}_{symbol}_{timeframe}_{num_candles}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data, "D·ªØ li·ªáu t·ª´ cache, kh√¥ng ki·ªÉm tra outlier"

        try:
            if self.source == 'vnstock':
                @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
                def fetch_vnstock():
                    stock = Vnstock().stock(symbol=symbol, source='TCBS')
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=(num_candles + 1) * 3)).strftime('%Y-%m-%d')
                    df = stock.quote.history(start=start_date, end=end_date, interval=timeframe)
                    if df is None or df.empty or len(df) < 20:
                        raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {'ch·ªâ s·ªë' if is_index(symbol) else 'm√£'} {symbol}")
                    df = df.rename(columns={'time': 'date', 'open': 'open', 'high': 'high',
                                            'low': 'low', 'close': 'close', 'volume': 'volume'})
                    df['date'] = pd.to_datetime(df['date'])
                    df = df.set_index('date')
                    df.index = df.index.tz_localize('Asia/Bangkok')
                    df = df[['open', 'high', 'low', 'close', 'volume']].dropna()
                    if 'close' not in df.columns:
                        raise ValueError(f"D·ªØ li·ªáu cho {symbol} kh√¥ng c√≥ c·ªôt 'close'")
                    if not (df['high'] >= df['low']).all() or not ((df['close'] >= df['low']) & (df['close'] <= df['high'])).all():
                        raise ValueError(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho {symbol}")
                    if len(df) < 200:
                        logger.warning(f"D·ªØ li·ªáu cho {symbol} d∆∞·ªõi 200 n·∫øn, SMA200 c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")
                    return df.tail(num_candles + 1)
                df = await run_in_thread(fetch_vnstock)
            elif self.source == 'yahoo':
                period_map = {'1D': 'd', '1W': 'wk', '1M': 'mo'}
                df = await self._download_yahoo_data(symbol, num_candles + 1, period_map.get(timeframe, 'd'))
                if df is None or df.empty or len(df) < 20:
                    raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {symbol} t·ª´ Yahoo Finance")
                df = df.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low',
                                        'Close': 'close', 'Volume': 'volume'})
                df = df[['open', 'high', 'low', 'close', 'volume']].dropna()
                df.index = df.index.tz_localize('Asia/Bangkok')
                if 'close' not in df.columns:
                    raise ValueError(f"D·ªØ li·ªáu cho {symbol} kh√¥ng c√≥ c·ªôt 'close'")
                if not (df['high'] >= df['low']).all() or not ((df['close'] >= df['low']) & (df['close'] <= df['high'])).all():
                    raise ValueError(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho {symbol}")
                if len(df) < 200:
                    logger.warning(f"D·ªØ li·ªáu cho {symbol} d∆∞·ªõi 200 n·∫øn, SMA200 c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")
            else:
                raise ValueError("Ngu·ªìn d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá")

            trading_df = filter_trading_days(df)
            trading_df, outlier_report = self.detect_outliers(trading_df)
            await redis_manager.set(cache_key, trading_df, expire=expire)
            return trading_df, outlier_report
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu cho {symbol}: {str(e)}")
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8), reraise=True)
    async def _download_yahoo_data(self, symbol: str, num_candles: int, period: str) -> pd.DataFrame:
        import aiohttp
        try:
            async with aiohttp.ClientSession() as session:
                start_ts = int((datetime.now() - timedelta(days=num_candles * 3)).timestamp())
                end_ts = int(datetime.now().timestamp())
                url = (f"https://query1.finance.yahoo.com/v7/finance/download/{symbol}"
                       f"?period1={start_ts}&period2={end_ts}&interval=1{period}&events=history")
                async with asyncio.wait_for(session.get(url), timeout=15) as response:
                    if response.status != 200:
                        raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ Yahoo, HTTP {response.status}")
                    text = await response.text()
                    df = pd.read_csv(io.StringIO(text))
                    if df.empty:
                        raise ValueError("D·ªØ li·ªáu Yahoo r·ªóng")
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.set_index('Date')
                    return df.tail(num_candles)
        except asyncio.TimeoutError:
            logger.error("Timeout khi t·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance.")
            raise
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu Yahoo: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def fetch_fundamental_data_vnstock(self, symbol: str) -> dict:
        cache_key = f"fundamental_vnstock_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = Vnstock().stock(symbol=symbol, source='TCBS')
            fundamental_data = {}
            ratios = stock.finance.ratio()
            if ratios is not None and not ratios.empty:
                fundamental_data.update(ratios.iloc[-1].to_dict())
            if hasattr(stock.finance, 'valuation'):
                valuation = stock.finance.valuation()
                if valuation is not None and not valuation.empty:
                    fundamental_data.update(valuation.iloc[-1].to_dict())
            if not fundamental_data:
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock")
            return fundamental_data

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock: {str(e)}")
            return {}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def fetch_fundamental_data_yahoo(self, symbol: str) -> dict:
        cache_key = f"fundamental_yahoo_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = yf.Ticker(f"{symbol}.VN")
            info = stock.info
            fundamental_data = {
                'EPS': info.get('trailingEps', None),
                'P/E': info.get('trailingPE', None),
                'P/B': info.get('priceToBook', None),
                'ROE': info.get('returnOnEquity', None),
                'Dividend Yield': info.get('dividendYield', None),
                'Market Cap': info.get('marketCap', None),
                'BVPS': info.get('bookValue', None)
            }
            if all(v is None for v in fundamental_data.values()):
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo Finance")
            return fundamental_data

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo: {str(e)}")
            return {}

    async def get_fundamental_data(self, symbol: str) -> dict:
        if is_index(symbol):
            return {"error": f"{symbol} l√† ch·ªâ s·ªë, kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n"}
        fundamental_data = await self.fetch_fundamental_data_vnstock(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        fundamental_data = await self.fetch_fundamental_data_yahoo(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        return {"error": f"Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n cho {symbol}"}

# ---------- PH√ÇN T√çCH K·ª∏ THU·∫¨T ----------
class TechnicalAnalyzer:
    @staticmethod
    def _calculate_common_indicators(df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty:
            raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ t√≠nh to√°n ch·ªâ b√°o")
        if 'close' not in df.columns:
            raise ValueError("D·ªØ li·ªáu kh√¥ng c√≥ c·ªôt 'close' c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o")
        if len(df) < 20:
            raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh to√°n SMA20 (c·∫ßn √≠t nh·∫•t 20 n·∫øn)")
        try:
            df['sma20'] = trend.SMAIndicator(df['close'], window=20).sma_indicator()
            df['sma50'] = trend.SMAIndicator(df['close'], window=50).sma_indicator()
            df['sma200'] = trend.SMAIndicator(df['close'], window=200).sma_indicator() if len(df) >= 200 else np.nan
            df['rsi'] = momentum.RSIIndicator(df['close'], window=14).rsi() if len(df) >= 14 else np.nan
            macd = trend.MACD(df['close'])
            df['macd'] = macd.macd()
            df['signal'] = macd.macd_signal()
            bb = volatility.BollingerBands(df['close'])
            df['bb_high'] = bb.bollinger_hband()
            df['bb_low'] = bb.bollinger_lband()
            ichimoku = trend.IchimokuIndicator(df['high'], df['low'])
            df['ichimoku_a'] = ichimoku.ichimoku_a()
            df['ichimoku_b'] = ichimoku.ichimoku_b()
            df['vwap'] = (df['volume'] * (df['high'] + df['low'] + df['close']) / 3).cumsum() / df['volume'].cumsum()
            df['mfi'] = MFIIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume'], window=14).money_flow_index()
            ichimoku_short = trend.IchimokuIndicator(df['high'], df['low'], window1=9, window2=26)
            df['tenkan_sen'] = ichimoku_short.ichimoku_a()
            ichimoku_medium = trend.IchimokuIndicator(df['high'], df['low'], window1=26, window2=52)
            df['kijun_sen'] = ichimoku_medium.ichimoku_b()
            df['chikou_span'] = df['close'].shift(-26) if len(df) > 26 else np.nan
            high_price = df['high'].max()
            low_price = df['low'].min()
            diff = high_price - low_price
            df['fib_0.0'] = high_price
            df['fib_23.6'] = high_price - 0.236 * diff
            df['fib_38.2'] = high_price - 0.382 * diff
            df['fib_50.0'] = high_price - 0.5 * diff
            df['fib_61.8'] = high_price - 0.618 * diff
            df['fib_100.0'] = low_price
            return df
        except Exception as e:
            logger.error(f"L·ªói t√≠nh to√°n ch·ªâ s·ªë k·ªπ thu·∫≠t: {str(e)}")
            raise

    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        return self._calculate_common_indicators(df)

    def calculate_multi_timeframe_indicators(self, dfs: dict) -> dict:
        indicators = {}
        for timeframe, df in dfs.items():
            try:
                df_processed = self._calculate_common_indicators(df)
                indicators[timeframe] = df_processed.tail(1).to_dict(orient='records')[0]
            except Exception as e:
                logger.error(f"L·ªói t√≠nh to√°n cho khung {timeframe}: {str(e)}")
                indicators[timeframe] = {}
        return indicators

# ---------- THU TH·∫¨P TIN T·ª®C (S·ª¨A L·ªñI) ----------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
async def fetch_rss_feed(url: str) -> str:
    import aiohttp
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    raise ValueError(f"HTTP {response.status} khi l·∫•y RSS t·ª´ {url}")
    except asyncio.TimeoutError:
        logger.error(f"Timeout khi l·∫•y RSS t·ª´ {url}")
    except Exception as e:
        logger.error(f"L·ªói l·∫•y RSS t·ª´ {url}: {str(e)}")
    return None

async def get_news(symbol: str = None, limit: int = 3) -> list:
    cache_key = f"news_{symbol}_{limit}" if symbol else f"news_market_{limit}"
    cached_news = await redis_manager.get(cache_key)
    if cached_news is not None:
        return cached_news

    rss_urls = [
        "https://cafef.vn/thi-truong-chung-khoan.rss",
        "https://cafef.vn/smart-money.rss",
        "https://cafef.vn/tai-chinh-ngan-hang.rss",
        "https://cafef.vn/doanh-nghiep.rss",
        "https://vnexpress.net/rss/kinh-doanh.rss",
        "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
        "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml"
    ]

    market_keywords = ["market", "stock", "index", "economy", "th·ªã tr∆∞·ªùng", "ch·ª©ng kho√°n", "l√£i su·∫•t", "vnindex"]
    symbol_keywords = [symbol.lower(), f"{symbol.lower()} ", f"m√£ {symbol.lower()}"] if symbol else []

    news_list = []
    tasks = [fetch_rss_feed(url) for url in rss_urls]
    rss_results = await asyncio.gather(*tasks)
    for rss_text in rss_results:
        if rss_text is None:
            continue
        feed = parse_rss_content(rss_text)
        if not feed or not feed.entries:
            continue
        for entry in feed.entries[:2]:
            title = entry.get("title", "").strip()
            link = entry.get("link", "")
            summary = entry.get("summary", entry.get("description", "")).strip()
            pub_date = entry.get("published", datetime.now().isoformat())
            content = f"{title} {summary}".lower()
            match = False
            if symbol and any(kw in content for kw in symbol_keywords):
                match = True
            elif not symbol and any(kw in content for kw in market_keywords):
                match = True
            if match:
                news_list.append({
                    "title": title,
                    "link": link,
                    "summary": summary[:200] + "..." if len(summary) > 200 else summary,
                    "published": pub_date
                })
    unique_news = sorted(news_list, key=lambda x: x.get("published", ""), reverse=True)[:limit]
    result = unique_news if unique_news else [{"title": "‚ö†Ô∏è Kh√¥ng c√≥ tin t·ª©c", "link": "#", "summary": ""}]
    await redis_manager.set(cache_key, result, expire=NEWS_CACHE_EXPIRE)
    return result

def parse_rss_content(rss_text: str):
    try:
        return feedparser.parse(rss_text) if rss_text else None
    except Exception as e:
        logger.error(f"L·ªói ph√¢n t√≠ch RSS: {str(e)}")
        return None

# ---------- PH√ÇN T√çCH C∆† B·∫¢N ----------
def deep_fundamental_analysis(fundamental_data: dict) -> str:
    report = "üìä **Ph√¢n t√≠ch c∆° b·∫£n**:\n"
    if not fundamental_data or 'error' in fundamental_data:
        return report + f"‚ùå {fundamental_data.get('error', 'Kh√¥ng c√≥ d·ªØ li·ªáu')}\n"

    keys_mapping = {
        'EPS': ['EPS', 'eps', 'Earning Per Share'],
        'P/E': ['P/E', 'PE', 'pe', 'Price to Earning', 'trailingPE'],
        'P/B': ['P/B', 'PB', 'pb', 'Price to Book', 'priceToBook'],
        'ROE': ['ROE', 'roe', 'Return on Equity', 'returnOnEquity'],
        'Dividend Yield': ['Dividend Yield', 'DY', 'dividend_yield', 'dividendYield'],
        'BVPS': ['BVPS', 'Book Value Per Share', 'bookValue'],
        'Market Cap': ['Market Cap', 'market_cap', 'marketCap']
    }

    extracted = {}
    for standard_key, possible_keys in keys_mapping.items():
        for key in possible_keys:
            if key in fundamental_data and fundamental_data[key] is not None:
                extracted[standard_key] = fundamental_data[key]
                break

    for key, value in extracted.items():
        report += f"- **{key}**: {value:.2f}\n" if isinstance(value, (int, float)) else f"- **{key}**: {value}\n"

    if 'P/E' in extracted and isinstance(extracted['P/E'], (int, float)):
        pe = extracted['P/E']
        report += "- **P/E**: C·ªï phi·∫øu c√≥ th·ªÉ ƒë·ªãnh gi√° th·∫•p\n" if pe < 10 else "- **P/E**: ƒê·ªãnh gi√° cao\n" if pe > 20 else "- **P/E**: ƒê·ªãnh gi√° h·ª£p l√Ω\n"

    if 'ROE' in extracted and isinstance(extracted['ROE'], (int, float)):
        report += "- **ROE**: Hi·ªáu qu·∫£ s·ª≠ d·ª•ng v·ªën t·ªët\n" if extracted['ROE'] > 15 else "- **ROE**: C·∫ßn c·∫£i thi·ªán\n"

    if 'Dividend Yield' in extracted and isinstance(extracted['Dividend Yield'], (int, float)):
        report += "- **Dividend**: H·∫•p d·∫´n\n" if extracted['Dividend Yield'] > 5 else "- **Dividend**: Trung b√¨nh\n"

    return report

# ---------- HU·∫§N LUY·ªÜN V√Ä L∆ØU M√î H√åNH ----------
def prepare_data_for_prophet(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ d·ª± b√°o")
    df_reset = df.reset_index()
    df_reset['date'] = df_reset['date'].dt.tz_localize(None)
    df_reset = df_reset.rename(columns={'date': 'ds', 'close': 'y'})
    return df_reset[['ds', 'y']]

def get_vietnam_holidays(years) -> pd.DataFrame:
    holiday_list = []
    for year in years:
        vn_holidays = holidays.Vietnam(years=year)
        for date, name in vn_holidays.items():
            holiday_list.append({'ds': pd.to_datetime(date), 'holiday': name})
    holiday_df = pd.DataFrame(holiday_list)
    holiday_df['ds'] = holiday_df['ds'].dt.tz_localize(None)
    return holiday_df

def forecast_with_prophet(df: pd.DataFrame, periods: int = 7) -> (pd.DataFrame, Prophet):
    try:
        data = prepare_data_for_prophet(df)
        if data.empty:
            raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o Prophet.")
        current_year = datetime.now().year
        holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
        model = Prophet(holidays=holiday_df)
        model.fit(data)
        future = model.make_future_dataframe(periods=periods)
        forecast = model.predict(future)
        return forecast, model
    except Exception as e:
        logger.error(f"L·ªói d·ª± b√°o Prophet: {str(e)}")
        raise

def evaluate_prophet_performance(df: pd.DataFrame, forecast: pd.DataFrame) -> float:
    actual = df['close'].iloc[-len(forecast):].values
    predicted = forecast['yhat'].values[:len(actual)]
    if len(actual) < 1 or len(predicted) < 1:
        return 0.0
    mse = np.mean((actual - predicted) ** 2)
    return 1 / (1 + mse)

def predict_xgboost_signal(df: pd.DataFrame, features: list) -> (int, float):
    if df is None or df.empty:
        return "D·ªØ li·ªáu r·ªóng", 0.0
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu", 0.0
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    X_pred = X.iloc[-1:]
    y_test = y.iloc[-1:]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    pred = model.predict(X_pred)[0]
    accuracy = accuracy_score(y_test, [pred])
    return pred, accuracy

def train_prophet_model(df: pd.DataFrame) -> (Prophet, float):
    data = prepare_data_for_prophet(df)
    if data.empty:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán Prophet")
    current_year = datetime.now().year
    holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
    model = Prophet(holidays=holiday_df)
    model.fit(data)
    future = model.make_future_dataframe(periods=0)
    forecast = model.predict(future)
    performance = evaluate_prophet_performance(df, forecast)
    return model, performance

def train_xgboost_model(df: pd.DataFrame, features: list) -> (xgb.XGBClassifier, float):
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ hu·∫•n luy·ªán XGBoost")
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán XGBoost")
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_train)
    performance = accuracy_score(y_train, y_pred)
    return model, performance

# ---------- AUTO TRAINING ----------
async def get_training_symbols() -> list:
    try:
        async with SessionLocal() as session:
            result = await session.execute(select(ReportHistory.symbol).distinct())
            symbols = [row[0] for row in result.fetchall()]
            logger.info(f"C√°c m√£ ƒë∆∞·ª£c l·∫•y t·ª´ l·ªãch s·ª≠ b√°o c√°o: {symbols}")
            return symbols
    except Exception as e:
        logger.error(f"L·ªói truy v·∫•n symbols t·ª´ ReportHistory: {str(e)}")
        return []

async def train_models_for_symbol(symbol: str):
    try:
        logger.info(f"B·∫Øt ƒë·∫ßu auto training cho m√£: {symbol}")
        loader = DataLoader()
        tech_analyzer = TechnicalAnalyzer()
        df, _ = await loader.load_data(symbol, '1D', 500)
        df = tech_analyzer.calculate_indicators(df)
        features = ['sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal',
                    'bb_high', 'bb_low', 'ichimoku_a', 'ichimoku_b', 'vwap', 'mfi']
        task_prophet = asyncio.to_thread(train_prophet_model, df)
        task_xgb = asyncio.to_thread(train_xgboost_model, df, features)
        (prophet_model, prophet_perf), (xgb_model, xgb_perf) = await asyncio.gather(task_prophet, task_xgb)
        await model_db_manager.store_trained_model(symbol, 'prophet', prophet_model, prophet_perf)
        await model_db_manager.store_trained_model(symbol, 'xgboost', xgb_model, xgb_perf)
        logger.info(f"Auto training cho {symbol} ho√†n t·∫•t.")
    except Exception as e:
        logger.error(f"L·ªói auto training cho {symbol}: {str(e)}")

async def auto_train_models():
    try:
        symbols = await get_training_symbols()
        if not symbols:
            logger.info("Kh√¥ng c√≥ m√£ n√†o trong ReportHistory, b·ªè qua auto training.")
            return
        tasks = [train_models_for_symbol(symbol) for symbol in symbols]
        await asyncio.gather(*tasks)
    except Exception as e:
        logger.error(f"L·ªói auto training: {str(e)}")

# ---------- AI V√Ä B√ÅO C√ÅO ----------
class AIAnalyzer:
    def __init__(self):
        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-1.5-pro')

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def generate_content(self, prompt):
        return await self.model.generate_content_async(prompt)

    async def load_report_history(self, symbol: str) -> list:
        return await db.load_report_history(symbol)

    async def save_report_history(self, symbol: str, report: str, close_today: float, close_yesterday: float) -> None:
        await db.save_report_history(symbol, report, close_today, close_yesterday)

    def analyze_price_action(self, df: pd.DataFrame) -> str:
        if len(df) < 6:
            return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch."
        last_5_days = df['close'].tail(5)
        changes = last_5_days.pct_change().dropna()
        trend_summary = []
        for i, change in enumerate(changes):
            date = last_5_days.index[i+1].strftime('%Y-%m-%d')
            if 'is_outlier' in df.columns and df.loc[last_5_days.index[i+1], 'is_outlier']:
                outlier_note = " (‚ö†Ô∏è outlier)"
            else:
                outlier_note = ""
            if change > 0:
                trend_summary.append(f"{date}: TƒÉng {change*100:.2f}%{outlier_note}")
            elif change < 0:
                trend_summary.append(f"{date}: Gi·∫£m {-change*100:.2f}%{outlier_note}")
            else:
                trend_summary.append(f"{date}: Kh√¥ng ƒë·ªïi{outlier_note}")
        consecutive_up = consecutive_down = 0
        for change in changes[::-1]:
            if change > 0:
                consecutive_up += 1
                consecutive_down = 0
            elif change < 0:
                consecutive_down += 1
                consecutive_up = 0
            else:
                break
        if consecutive_up >= 3:
            summary = f"‚úÖ Gi√° tƒÉng {consecutive_up} phi√™n li√™n ti·∫øp.\n"
        elif consecutive_down >= 3:
            summary = f"‚ö†Ô∏è Gi√° gi·∫£m {consecutive_down} phi√™n li√™n ti·∫øp.\n"
        else:
            summary = "üîç Xu h∆∞·ªõng ch∆∞a r√µ.\n"
        summary += "\n".join(trend_summary)
        return summary

    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))
    async def analyze_with_openrouter(self, technical_data):
        """Ph√¢n t√≠ch m·∫´u h√¨nh k·ªπ thu·∫≠t b·∫±ng OpenRouter API (Claude)"""
        if not OPENROUTER_API_KEY:
            raise Exception("Ch∆∞a c√≥ OPENROUTER_API_KEY")

        prompt = (
            "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t ch·ª©ng kho√°n."
            " D·ª±a tr√™n d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y, h√£y nh·∫≠n di·ªán c√°c m·∫´u h√¨nh n·∫øn nh∆∞ Doji, Hammer, Shooting Star, Engulfing,"
            " s√≥ng Elliott, m√¥ h√¨nh Wyckoff, v√† c√°c v√πng h·ªó tr·ª£/kh√°ng c·ª±."
            "\n\nCh·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ ·ªü d·∫°ng JSON nh∆∞ sau, kh√¥ng th√™m gi·∫£i th√≠ch n√†o kh√°c:\n"
            "{\n"
            "  \"support_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"resistance_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"patterns\": [\n"
            "    {\"name\": \"t√™n m·∫´u h√¨nh\", \"description\": \"gi·∫£i th√≠ch ng·∫Øn\"},\n"
            "    ...\n"
            "  ]\n"
            "}\n\n"
            f"D·ªØ li·ªáu:\n{json.dumps(technical_data, ensure_ascii=False, indent=2)}"
        )

        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://yourapp.com",
            "X-Title": "Stock Analysis Bot"
        }
        payload = {
            "model": "anthropic/claude-3-haiku",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 1024,
            "temperature": 0.2
        }

        async with aiohttp.ClientSession() as session:
            async with session.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload) as resp:
                text = await resp.text()
                try:
                    result = json.loads(text)
                    content = result['choices'][0]['message']['content']
                    return json.loads(content)
                except json.JSONDecodeError:
                    logger.error(f"Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ OpenRouter: {text}")
                    return {}
                except KeyError:
                    logger.error(f"Ph·∫£n h·ªìi thi·∫øu tr∆∞·ªùng c·∫ßn thi·∫øt: {text}")
                    return {}

    async def generate_report(self, dfs: dict, symbol: str, fundamental_data: dict, outlier_reports: dict, pattern_analysis: dict = None) -> str:
        """T·∫°o b√°o c√°o ph√¢n t√≠ch ch·ª©ng kho√°n t·ª´ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω, h·ªó tr·ª£ k·∫øt qu·∫£ t·ª´ OpenRouter"""
        try:
            # Ki·ªÉm tra d·ªØ li·ªáu khung ng√†y
            if '1D' not in dfs:
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu khung ng√†y (1D)")
                
            tech_analyzer = TechnicalAnalyzer()
            
            # T√≠nh ch·ªâ b√°o k·ªπ thu·∫≠t ƒëa khung th·ªùi gian
            indicators = tech_analyzer.calculate_multi_timeframe_indicators(dfs)
            
            # L·∫•y tin t·ª©c
            news = await get_news(symbol=symbol)
            news_text = "\n".join([f"üì∞ **{n['title']}**\nüîó {n['link']}\nüìù {n['summary']}" for n in news])
            
            # Chu·∫©n b·ªã th√¥ng tin gi√°
            df_1d = dfs.get('1D')
            close_today = df_1d['close'].iloc[-1]
            close_yesterday = df_1d['close'].iloc[-2] if len(df_1d) >= 2 else close_today
            
            # Ph√¢n t√≠ch h√†nh ƒë·ªông gi√°
            price_action = self.analyze_price_action(df_1d)
            
            # L·∫•y l·ªãch s·ª≠ b√°o c√°o
            history = await self.load_report_history(symbol)
            past_report = ""
            if history:
                last = history[-1]
                past_result = "ƒë√∫ng" if (close_today > last["close_today"] and "mua" in last["report"].lower()) else "sai"
                past_report = f"üìú **B√°o c√°o tr∆∞·ªõc** ({last['date']}): {last['close_today']} ‚Üí {close_today} ({past_result})\n"
            
            # Ph√¢n t√≠ch c∆° b·∫£n
            fundamental_report = deep_fundamental_analysis(fundamental_data)

            # Ph√¢n t√≠ch m·∫´u h√¨nh t·ª´ OpenRouter ho·∫∑c ph√°t hi·ªán local
            support_levels = []
            resistance_levels = []
            patterns = []
            
            if pattern_analysis is not None:
                # S·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ OpenRouter n·∫øu c√≥
                support_levels = pattern_analysis.get('support_levels', [])
                resistance_levels = pattern_analysis.get('resistance_levels', [])
                patterns = pattern_analysis.get('patterns', [])
            else:
                # Ki·ªÉm tra c√°c m·∫´u h√¨nh trong d·ªØ li·ªáu
                if '1D' in dfs and ENABLE_LOCAL_PATTERN_DETECTION:
                    pattern_cols = ['doji', 'hammer', 'shooting_star', 'bullish_engulfing', 'bearish_engulfing']
                    df = dfs['1D']
                    for col in pattern_cols:
                        if col in df.columns and df[col].iloc[-5:].any():
                            # Th√™m m·∫´u h√¨nh cu·ªëi c√πng ƒë∆∞·ª£c ph√°t hi·ªán
                            last_idx = df[df[col]].index[-1]
                            patterns.append({
                                "name": col.replace('_', ' ').title(),
                                "description": f"Ph√°t hi·ªán ng√†y {last_idx.strftime('%Y-%m-%d')}"
                            })
            
            # D·ª± b√°o gi√° (n·∫øu ƒë∆∞·ª£c b·∫≠t)
            forecast_summary = ""
            if ENABLE_PRICE_PREDICTION and len(df_1d) >= 30:
                try:
                    forecast, prophet_model = forecast_with_prophet(df_1d, periods=7)
                    prophet_perf = evaluate_prophet_performance(df_1d, forecast)
                    future_forecast = forecast[forecast['ds'] > df_1d.index[-1].tz_localize(None)]
                    
                    if not future_forecast.empty:
                        next_day_pred = future_forecast.iloc[0]
                        day7_pred = future_forecast.iloc[6] if len(future_forecast) >= 7 else future_forecast.iloc[-1]
                    else:
                        next_day_pred = forecast.iloc[-1]
                        day7_pred = forecast.iloc[-1]

                    forecast_summary = f"**D·ª± b√°o gi√° (Prophet)** (Hi·ªáu su·∫•t: {prophet_perf:.2f}):\n"
                    forecast_summary += f"- Ng√†y ti·∫øp theo ({next_day_pred['ds'].strftime('%d/%m/%Y')}): {next_day_pred['yhat']:.2f}\n"
                    forecast_summary += f"- Sau 7 ng√†y ({day7_pred['ds'].strftime('%d/%m/%Y')}): {day7_pred['yhat']:.2f}\n"
                except Exception as e:
                    logger.error(f"L·ªói khi d·ª± b√°o gi√°: {str(e)}")
                    forecast_summary = "Kh√¥ng th·ªÉ d·ª± b√°o gi√° do l·ªói m√¥ h√¨nh.\n"
            
            # T√≠n hi·ªáu XGBoost (n·∫øu ƒë∆∞·ª£c b·∫≠t)
            xgb_summary = ""
            if ENABLE_PRICE_PREDICTION and len(df_1d) >= 50:
                try:
                    features = ['sma20', 'sma50', 'rsi', 'macd', 'signal', 'bb_high', 'bb_low']
                    if ENABLE_ADVANCED_INDICATORS:
                        features.extend(['ichimoku_a', 'ichimoku_b', 'vwap', 'mfi'])
                    
                    xgb_signal, xgb_perf = predict_xgboost_signal(df_1d.copy(), features)
                    
                    if isinstance(xgb_signal, int):
                        xgb_text = "TƒÉng" if xgb_signal == 1 else "Gi·∫£m"
                    else:
                        xgb_text = xgb_signal
                    
                    xgb_summary = f"**XGBoost d·ª± ƒëo√°n t√≠n hi·ªáu giao d·ªãch** (Hi·ªáu su·∫•t: {xgb_perf:.2f}): {xgb_text}\n"
                except Exception as e:
                    logger.error(f"L·ªói khi d·ª± ƒëo√°n t√≠n hi·ªáu XGBoost: {str(e)}")
                    xgb_summary = ""
            
            # B√°o c√°o ph√°t hi·ªán outlier
            outlier_text = "\n".join([f"**{tf}**: {report}" for tf, report in outlier_reports.items()])

            # T·∫°o prompt cho Gemini
            prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n, trader chuy√™n nghi·ªáp, chuy√™n gia b·∫Øt ƒë√°y 30 nƒÉm kinh nghi·ªám ·ªü ch·ª©ng kho√°n Vi·ªát Nam. H√£y vi·∫øt b√°o c√°o chi ti·∫øt cho {symbol}:

**Th√¥ng tin c∆° b·∫£n:**
- Ng√†y: {datetime.now().strftime('%d/%m/%Y')}
- Gi√° h√¥m qua: {close_yesterday:.2f}
- Gi√° h√¥m nay: {close_today:.2f}

**H√†nh ƒë·ªông gi√°:**
{price_action}

**L·ªãch s·ª≠ d·ª± ƒëo√°n:**
{past_report}

**Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu:**
{outlier_text}

**Ch·ªâ s·ªë k·ªπ thu·∫≠t:**
"""
            for tf, ind in indicators.items():
                prompt += f"\n--- {tf} ---\n"
                for key, value in ind.items():
                    if key in ['close', 'sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal', 'bb_high', 'bb_low']:
                        prompt += f"- {key}: {value:.2f}\n"
            
            # Th√™m th√¥ng tin v·ªÅ m·∫´u h√¨nh k·ªπ thu·∫≠t (t·ª´ OpenRouter ho·∫∑c local)
            if patterns or support_levels or resistance_levels:
                prompt += "\n**Ph√¢n t√≠ch m·∫´u h√¨nh:**\n"
                
                if support_levels:
                    prompt += f"- H·ªó tr·ª£: {', '.join(map(str, support_levels))}\n"
                
                if resistance_levels:
                    prompt += f"- Kh√°ng c·ª±: {', '.join(map(str, resistance_levels))}\n"
                
                if patterns:
                    prompt += "- M·∫´u h√¨nh ph√°t hi·ªán:\n"
                    for pattern in patterns:
                        if isinstance(pattern, dict) and 'name' in pattern:
                            desc = pattern.get('description', '')
                            prompt += f"  + {pattern['name']}: {desc}\n"
                        else:
                            prompt += f"  + {pattern}\n"
            
            # Th√™m th√¥ng tin c∆° b·∫£n
            prompt += f"\n**C∆° b·∫£n:**\n{fundamental_report}\n"
            
            # Th√™m th√¥ng tin d·ª± b√°o
            if forecast_summary:
                prompt += f"\n{forecast_summary}\n"
                
            if xgb_summary:
                prompt += f"\n{xgb_summary}\n"
            
            # Th√™m tin t·ª©c
            prompt += f"\n**Tin t·ª©c:**\n{news_text}\n"
            
            # H∆∞·ªõng d·∫´n t·∫°o b√°o c√°o
            prompt += """
D·ª±a tr√™n nh·ªØng th√¥ng tin tr√™n, h√£y vi·∫øt m·ªôt b√°o c√°o ph√¢n t√≠ch chi ti·∫øt. B√°o c√°o ph·∫£i bao g·ªìm:
1. T√≥m t·∫Øt t·ªïng quan t√¨nh h√¨nh
2. Ph√¢n t√≠ch k·ªπ thu·∫≠t chi ti·∫øt 
3. ƒê√°nh gi√° c√°c m·ª©c h·ªó tr·ª£ v√† kh√°ng c·ª± quan tr·ªçng
4. Xu h∆∞·ªõng ng·∫Øn h·∫°n (1-3 ng√†y), trung h·∫°n (1-3 tu·∫ßn) v√† d√†i h·∫°n (1-3 th√°ng)
5. Khuy·∫øn ngh·ªã giao d·ªãch v·ªõi m·ª©c gi√° c·ª• th·ªÉ

Vi·∫øt b√°o c√°o chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu, d·ª±a v√†o d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p. N·∫øu b·∫°n ph√°t hi·ªán d·∫•u hi·ªáu b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu, h√£y n√™u r√µ.
"""

            # T·∫°o b√°o c√°o
            response = await self.generate_content(prompt)
            report = response.text
            
            # Th√™m metadata v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu v√† mode x·ª≠ l√Ω
            report_metadata = f"---\nCh·∫ø ƒë·ªô x·ª≠ l√Ω: {PROCESSING_MODE}"
            if '1D' in outlier_reports:
                if "kh√¥ng ph√°t hi·ªán b·∫•t th∆∞·ªùng" in outlier_reports['1D'].lower():
                    report_metadata += " | D·ªØ li·ªáu: S·∫°ch"
                elif "outlier" in outlier_reports['1D'].lower():
                    report_metadata += " | D·ªØ li·ªáu: C√≥ ƒëi·ªÉm b·∫•t th∆∞·ªùng ƒë√£ x·ª≠ l√Ω"
            
            report = report.strip()
            
            return report
            
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o b√°o c√°o: {str(e)}")
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫°o b√°o c√°o: {str(e)}")

# ---------- TELEGRAM COMMANDS ----------
async def notify_admin_new_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.message.from_user
    user_id = user.id
    if not await is_user_approved(user_id):
        message = f"üîî Ng∆∞·ªùi d√πng m·ªõi:\nID: {user_id}\nUsername: {user.username}\nT√™n: {user.full_name}\nDuy·ªát: /approve {user_id}"
        await context.bot.send_message(chat_id=ADMIN_ID, text=message)
        await update.message.reply_text("‚è≥ Ch·ªù admin duy·ªát!")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    logger.info(f"Start called: user_id={user_id}, ADMIN_ID={ADMIN_ID}")

    if str(user_id) == ADMIN_ID and not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        logger.info(f"Admin {user_id} t·ª± ƒë·ªông duy·ªát.")

    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return

    await update.message.reply_text(
        "üöÄ **V18.8 - THUA GIA C√ÅT L∆Ø·ª¢NG M·ªñI C√ÅI QU·∫†T!**\n"
        "üìä **L·ªánh**:\n"
        "- /analyze [M√£] [S·ªë n·∫øn] - Ph√¢n t√≠ch ƒëa khung.\n"
        "- /getid - L·∫•y ID.\n"
        "- /approve [user_id] - Duy·ªát ng∆∞·ªùi d√πng (admin).\n"
        "üí° **B·∫Øt ƒë·∫ßu n√†o!**"
    )

async def analyze_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return
    try:
        args = context.args
        if not args:
            raise ValueError("Nh·∫≠p m√£ ch·ª©ng kho√°n (e.g., VNINDEX, SSI).")
        symbol = args[0].upper()
        num_candles = int(args[1]) if len(args) > 1 else DEFAULT_CANDLES
        if num_candles < 20:
            raise ValueError("S·ªë n·∫øn ph·∫£i l·ªõn h∆°n ho·∫∑c b·∫±ng 20 ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o!")
        if num_candles > 500:
            raise ValueError("T·ªëi ƒëa 500 n·∫øn!")
        loader = DataLoader()
        tech_analyzer = TechnicalAnalyzer()
        ai_analyzer = AIAnalyzer()
        timeframes = ['1D', '1W', '1M']
        dfs = {}
        outlier_reports = {}
        for tf in timeframes:
            df, outlier_report = await loader.load_data(symbol, tf, num_candles)
            dfs[tf] = tech_analyzer.calculate_indicators(df)
            outlier_reports[tf] = outlier_report
        fundamental_data = await loader.get_fundamental_data(symbol)
        report = await ai_analyzer.generate_report(dfs, symbol, fundamental_data, outlier_reports)
        await redis_manager.set(f"report_{symbol}_{num_candles}", report, expire=CACHE_EXPIRE_SHORT)

        formatted_report = f"<b>üìà B√°o c√°o ph√¢n t√≠ch cho {symbol}</b>\n\n"
        formatted_report += f"<pre>{html.escape(report)}</pre>"
        await update.message.reply_text(formatted_report, parse_mode='HTML')
    except ValueError as e:
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")
    except Exception as e:
        logger.error(f"L·ªói trong analyze_command: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")

async def get_id(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    await update.message.reply_text(f"ID c·ªßa b·∫°n: {user_id}")

async def approve_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if str(update.message.from_user.id) != ADMIN_ID:
        await update.message.reply_text("‚ùå Ch·ªâ admin d√πng ƒë∆∞·ª£c l·ªánh n√†y!")
        return
    if len(context.args) != 1:
        await update.message.reply_text("‚ùå Nh·∫≠p user_id: /approve 123456789")
        return
    user_id = context.args[0]
    if not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        await update.message.reply_text(f"‚úÖ ƒê√£ duy·ªát {user_id}")
    else:
        await update.message.reply_text(f"‚ÑπÔ∏è {user_id} ƒë√£ ƒë∆∞·ª£c duy·ªát")

# ---------- MAIN & DEPLOY ----------
async def main():
    await init_db()

    scheduler = AsyncIOScheduler()
    scheduler.add_job(auto_train_models, 'cron', hour=2, minute=0)
    scheduler.start()
    logger.info("Auto training scheduler ƒë√£ kh·ªüi ƒë·ªông.")

    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("analyze", analyze_command))
    app.add_handler(CommandHandler("getid", get_id))
    app.add_handler(CommandHandler("approve", approve_user))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, notify_admin_new_user))
    logger.info("ü§ñ Bot kh·ªüi ƒë·ªông!")

    BASE_URL = os.getenv("RENDER_EXTERNAL_URL", f"https://{os.getenv('RENDER_SERVICE_NAME')}.onrender.com")
    WEBHOOK_URL = f"{BASE_URL}/{TELEGRAM_TOKEN}"
    await app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        webhook_url=WEBHOOK_URL,
        url_path=TELEGRAM_TOKEN
    )

if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        import unittest

        class TestTechnicalAnalysis(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=50, freq='D')
                data = {
                    'open': np.random.random(50) * 100,
                    'high': np.random.random(50) * 100,
                    'low': np.random.random(50) * 100,
                    'close': np.linspace(100, 150, 50),
                    'volume': np.random.randint(1000, 5000, 50)
                }
                self.df = pd.DataFrame(data, index=dates)

            def test_calculate_indicators(self):
                analyzer = TechnicalAnalyzer()
                df_processed = analyzer.calculate_indicators(self.df)
                self.assertIn('sma20', df_processed.columns)
                self.assertIn('rsi', df_processed.columns)

        class TestForecastProphet(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=100, freq='D')
                self.df = pd.DataFrame({
                    'close': np.linspace(100, 200, 100),
                    'open': np.linspace(100, 200, 100),
                    'high': np.linspace(100, 200, 100),
                    'low': np.linspace(100, 200, 100),
                    'volume': np.random.randint(1000, 5000, 100)
                }, index=dates)

            def test_forecast_with_prophet(self):
                forecast, model = forecast_with_prophet(self.df, periods=7)
                self.assertFalse(forecast.empty)
                self.assertIn('yhat', forecast.columns)

        class TestOutlierDetection(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=7, freq='D')
                self.df = pd.DataFrame({
                    'close': [100, 101, 102, 103, 500, 104, 105]
                }, index=dates)

            def test_detect_outliers(self):
                loader = DataLoader()
                df_with_outliers, report = loader.detect_outliers(self.df)
                self.assertIn('is_outlier', df_with_outliers.columns)
                self.assertEqual(df_with_outliers['is_outlier'].sum(), 1)
                self.assertIn('500', report)

        unittest.main(argv=[sys.argv[0]])
    else:
        import nest_asyncio
        nest_asyncio.apply()
        asyncio.run(main())