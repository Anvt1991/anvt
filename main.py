#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Bot Ch·ª©ng Kho√°n To√†n Di·ªán Phi√™n B·∫£n V18.11 (N√¢ng c·∫•p):
- T√≠ch h·ª£p AI OpenRouter cho ph√¢n t√≠ch m·∫´u h√¨nh, s√≥ng, v√† n·∫øn nh·∫≠t.
- S·ª≠ d·ª•ng m√¥ h√¨nh deepseek/deepseek-chat-v3-0324:free.
- Chu·∫©n h√≥a d·ªØ li·ªáu v√† pipeline x·ª≠ l√Ω.
- ƒê·∫£m b·∫£o c√°c ch·ª©c nƒÉng v√† c√¥ng ngh·ªá hi·ªán c√≥ kh√¥ng b·ªã ·∫£nh h∆∞·ªüng.
"""

import os
import sys
import io
import logging
import pickle
from datetime import datetime, timedelta
import asyncio
from concurrent.futures import ThreadPoolExecutor
import gc
import time

import pandas as pd
import numpy as np
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

import yfinance as yf
from vnstock import Vnstock
import google.generativeai as genai

from ta import trend, momentum, volatility
from ta.volume import MFIIndicator
import feedparser

import redis.asyncio as redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import Column, Integer, String, Float, Text, DateTime, select, LargeBinary

import xgboost as xgb
from sklearn.metrics import accuracy_score
from prophet import Prophet

import matplotlib.pyplot as plt
import holidays
import html

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from tenacity import retry, stop_after_attempt, wait_exponential

import aiohttp
import json
from pydantic import BaseModel, validator
import cProfile
import pstats
from io import StringIO

# ---------- C·∫§U H√åNH & LOGGING ----------
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
ADMIN_ID = os.getenv("ADMIN_ID", "1225226589")
PORT = int(os.environ.get("PORT", 10000))
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
RENDER_SERVICE_NAME = os.getenv("RENDER_SERVICE_NAME", "")

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

CACHE_EXPIRE_SHORT = 1800
CACHE_EXPIRE_MEDIUM = 3600
CACHE_EXPIRE_LONG = 86400
NEWS_CACHE_EXPIRE = 900
DEFAULT_CANDLES = 100
DEFAULT_TIMEFRAME = '1D'

executor = ThreadPoolExecutor(max_workers=5)

async def run_in_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, lambda: func(*args, **kwargs))

# ---------- CHU·∫®N H√ìA D·ªÆ LI·ªÜU ----------
class DataNormalizer:
    """
    L·ªõp chu·∫©n h√≥a v√† x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi c√°c ph∆∞∆°ng ph√°p n√¢ng cao
    ƒë·ªÉ x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá v√† b·∫•t th∆∞·ªùng.
    """
    
    @staticmethod
    def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """
        Chu·∫©n h√≥a dataframe theo nhi·ªÅu ph∆∞∆°ng ph√°p kh√°c nhau
        """
        if df.empty:
            return df
            
        # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
        normalized_df = df.copy()
        
        # ƒê·∫£m b·∫£o c√°c c·ªôt s·ªë ƒë√∫ng ƒë·ªãnh d·∫°ng
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in normalized_df.columns:
                # Chuy·ªÉn ƒë·ªïi sang ki·ªÉu s·ªë
                normalized_df[col] = pd.to_numeric(normalized_df[col], errors='coerce')
        
        # X·ª≠ l√Ω c√°c c·ªôt ng√†y th√°ng
        date_columns = ['date', 'time', 'datetime']
        for col in date_columns:
            if col in normalized_df.columns:
                try:
                    normalized_df[col] = pd.to_datetime(normalized_df[col], errors='coerce')
                except:
                    pass
                    
        # X√≥a c√°c h√†ng tr√πng l·∫∑p
        normalized_df = normalized_df.drop_duplicates()
        
        # S·∫Øp x·∫øp theo ng√†y n·∫øu c√≥ c·ªôt date
        if 'date' in normalized_df.columns:
            normalized_df = normalized_df.sort_values('date')
            
        return normalized_df
    
    @staticmethod
    def validate_data(df: pd.DataFrame) -> (bool, str):
        """
        Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu
        """
        if df.empty:
            return False, "DataFrame r·ªóng"
            
        required_columns = ['open', 'high', 'low', 'close']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_columns)}"
            
        # Ki·ªÉm tra d·ªØ li·ªáu h·ª£p l·ªá
        if df['high'].min() < df['low'].min():
            return False, "C√≥ gi√° tr·ªã high nh·ªè h∆°n gi√° tr·ªã low"
            
        # Ki·ªÉm tra gi√° √¢m
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns and (df[col] < 0).any():
                return False, f"C√≥ gi√° tr·ªã √¢m trong c·ªôt {col}"
                
        # Ki·ªÉm tra gi√° ƒë√≥ng c·ª≠a n·∫±m ngo√†i ph·∫°m vi high-low
        invalid_close = ((df['close'] > df['high']) | (df['close'] < df['low'])).sum()
        if invalid_close > 0:
            return False, f"C√≥ {invalid_close} gi√° ƒë√≥ng c·ª≠a n·∫±m ngo√†i ph·∫°m vi high-low"
            
        return True, "D·ªØ li·ªáu h·ª£p l·ªá"
    
    @staticmethod
    def detect_outliers(df: pd.DataFrame, columns=['open', 'high', 'low', 'close'], 
                         method='zscore', threshold=3) -> (pd.DataFrame, str):
        """
        Ph√°t hi·ªán gi√° tr·ªã ngo·∫°i lai trong d·ªØ li·ªáu s·ª≠ d·ª•ng nhi·ªÅu ph∆∞∆°ng ph√°p
        """
        if df.empty:
            return df, "DataFrame r·ªóng"
            
        outlier_report = {}
        outlier_indices = set()
        
        # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
        df_copy = df.copy()
        
        for col in columns:
            if col not in df_copy.columns:
                continue
                
            # Ph√°t hi·ªán gi√° tr·ªã ngo·∫°i lai b·∫±ng ph∆∞∆°ng ph√°p Z-Score
            if method == 'zscore':
                z_scores = np.abs((df_copy[col] - df_copy[col].mean()) / df_copy[col].std())
                outliers = z_scores > threshold
                col_outliers = df_copy.index[outliers].tolist()
                
                if len(col_outliers) > 0:
                    outlier_report[col] = {
                        'count': len(col_outliers),
                        'indices': col_outliers[:10],  # Ch·ªâ l·∫•y 10 v·ªã tr√≠ ƒë·∫ßu ti√™n ƒë·ªÉ tr√°nh qu√° d√†i
                        'method': 'Z-Score'
                    }
                    outlier_indices.update(col_outliers)
            
            # Ph√°t hi·ªán gi√° tr·ªã ngo·∫°i lai b·∫±ng IQR (Interquartile Range)
            elif method == 'iqr':
                Q1 = df_copy[col].quantile(0.25)
                Q3 = df_copy[col].quantile(0.75)
                IQR = Q3 - Q1
                
                outliers = (df_copy[col] < (Q1 - 1.5 * IQR)) | (df_copy[col] > (Q3 + 1.5 * IQR))
                col_outliers = df_copy.index[outliers].tolist()
                
                if len(col_outliers) > 0:
                    outlier_report[col] = {
                        'count': len(col_outliers),
                        'indices': col_outliers[:10],
                        'method': 'IQR'
                    }
                    outlier_indices.update(col_outliers)
            
            # Ph∆∞∆°ng ph√°p Modified Z-Score (Robust Z-Score)
            elif method == 'modified_zscore':
                median = df_copy[col].median()
                # S·ª≠ d·ª•ng MAD (Median Absolute Deviation) thay v√¨ ƒë·ªô l·ªách chu·∫©n
                mad = np.median(np.abs(df_copy[col] - median))
                
                if mad == 0:  # Tr√°nh chia cho 0
                    continue
                    
                modified_z_scores = 0.6745 * np.abs(df_copy[col] - median) / mad
                outliers = modified_z_scores > threshold
                col_outliers = df_copy.index[outliers].tolist()
                
                if len(col_outliers) > 0:
                    outlier_report[col] = {
                        'count': len(col_outliers),
                        'indices': col_outliers[:10],
                        'method': 'Modified Z-Score'
                    }
                    outlier_indices.update(col_outliers)
        
        # T·∫°o b√°o c√°o t√≥m t·∫Øt
        report_text = ""
        total_outliers = len(outlier_indices)
        
        if total_outliers > 0:
            report_text = f"Ph√°t hi·ªán {total_outliers} gi√° tr·ªã ngo·∫°i lai trong {len(columns)} c·ªôt.\n"
            for col, details in outlier_report.items():
                report_text += f"- C·ªôt {col}: {details['count']} gi√° tr·ªã ngo·∫°i lai (ph∆∞∆°ng ph√°p {details['method']})\n"
        else:
            report_text = "Kh√¥ng ph√°t hi·ªán gi√° tr·ªã ngo·∫°i lai."
            
        return df_copy, report_text
    
    @staticmethod
    def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:
        """
        ƒêi·ªÅn c√°c gi√° tr·ªã b·ªã thi·∫øu b·∫±ng nhi·ªÅu ph∆∞∆°ng ph√°p n√¢ng cao
        """
        if df.empty:
            return df
            
        # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
        df_filled = df.copy()
        
        # Danh s√°ch c√°c c·ªôt s·ªë
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col not in df_filled.columns:
                continue
                
            # Ki·ªÉm tra gi√° tr·ªã NaN
            if df_filled[col].isna().sum() > 0:
                # N·∫øu √≠t h∆°n 10% gi√° tr·ªã b·ªã thi·∫øu, s·ª≠ d·ª•ng n·ªôi suy tuy·∫øn t√≠nh
                if df_filled[col].isna().mean() < 0.1:
                    df_filled[col] = df_filled[col].interpolate(method='linear')
                # N·∫øu l·ªõn h∆°n 10% nh∆∞ng nh·ªè h∆°n 30%, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p n·ªôi suy spline
                elif df_filled[col].isna().mean() < 0.3:
                    df_filled[col] = df_filled[col].interpolate(method='spline', order=3)
                # N·∫øu qu√° nhi·ªÅu gi√° tr·ªã b·ªã thi·∫øu, s·ª≠ d·ª•ng gi√° tr·ªã trung b√¨nh
                else:
                    df_filled[col] = df_filled[col].fillna(df_filled[col].mean())
        
        # ƒê·ªëi v·ªõi c√°c c·ªôt phi s·ªë
        for col in df_filled.columns:
            if col not in numeric_columns and df_filled[col].isna().any():
                # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ffill (forward fill) cho d·ªØ li·ªáu th·ªùi gian
                if col in ['date', 'time', 'datetime']:
                    df_filled[col] = df_filled[col].fillna(method='ffill')
                # ƒê·ªëi v·ªõi c√°c c·ªôt kh√°c, s·ª≠ d·ª•ng ch·∫ø ƒë·ªô (gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t)
                else:
                    df_filled[col] = df_filled[col].fillna(df_filled[col].mode()[0])
        
        return df_filled
    
    @staticmethod
    def standardize_for_db(data: dict) -> dict:
        """
        Chu·∫©n h√≥a d·ªØ li·ªáu cho c∆° s·ªü d·ªØ li·ªáu
        """
        if not data:
            return {}
            
        # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
        standardized = data.copy()
        
        # X·ª≠ l√Ω c√°c ki·ªÉu d·ªØ li·ªáu ph·ª©c t·∫°p
        for key, value in standardized.items():
            # Chuy·ªÉn ƒë·ªïi datetime th√†nh chu·ªói ISO format
            if isinstance(value, datetime):
                standardized[key] = value.isoformat()
            # Chuy·ªÉn ƒë·ªïi pandas Timestamp th√†nh chu·ªói ISO format
            elif hasattr(value, 'timestamp') and callable(getattr(value, 'timestamp')):
                standardized[key] = value.isoformat()
            # Chuy·ªÉn ƒë·ªïi numpy int/float th√†nh Python int/float
            elif hasattr(value, 'item') and callable(getattr(value, 'item')):
                standardized[key] = value.item()
            # Chuy·ªÉn ƒë·ªïi NaN th√†nh None
            elif pd.isna(value):
                standardized[key] = None
                
        return standardized

# ---------- K·∫æT N·ªêI REDIS (Async) ----------
class RedisManager:
    def __init__(self):
        try:
            self.redis_client = redis.from_url(REDIS_URL)
            logger.info("K·∫øt n·ªëi Redis th√†nh c√¥ng.")
        except Exception as e:
            logger.error(f"L·ªói k·∫øt n·ªëi Redis: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def set(self, key, value, expire):
        try:
            serialized_value = pickle.dumps(value)
            await self.redis_client.set(key, serialized_value, ex=expire)
            return True
        except Exception as e:
            logger.error(f"L·ªói Redis set: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def get(self, key):
        try:
            data = await self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            logger.error(f"L·ªói Redis get: {str(e)}")
            return None
            
    async def optimize_cache(self):
        """T·ªëi ∆∞u b·ªô nh·ªõ Redis b·∫±ng c√°ch x√≥a cache c≈© v√† kh√¥ng s·ª≠ d·ª•ng"""
        try:
            # L·∫•y t·∫•t c·∫£ c√°c key t·ª´ Redis
            all_keys = await self.redis_client.keys("*")
            current_time = datetime.now()
            deleted_count = 0
            
            # ∆Øu ti√™n x√≥a c√°c lo·∫°i cache kh√°c nhau
            for key in all_keys:
                try:
                    key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                    
                    # Kh√¥ng x√≥a d·ªØ li·ªáu ng∆∞·ªùi d√πng v√† b√°o c√°o
                    if key_str.startswith(("user_", "report_history_")):
                        continue
                        
                    # X√≥a cache d·ªØ li·ªáu c≈© (>1 ng√†y) v√† tin t·ª©c
                    if (key_str.startswith("data_") and "1D" not in key_str) or key_str.startswith("news_"):
                        await self.redis_client.delete(key)
                        deleted_count += 1
                    
                    # Ch·ªâ gi·ªØ l·∫°i cache cho c√°c c·ªï phi·∫øu VN30 v√† c√°c ch·ªâ s·ªë
                    elif key_str.startswith("data_") and not any(index in key_str for index in ["VNINDEX", "VN30", "HNX30"]):
                        # Ki·ªÉm tra TTL, n·∫øu c√≤n tr√™n 1 gi·ªù th√¨ gi·ªØ l·∫°i
                        ttl = await self.redis_client.ttl(key)
                        if ttl < 3600:  # D∆∞·ªõi 1 gi·ªù
                            await self.redis_client.delete(key)
                            deleted_count += 1
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω key {key}: {str(e)}")
                    continue
                    
            # Ch·∫°y garbage collector sau khi d·ªçn d·∫πp
            gc.collect()
            
            logger.info(f"ƒê√£ t·ªëi ∆∞u Redis cache: x√≥a {deleted_count}/{len(all_keys)} key")
            return deleted_count
        except Exception as e:
            logger.error(f"L·ªói t·ªëi ∆∞u Redis cache: {str(e)}")
            return 0

redis_manager = RedisManager()

# ---------- K·∫æT N·ªêI POSTGRESQL (Async) ----------
Base = declarative_base()

class ApprovedUser(Base):
    __tablename__ = 'approved_users'
    id = Column(Integer, primary_key=True)
    user_id = Column(String, unique=True, nullable=False)
    approved_at = Column(DateTime, default=datetime.now)

class ReportHistory(Base):
    __tablename__ = 'report_history'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    date = Column(String, nullable=False)
    report = Column(Text, nullable=False)
    close_today = Column(Float, nullable=False)
    close_yesterday = Column(Float, nullable=False)
    timestamp = Column(DateTime, default=datetime.now)

class TrainedModel(Base):
    __tablename__ = 'trained_models'
    id = Column(Integer, primary_key=True)
    symbol = Column(String, nullable=False)
    model_type = Column(String, nullable=False)
    model_blob = Column(LargeBinary, nullable=False)
    created_at = Column(DateTime, default=datetime.now)
    performance = Column(Float, nullable=True)

engine = create_async_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine, class_=AsyncSession, expire_on_commit=False)

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

class DBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def is_user_approved(self, user_id) -> bool:
        try:
            async with self.Session() as session:
                result = await session.execute(select(ApprovedUser).filter_by(user_id=str(user_id)))
                return result.scalar_one_or_none() is not None or str(user_id) == ADMIN_ID
        except Exception as e:
            logger.error(f"L·ªói ki·ªÉm tra ng∆∞·ªùi d√πng: {str(e)}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def add_approved_user(self, user_id, approved_at=None) -> None:
        try:
            async with self.Session() as session:
                if not await self.is_user_approved(user_id) and str(user_id) != ADMIN_ID:
                    new_user = ApprovedUser(user_id=str(user_id), approved_at=approved_at or datetime.now())
                    session.add(new_user)
                    await session.commit()
                    logger.info(f"Th√™m ng∆∞·ªùi d√πng ƒë∆∞·ª£c ph√™ duy·ªát: {user_id}")
        except Exception as e:
            logger.error(f"L·ªói th√™m ng∆∞·ªùi d√πng: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_report_history(self, symbol: str) -> list:
        try:
            async with self.Session() as session:
                reports = await session.execute(select(ReportHistory).filter_by(symbol=symbol).order_by(ReportHistory.id.asc()))
                reports = reports.scalars().all()
                return [
                    {
                        "id": report.id,
                        "symbol": report.symbol,
                        "date": report.date,
                        "report": report.report,
                        "close_today": report.close_today,
                        "close_yesterday": report.close_yesterday,
                        "timestamp": report.timestamp.isoformat()
                    }
                    for report in reports
                ]
        except Exception as e:
            logger.error(f"L·ªói t·∫£i l·ªãch s·ª≠ b√°o c√°o: {str(e)}")
            return []

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def save_report_history(self, symbol: str, report: str, close_today: float, close_yesterday: float) -> None:
        try:
            async with self.Session() as session:
                date_str = datetime.now().strftime('%Y-%m-%d')
                standardized_data = DataNormalizer.standardize_for_db({
                    'symbol': symbol,
                    'date': date_str,
                    'report': report,
                    'close_today': close_today,
                    'close_yesterday': close_yesterday
                })
                new_report = ReportHistory(**standardized_data)
                session.add(new_report)
                await session.commit()
                logger.info(f"L∆∞u b√°o c√°o m·ªõi cho {symbol}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u b√°o c√°o: {str(e)}")
            raise

db = DBManager()

# ---------- QU·∫¢N L√ù M√î H√åNH (Prophet & XGBoost) ----------
class ModelDBManager:
    def __init__(self):
        self.Session = SessionLocal

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def store_trained_model(self, symbol: str, model_type: str, model, performance: float = None):
        try:
            model_blob = pickle.dumps(model)
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                existing = result.scalar_one_or_none()
                if existing:
                    existing.model_blob = model_blob
                    existing.created_at = datetime.now()
                    existing.performance = performance
                else:
                    new_model = TrainedModel(symbol=symbol, model_type=model_type, model_blob=model_blob, performance=performance)
                    session.add(new_model)
                await session.commit()
            logger.info(f"L∆∞u m√¥ h√¨nh {model_type} cho {symbol} th√†nh c√¥ng v·ªõi hi·ªáu su·∫•t: {performance}")
        except Exception as e:
            logger.error(f"L·ªói l∆∞u m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def load_trained_model(self, symbol: str, model_type: str):
        try:
            async with self.Session() as session:
                result = await session.execute(select(TrainedModel).filter_by(symbol=symbol, model_type=model_type))
                model_record = result.scalar_one_or_none()
                if model_record:
                    model = pickle.loads(model_record.model_blob)
                    return model, model_record.performance
            return None, None
        except Exception as e:
            logger.error(f"L·ªói t·∫£i m√¥ h√¨nh {model_type} cho {symbol}: {str(e)}")
            return None, None

model_db_manager = ModelDBManager()

# ---------- H√ÄM H·ªñ TR·ª¢ ----------
def is_index(symbol: str) -> bool:
    indices = ['VNINDEX', 'VN30', 'HNX30', 'HNXINDEX', 'UPCOM']
    return symbol.upper() in indices

async def is_user_approved(user_id) -> bool:
    return await db.is_user_approved(user_id)

def standardize_data_for_db(data: dict) -> dict:
    return DataNormalizer.standardize_for_db(data)

# ---------- H√ÄM H·ªñ TR·ª¢: L·ªåC NG√ÄY GIAO D·ªäCH -----------
def filter_trading_days(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    df = df[df.index.weekday < 5]
    years = df.index.year.unique()
    vn_holidays = holidays.Vietnam(years=years)
    holiday_dates = set(vn_holidays.keys())
    df = df[~pd.to_datetime(df.index.date).isin(holiday_dates)]
    return df

# ---------- T·∫¢I D·ªÆ LI·ªÜU (N√ÇNG C·∫§P) ----------
class DataLoader:
    def __init__(self, source: str = 'vnstock'):
        self.source = source

    async def load_data(self, symbol: str, timeframe: str, num_candles: int) -> (pd.DataFrame, str):
        timeframe_map = {'1d': '1D', '1w': '1W', '1mo': '1M'}
        timeframe = timeframe_map.get(timeframe.lower(), timeframe).upper()
        
        # T·ªëi ∆∞u cache theo lo·∫°i d·ªØ li·ªáu
        is_popular = symbol.upper() in ['VNINDEX', 'VN30', 'HNX30', 'HNXINDEX', 'UPCOM']
        is_intraday = timeframe not in ['1D', '1W', '1M']
        
        if is_popular:
            expire = CACHE_EXPIRE_SHORT if is_intraday else CACHE_EXPIRE_MEDIUM
        else:
            expire = CACHE_EXPIRE_SHORT // 2 if is_intraday else CACHE_EXPIRE_SHORT
        
        # Gi·ªõi h·∫°n s·ªë n·∫øn ƒë·ªÉ tr√°nh qu√° t·∫£i
        effective_num_candles = min(num_candles, 300)  # Gi·ªõi h·∫°n t·ªëi ƒëa 300 n·∫øn
        
        cache_key = f"data_{self.source}_{symbol}_{timeframe}_{effective_num_candles}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data, "D·ªØ li·ªáu t·ª´ cache, kh√¥ng ki·ªÉm tra outlier"

        try:
            if self.source == 'vnstock':
                @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
                def fetch_vnstock():
                    stock = Vnstock().stock(symbol=symbol, source='TCBS')
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=(num_candles + 1) * 3)).strftime('%Y-%m-%d')
                    df = stock.quote.history(start=start_date, end=end_date, interval=timeframe)
                    if df is None or df.empty or len(df) < 20:
                        raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {'ch·ªâ s·ªë' if is_index(symbol) else 'm√£'} {symbol}")
                    
                    # Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi DataNormalizer
                    df = df.rename(columns={'time': 'date', 'open': 'open', 'high': 'high',
                                            'low': 'low', 'close': 'close', 'volume': 'volume'})
                    df['date'] = pd.to_datetime(df['date'])
                    df = df.set_index('date')
                    df = DataNormalizer.normalize_dataframe(df)
                    df.index = df.index.tz_localize('Asia/Bangkok')
                    
                    # X√°c th·ª±c d·ªØ li·ªáu
                    is_valid, error_msg = DataNormalizer.validate_data(df)
                    if not is_valid:
                        logger.warning(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho {symbol}: {error_msg}")
                    
                    # ƒêi·ªÅn gi√° tr·ªã thi·∫øu
                    df = DataNormalizer.fill_missing_values(df)
                    
                    if len(df) < 200:
                        logger.warning(f"D·ªØ li·ªáu cho {symbol} d∆∞·ªõi 200 n·∫øn, SMA200 c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")
                    
                    return df.tail(num_candles + 1)
                df = await run_in_thread(fetch_vnstock)
            elif self.source == 'yahoo':
                period_map = {'1D': 'd', '1W': 'wk', '1M': 'mo'}
                df = await self._download_yahoo_data(symbol, num_candles + 1, period_map.get(timeframe, 'd'))
                if df is None or df.empty or len(df) < 20:
                    raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {symbol} t·ª´ Yahoo Finance")
                
                # Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi DataNormalizer
                df = DataNormalizer.normalize_dataframe(df)
                df = DataNormalizer.fill_missing_values(df)
                df.index = df.index.tz_localize('Asia/Bangkok')
                
                # X√°c th·ª±c d·ªØ li·ªáu
                is_valid, error_msg = DataNormalizer.validate_data(df)
                if not is_valid:
                    logger.warning(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho {symbol}: {error_msg}")
                
                if len(df) < 200:
                    logger.warning(f"D·ªØ li·ªáu cho {symbol} d∆∞·ªõi 200 n·∫øn, SMA200 c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")
            else:
                raise ValueError("Ngu·ªìn d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá")

            trading_df = filter_trading_days(df)
            trading_df, outlier_report = DataNormalizer.detect_outliers(trading_df)
            await redis_manager.set(cache_key, trading_df, expire=expire)
            return trading_df, outlier_report
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu cho {symbol}: {str(e)}")
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8), reraise=True)
    async def _download_yahoo_data(self, symbol: str, num_candles: int, period: str) -> pd.DataFrame:
        import aiohttp
        try:
            async with aiohttp.ClientSession() as session:
                start_ts = int((datetime.now() - timedelta(days=num_candles * 3)).timestamp())
                end_ts = int(datetime.now().timestamp())
                url = (f"https://query1.finance.yahoo.com/v7/finance/download/{symbol}"
                       f"?period1={start_ts}&period2={end_ts}&interval=1{period}&events=history")
                async with asyncio.wait_for(session.get(url), timeout=15) as response:
                    if response.status != 200:
                        raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ Yahoo, HTTP {response.status}")
                    text = await response.text()
                    df = pd.read_csv(io.StringIO(text))
                    if df.empty:
                        raise ValueError("D·ªØ li·ªáu Yahoo r·ªóng")
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.set_index('Date')
                    return df.tail(num_candles)
        except asyncio.TimeoutError:
            logger.error("Timeout khi t·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance.")
            raise
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu Yahoo: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def fetch_fundamental_data_vnstock(self, symbol: str) -> dict:
        cache_key = f"fundamental_vnstock_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = Vnstock().stock(symbol=symbol, source='TCBS')
            fundamental_data = {}
            ratios = stock.finance.ratio()
            if ratios is not None and not ratios.empty:
                fundamental_data.update(ratios.iloc[-1].to_dict())
            if hasattr(stock.finance, 'valuation'):
                valuation = stock.finance.valuation()
                if valuation is not None and not valuation.empty:
                    fundamental_data.update(valuation.iloc[-1].to_dict())
            if not fundamental_data:
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock")
            return DataNormalizer.standardize_for_db(fundamental_data)

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ VNStock: {str(e)}")
            return {}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    async def fetch_fundamental_data_yahoo(self, symbol: str) -> dict:
        cache_key = f"fundamental_yahoo_{symbol}"
        cached_data = await redis_manager.get(cache_key)
        if cached_data is not None:
            return cached_data

        def fetch():
            stock = yf.Ticker(f"{symbol}.VN")
            info = stock.info
            fundamental_data = {
                'EPS': info.get('trailingEps', None),
                'P/E': info.get('trailingPE', None),
                'P/B': info.get('priceToBook', None),
                'ROE': info.get('returnOnEquity', None),
                'Dividend Yield': info.get('dividendYield', None),
                'Market Cap': info.get('marketCap', None),
                'BVPS': info.get('bookValue', None)
            }
            if all(v is None for v in fundamental_data.values()):
                raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo Finance")
            return DataNormalizer.standardize_for_db(fundamental_data)

        try:
            fundamental_data = await run_in_thread(fetch)
            await redis_manager.set(cache_key, fundamental_data, expire=86400)
            return fundamental_data
        except Exception as e:
            logger.error(f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n t·ª´ Yahoo: {str(e)}")
            return {}

    async def get_fundamental_data(self, symbol: str) -> dict:
        if is_index(symbol):
            return {"error": f"{symbol} l√† ch·ªâ s·ªë, kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n"}
        fundamental_data = await self.fetch_fundamental_data_vnstock(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        fundamental_data = await self.fetch_fundamental_data_yahoo(symbol)
        if fundamental_data and any(v is not None for v in fundamental_data.values()):
            return fundamental_data
        return {"error": f"Kh√¥ng c√≥ d·ªØ li·ªáu c∆° b·∫£n cho {symbol}"}

# ---------- PH√ÇN T√çCH K·ª∏ THU·∫¨T ----------
class TechnicalAnalyzer:
    """
    L·ªõp ph√¢n t√≠ch k·ªπ thu·∫≠t v·ªõi c√°c ph∆∞∆°ng ph√°p t√≠nh to√°n ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
    v√† √°p d·ª•ng l∆∞u tr·ªØ ƒë·ªám (caching)
    """
    def __init__(self):
        # Cache ƒë·ªÉ l∆∞u k·∫øt qu·∫£ t√≠nh to√°n
        self._cache = {}
    
    @staticmethod
    def _calculate_common_indicators(df: pd.DataFrame) -> pd.DataFrame:
        """T√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t ph·ªï bi·∫øn"""
        if df.empty:
            return df
            
        result_df = df.copy()
        
        # Chuy·ªÉn ƒë·ªïi sang dataframe TA-Lib
        try:
            # RSI (Relative Strength Index)
            result_df['rsi_14'] = ta.momentum.RSIIndicator(
                close=result_df['close'], window=14
            ).rsi()
            
            # MACD (Moving Average Convergence Divergence)
            macd = ta.trend.MACD(
                close=result_df['close'], window_slow=26, window_fast=12, window_sign=9
            )
            result_df['macd'] = macd.macd()
            result_df['macd_signal'] = macd.macd_signal()
            result_df['macd_diff'] = macd.macd_diff()
            
            # Bollinger Bands
            bollinger = ta.volatility.BollingerBands(
                close=result_df['close'], window=20, window_dev=2
            )
            result_df['bb_upper'] = bollinger.bollinger_hband()
            result_df['bb_lower'] = bollinger.bollinger_lband()
            result_df['bb_mavg'] = bollinger.bollinger_mavg()
            
            # Stochastic Oscillator
            stoch = ta.momentum.StochasticOscillator(
                high=result_df['high'], low=result_df['low'], close=result_df['close'], window=14, smooth_window=3
            )
            result_df['stoch_k'] = stoch.stoch()
            result_df['stoch_d'] = stoch.stoch_signal()
            
            # Moving Averages
            result_df['sma_20'] = ta.trend.SMAIndicator(close=result_df['close'], window=20).sma_indicator()
            result_df['sma_50'] = ta.trend.SMAIndicator(close=result_df['close'], window=50).sma_indicator()
            result_df['sma_200'] = ta.trend.SMAIndicator(close=result_df['close'], window=200).sma_indicator()
            result_df['ema_9'] = ta.trend.EMAIndicator(close=result_df['close'], window=9).ema_indicator()
            
            # ATR (Average True Range)
            result_df['atr_14'] = ta.volatility.AverageTrueRange(
                high=result_df['high'], low=result_df['low'], close=result_df['close'], window=14
            ).average_true_range()
            
            # ADX (Average Directional Index)
            adx = ta.trend.ADXIndicator(
                high=result_df['high'], low=result_df['low'], close=result_df['close'], window=14
            )
            result_df['adx_14'] = adx.adx()
            result_df['adx_pos'] = adx.adx_pos()
            result_df['adx_neg'] = adx.adx_neg()
            
        except Exception as e:
            logger.error(f"L·ªói khi t√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t: {str(e)}")
        
        return result_df
    
    @lru_cache(maxsize=32)
    def _cached_calculate_indicators(self, df_key, columns_hash):
        """
        Phi√™n b·∫£n cached c·ªßa calculate_indicators
        df_key: kh√≥a ƒë·∫°i di·ªán cho dataframe
        columns_hash: hash c·ªßa c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ tr√°nh cache sai
        """
        # Kh√¥i ph·ª•c dataframe t·ª´ cache, ho·∫∑c tr·∫£ v·ªÅ None n·∫øu kh√¥ng t√¨m th·∫•y
        if df_key in self._cache:
            return self._cache[df_key]['indicators']
        return None
    
    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        T√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t v·ªõi cache ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
        """
        if df is None or df.empty:
            return df
        
        # T·∫°o key cho dataframe d·ª±a tr√™n n·ªôi dung
        required_cols = ['open', 'high', 'low', 'close', 'volume'] if 'volume' in df.columns else ['open', 'high', 'low', 'close']
        cols_available = all(col in df.columns for col in required_cols)
        
        if not cols_available:
            return df
        
        # T·∫°o hash cho dataframe ƒë·ªÉ d√πng l√†m key
        df_hash = hash(tuple(map(tuple, df[required_cols].tail(5).values.tolist())))
        columns_hash = hash(tuple(required_cols))
        
        # Ki·ªÉm tra cache
        cached_result = self._cached_calculate_indicators(df_hash, columns_hash)
        if cached_result is not None:
            return cached_result
        
        # N·∫øu kh√¥ng c√≥ trong cache, t√≠nh to√°n v√† l∆∞u v√†o cache
        result = self._calculate_common_indicators(df)
        self._cache[df_hash] = {
            'indicators': result,
            'timestamp': datetime.now()
        }
        
        # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc cache
        if len(self._cache) > 50:  # Gi·ªØ t·ªëi ƒëa 50 k·∫øt qu·∫£
            # X√≥a c√°c m·ª•c c≈© nh·∫•t
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k]['timestamp'])
            del self._cache[oldest_key]
        
        return result
    
    def calculate_multi_timeframe_indicators(self, dfs: dict) -> dict:
        """
        T√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t cho nhi·ªÅu khung th·ªùi gian
        dfs: dict v·ªõi key l√† timeframe, value l√† dataframe
        """
        result = {}
        for timeframe, df in dfs.items():
            if df is not None and not df.empty:
                result[timeframe] = self.calculate_indicators(df)
            else:
                result[timeframe] = df
        return result

# ---------- CHU·∫®N H√ìA PIPELINE D·ªÆ LI·ªÜU ----------
class DataPipeline:
    """
    L·ªõp chu·∫©n h√≥a pipeline x·ª≠ l√Ω d·ªØ li·ªáu:
    - T·∫£i d·ªØ li·ªáu
    - Chu·∫©n h√≥a v√† ki·ªÉm tra ch·∫•t l∆∞·ª£ng
    - T√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t
    - Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
    """
    def __init__(self):
        self.data_loader = DataLoader()
        self.tech_analyzer = TechnicalAnalyzer()
    
    async def prepare_symbol_data(self, symbol: str, timeframes: list = None, num_candles: int = DEFAULT_CANDLES) -> dict:
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß cho m·ªôt m√£ ch·ª©ng kho√°n bao g·ªìm:
        - D·ªØ li·ªáu ƒëa khung th·ªùi gian 
        - Ch·ªâ b√°o k·ªπ thu·∫≠t
        - Ph√°t hi·ªán outlier
        - D·ªØ li·ªáu c∆° b·∫£n (n·∫øu c√≥)
        """
        if timeframes is None:
            timeframes = ['1D', '1W', '1M']
        
        result = {
            'symbol': symbol,
            'dataframes': {},
            'indicators': {},
            'outlier_reports': {},
            'fundamental_data': {},
            'errors': []
        }
        
        # T·∫£i d·ªØ li·ªáu ƒëa khung th·ªùi gian
        for tf in timeframes:
            try:
                df, outlier_report = await self.data_loader.load_data(symbol, tf, num_candles)
                result['dataframes'][tf] = df
                result['outlier_reports'][tf] = outlier_report
            except Exception as e:
                error_msg = f"L·ªói t·∫£i d·ªØ li·ªáu {tf} cho {symbol}: {str(e)}"
                logger.error(error_msg)
                result['errors'].append(error_msg)
        
        # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
        if not result['dataframes']:
            return result
        
        # T√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t
        try:
            result['indicators'] = self.tech_analyzer.calculate_multi_timeframe_indicators(result['dataframes'])
        except Exception as e:
            error_msg = f"L·ªói t√≠nh to√°n ch·ªâ b√°o k·ªπ thu·∫≠t cho {symbol}: {str(e)}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
        
        # L·∫•y d·ªØ li·ªáu c∆° b·∫£n
        if not is_index(symbol):
            try:
                result['fundamental_data'] = await self.data_loader.get_fundamental_data(symbol)
            except Exception as e:
                error_msg = f"L·ªói l·∫•y d·ªØ li·ªáu c∆° b·∫£n cho {symbol}: {str(e)}"
                logger.error(error_msg)
                result['errors'].append(error_msg)
        
        return result
    
    async def prepare_market_data(self, market_symbols: list = None) -> dict:
        """Chu·∫©n b·ªã d·ªØ li·ªáu t·ªïng quan th·ªã tr∆∞·ªùng"""
        if market_symbols is None:
            market_symbols = ['VNINDEX', 'VN30', 'HNX30', 'HNXINDEX']
        
        result = {
            'market_data': {},
            'market_indicators': {},
            'errors': []
        }
        
        for symbol in market_symbols:
            try:
                symbol_data = await self.prepare_symbol_data(symbol, timeframes=['1D'], num_candles=100)
                result['market_data'][symbol] = symbol_data['dataframes'].get('1D')
                result['market_indicators'][symbol] = symbol_data['indicators'].get('1D', {})
            except Exception as e:
                error_msg = f"L·ªói chu·∫©n b·ªã d·ªØ li·ªáu th·ªã tr∆∞·ªùng cho {symbol}: {str(e)}"
                logger.error(error_msg)
                result['errors'].append(error_msg)
        
        return result
    
    @staticmethod
    def extract_last_candle_info(df: pd.DataFrame) -> dict:
        """Tr√≠ch xu·∫•t th√¥ng tin n·∫øn g·∫ßn nh·∫•t"""
        if df is None or df.empty:
            return {}
        
        last_candle = df.iloc[-1].to_dict()
        previous_candle = df.iloc[-2].to_dict() if len(df) > 1 else {}
        
        if previous_candle:
            last_candle['change'] = last_candle.get('close', 0) - previous_candle.get('close', 0)
            last_candle['change_pct'] = (last_candle['change'] / previous_candle.get('close', 1)) * 100 if previous_candle.get('close') else 0
        
        return last_candle
    
    @staticmethod
    def extract_patterns(dfs: dict) -> dict:
        """Tr√≠ch xu·∫•t c√°c m·∫´u h√¨nh n·∫øn v√† ƒë·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t quan tr·ªçng"""
        patterns = {}
        
        for tf, df in dfs.items():
            if df is None or df.empty or len(df) < 5:
                continue
                
            df_tail = df.tail(5)
            
            # Ki·ªÉm tra xu h∆∞·ªõng
            close_prices = df_tail['close']
            trend = 'uptrend' if close_prices.iloc[-1] > close_prices.iloc[0] else 'downtrend'
            
            # Ki·ªÉm tra vi·ªác c·∫Øt qua SMA
            if 'sma20' in df.columns and 'sma50' in df.columns:
                last_row = df.iloc[-1]
                cross_sma20 = close_prices.iloc[-2] < df['sma20'].iloc[-2] and close_prices.iloc[-1] > df['sma20'].iloc[-1]
                cross_sma50 = close_prices.iloc[-2] < df['sma50'].iloc[-2] and close_prices.iloc[-1] > df['sma50'].iloc[-1]
                
                if cross_sma20:
                    patterns[f'{tf}_cross_sma20'] = 'bullish'
                if cross_sma50:
                    patterns[f'{tf}_cross_sma50'] = 'bullish'
            
            # Th√™m c√°c m·∫´u h√¨nh kh√°c khi c·∫ßn
            
            # L∆∞u xu h∆∞·ªõng
            patterns[f'{tf}_trend'] = trend
            
        return patterns

# ---------- THU TH·∫¨P TIN T·ª®C (S·ª¨A L·ªñI) ----------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
async def fetch_rss_feed(url: str) -> str:
    import aiohttp
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    raise ValueError(f"HTTP {response.status} khi l·∫•y RSS t·ª´ {url}")
    except asyncio.TimeoutError:
        logger.error(f"Timeout khi l·∫•y RSS t·ª´ {url}")
    except Exception as e:
        logger.error(f"L·ªói l·∫•y RSS t·ª´ {url}: {str(e)}")
    return None

async def get_news(symbol: str = None, limit: int = 3) -> list:
    cache_key = f"news_{symbol}_{limit}" if symbol else f"news_market_{limit}"
    cached_news = await redis_manager.get(cache_key)
    if cached_news is not None:
        return cached_news

    rss_urls = [
        "https://cafef.vn/thi-truong-chung-khoan.rss",
        "https://cafef.vn/smart-money.rss",
        "https://cafef.vn/tai-chinh-ngan-hang.rss",
        "https://cafef.vn/doanh-nghiep.rss",
        "https://vnexpress.net/rss/kinh-doanh.rss",
        "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
        "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml"
    ]

    market_keywords = ["market", "stock", "index", "economy", "th·ªã tr∆∞·ªùng", "ch·ª©ng kho√°n", "l√£i su·∫•t", "vnindex"]
    symbol_keywords = [symbol.lower(), f"{symbol.lower()} ", f"m√£ {symbol.lower()}"] if symbol else []

    news_list = []
    tasks = [fetch_rss_feed(url) for url in rss_urls]
    rss_results = await asyncio.gather(*tasks)
    for rss_text in rss_results:
        if rss_text is None:
            continue
        feed = parse_rss_content(rss_text)
        if not feed or not feed.entries:
            continue
        for entry in feed.entries[:2]:
            title = entry.get("title", "").strip()
            link = entry.get("link", "")
            summary = entry.get("summary", entry.get("description", "")).strip()
            pub_date = entry.get("published", datetime.now().isoformat())
            content = f"{title} {summary}".lower()
            match = False
            if symbol and any(kw in content for kw in symbol_keywords):
                match = True
            elif not symbol and any(kw in content for kw in market_keywords):
                match = True
            if match:
                news_list.append({
                    "title": title,
                    "link": link,
                    "summary": summary[:200] + "..." if len(summary) > 200 else summary,
                    "published": pub_date
                })
    unique_news = sorted(news_list, key=lambda x: x.get("published", ""), reverse=True)[:limit]
    result = unique_news if unique_news else [{"title": "‚ö†Ô∏è Kh√¥ng c√≥ tin t·ª©c", "link": "#", "summary": ""}]
    await redis_manager.set(cache_key, result, expire=NEWS_CACHE_EXPIRE)
    return result

def parse_rss_content(rss_text: str):
    try:
        return feedparser.parse(rss_text) if rss_text else None
    except Exception as e:
        logger.error(f"L·ªói ph√¢n t√≠ch RSS: {str(e)}")
        return None

# ---------- PH√ÇN T√çCH C∆† B·∫¢N ----------
def deep_fundamental_analysis(fundamental_data: dict) -> str:
    report = "üìä **Ph√¢n t√≠ch c∆° b·∫£n**:\n"
    if not fundamental_data or 'error' in fundamental_data:
        return report + f"‚ùå {fundamental_data.get('error', 'Kh√¥ng c√≥ d·ªØ li·ªáu')}\n"

    keys_mapping = {
        'EPS': ['EPS', 'eps', 'Earning Per Share'],
        'P/E': ['P/E', 'PE', 'pe', 'Price to Earning', 'trailingPE'],
        'P/B': ['P/B', 'PB', 'pb', 'Price to Book', 'priceToBook'],
        'ROE': ['ROE', 'roe', 'Return on Equity', 'returnOnEquity'],
        'Dividend Yield': ['Dividend Yield', 'DY', 'dividend_yield', 'dividendYield'],
        'BVPS': ['BVPS', 'Book Value Per Share', 'bookValue'],
        'Market Cap': ['Market Cap', 'market_cap', 'marketCap']
    }

    extracted = {}
    for standard_key, possible_keys in keys_mapping.items():
        for key in possible_keys:
            if key in fundamental_data and fundamental_data[key] is not None:
                extracted[standard_key] = fundamental_data[key]
                break

    for key, value in extracted.items():
        report += f"- **{key}**: {value:.2f}\n" if isinstance(value, (int, float)) else f"- **{key}**: {value}\n"

    if 'P/E' in extracted and isinstance(extracted['P/E'], (int, float)):
        pe = extracted['P/E']
        report += "- **P/E**: C·ªï phi·∫øu c√≥ th·ªÉ ƒë·ªãnh gi√° th·∫•p\n" if pe < 10 else "- **P/E**: ƒê·ªãnh gi√° cao\n" if pe > 20 else "- **P/E**: ƒê·ªãnh gi√° h·ª£p l√Ω\n"

    if 'ROE' in extracted and isinstance(extracted['ROE'], (int, float)):
        report += "- **ROE**: Hi·ªáu qu·∫£ s·ª≠ d·ª•ng v·ªën t·ªët\n" if extracted['ROE'] > 15 else "- **ROE**: C·∫ßn c·∫£i thi·ªán\n"

    if 'Dividend Yield' in extracted and isinstance(extracted['Dividend Yield'], (int, float)):
        report += "- **Dividend**: H·∫•p d·∫´n\n" if extracted['Dividend Yield'] > 5 else "- **Dividend**: Trung b√¨nh\n"

    return report

# ---------- HU·∫§N LUY·ªÜN V√Ä L∆ØU M√î H√åNH ----------
def prepare_data_for_prophet(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ d·ª± b√°o")
    df_reset = df.reset_index()
    df_reset['date'] = df_reset['date'].dt.tz_localize(None)
    df_reset = df_reset.rename(columns={'date': 'ds', 'close': 'y'})
    return df_reset[['ds', 'y']]

def get_vietnam_holidays(years) -> pd.DataFrame:
    holiday_list = []
    for year in years:
        vn_holidays = holidays.Vietnam(years=year)
        for date, name in vn_holidays.items():
            holiday_list.append({'ds': pd.to_datetime(date), 'holiday': name})
    holiday_df = pd.DataFrame(holiday_list)
    holiday_df['ds'] = holiday_df['ds'].dt.tz_localize(None)
    return holiday_df

def forecast_with_prophet(df: pd.DataFrame, periods: int = 7) -> (pd.DataFrame, Prophet):
    try:
        data = prepare_data_for_prophet(df)
        if data.empty:
            raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o Prophet.")
        current_year = datetime.now().year
        holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
        model = Prophet(holidays=holiday_df)
        model.fit(data)
        future = model.make_future_dataframe(periods=periods)
        forecast = model.predict(future)
        return forecast, model
    except Exception as e:
        logger.error(f"L·ªói d·ª± b√°o Prophet: {str(e)}")
        raise

def evaluate_prophet_performance(df: pd.DataFrame, forecast: pd.DataFrame) -> float:
    actual = df['close'].iloc[-len(forecast):].values
    predicted = forecast['yhat'].values[:len(actual)]
    if len(actual) < 1 or len(predicted) < 1:
        return 0.0
    mse = np.mean((actual - predicted) ** 2)
    return 1 / (1 + mse)

def predict_xgboost_signal(df: pd.DataFrame, features: list) -> (int, float):
    if df is None or df.empty:
        return "D·ªØ li·ªáu r·ªóng", 0.0
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        return "Kh√¥ng ƒë·ªß d·ªØ li·ªáu", 0.0
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    X_pred = X.iloc[-1:]
    y_test = y.iloc[-1:]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    pred = model.predict(X_pred)[0]
    accuracy = accuracy_score(y_test, [pred])
    return pred, accuracy

def train_prophet_model(df: pd.DataFrame) -> (Prophet, float):
    data = prepare_data_for_prophet(df)
    if data.empty:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán Prophet")
    current_year = datetime.now().year
    holiday_df = get_vietnam_holidays(range(current_year-1, current_year+2))
    model = Prophet(holidays=holiday_df)
    model.fit(data)
    future = model.make_future_dataframe(periods=0)
    forecast = model.predict(future)
    performance = evaluate_prophet_performance(df, forecast)
    return model, performance

def train_xgboost_model(df: pd.DataFrame, features: list) -> (xgb.XGBClassifier, float):
    if df is None or df.empty:
        raise ValueError("DataFrame r·ªóng, kh√¥ng th·ªÉ hu·∫•n luy·ªán XGBoost")
    df = df.copy()
    df['target'] = (df['close'] > df['close'].shift(1)).astype(int)
    X = df[features].shift(1)
    y = df['target']
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    if len(X) < 100:
        raise ValueError("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán XGBoost")
    X_train = X.iloc[:-1]
    y_train = y.iloc[:-1]
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_train)
    performance = accuracy_score(y_train, y_pred)
    return model, performance

# ---------- AUTO TRAINING ----------
async def get_training_symbols() -> list:
    try:
        async with SessionLocal() as session:
            result = await session.execute(select(ReportHistory.symbol).distinct())
            symbols = [row[0] for row in result.fetchall()]
            logger.info(f"C√°c m√£ ƒë∆∞·ª£c l·∫•y t·ª´ l·ªãch s·ª≠ b√°o c√°o: {symbols}")
            return symbols
    except Exception as e:
        logger.error(f"L·ªói truy v·∫•n symbols t·ª´ ReportHistory: {str(e)}")
        return []

async def train_models_for_symbol(symbol: str):
    try:
        logger.info(f"B·∫Øt ƒë·∫ßu auto training cho m√£: {symbol}")
        loader = DataLoader()
        tech_analyzer = TechnicalAnalyzer()
        df, _ = await loader.load_data(symbol, '1D', 500)
        df = tech_analyzer.calculate_indicators(df)
        features = ['sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal',
                    'bb_high', 'bb_low', 'ichimoku_a', 'ichimoku_b', 'vwap', 'mfi']
        task_prophet = asyncio.to_thread(train_prophet_model, df)
        task_xgb = asyncio.to_thread(train_xgboost_model, df, features)
        (prophet_model, prophet_perf), (xgb_model, xgb_perf) = await asyncio.gather(task_prophet, task_xgb)
        await model_db_manager.store_trained_model(symbol, 'prophet', prophet_model, prophet_perf)
        await model_db_manager.store_trained_model(symbol, 'xgboost', xgb_model, xgb_perf)
        logger.info(f"Auto training cho {symbol} ho√†n t·∫•t.")
    except Exception as e:
        logger.error(f"L·ªói auto training cho {symbol}: {str(e)}")

async def auto_train_models():
    try:
        symbols = await get_training_symbols()
        if not symbols:
            logger.info("Kh√¥ng c√≥ m√£ n√†o trong ReportHistory, b·ªè qua auto training.")
            return
        tasks = [train_models_for_symbol(symbol) for symbol in symbols]
        await asyncio.gather(*tasks)
    except Exception as e:
        logger.error(f"L·ªói auto training: {str(e)}")

# ---------- AI V√Ä B√ÅO C√ÅO ----------
class AIAnalyzer:
    def __init__(self):
        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        # Khai b√°o b·ªô ƒë·∫øm API calls ƒë·ªÉ theo d√µi
        self.api_calls_count = 0
        self.last_reset_time = datetime.now()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def generate_content(self, prompt):
        # S·ª≠ d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng API call ƒë·ªìng th·ªùi
        async with api_semaphore:
            # Theo d√µi s·ªë l∆∞·ª£ng API calls
            current_time = datetime.now()
            if (current_time - self.last_reset_time).total_seconds() > 60:
                # Reset b·ªô ƒë·∫øm m·ªói ph√∫t
                self.api_calls_count = 0
                self.last_reset_time = current_time
                
            self.api_calls_count += 1
            if self.api_calls_count > 20:  # Gi·ªõi h·∫°n 20 calls/ph√∫t
                # Ch·ªù ƒë·ª£i n·∫øu v∆∞·ª£t qu√° gi·ªõi h·∫°n
                wait_time = 60 - (current_time - self.last_reset_time).total_seconds()
                if wait_time > 0:
                    logger.info(f"ƒê√£ ƒë·∫°t gi·ªõi h·∫°n API calls, ch·ªù {wait_time:.1f}s tr∆∞·ªõc khi ti·∫øp t·ª•c")
                    await asyncio.sleep(wait_time)
                self.api_calls_count = 1
                self.last_reset_time = datetime.now()
                
            logger.info(f"Th·ª±c hi·ªán API call ({self.api_calls_count}/20 trong ph√∫t n√†y)")
            return await self.model.generate_content_async(prompt)
    
    async def analyze_with_openrouter(self, technical_data):
        if not OPENROUTER_API_KEY:
            raise Exception("Ch∆∞a c√≥ OPENROUTER_API_KEY")

        # T√≠nh to√°n m·ª©c h·ªó tr·ª£/kh√°ng c·ª± t·ª´ d·ªØ li·ªáu candlestick
        df = pd.DataFrame(technical_data["candlestick_data"])
        calculated_levels = self.calculate_support_resistance_levels(df)
        
        # T·ªëi ∆∞u prompt ƒë·ªÉ gi·∫£m token
        prompt = (
            "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t ch·ª©ng kho√°n. Nh·∫≠n di·ªán m·∫´u h√¨nh n·∫øn, "
            "s√≥ng Elliott, Wyckoff, v√† c√°c v√πng h·ªó tr·ª£/kh√°ng c·ª± t·ª´ d·ªØ li·ªáu sau:"
            f"\n\nGi√° hi·ªán t·∫°i: {df['close'].iloc[-1]:.2f}"
            "\n\nCh·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON, kh√¥ng th√™m gi·∫£i th√≠ch:\n"
            "{\n"
            "  \"support_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"resistance_levels\": [gi√°1, gi√°2, ...],\n"
            "  \"patterns\": [\n"
            "    {\"name\": \"t√™n m·∫´u h√¨nh\", \"description\": \"gi·∫£i th√≠ch ng·∫Øn\"},\n"
            "    ...\n"
            "  ]\n"
            "}\n\n"
            f"D·ªØ li·ªáu:\n{json.dumps(technical_data, ensure_ascii=False)}"
        )

        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://yourapp.com",
            "X-Title": "Stock Analysis Bot"
        }
        payload = {
            "model": "deepseek/deepseek-chat-v3-0324:free",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 1024,
            "temperature": 0.2
        }

        # S·ª≠ d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n API calls
        async with api_semaphore:
            async with aiohttp.ClientSession() as session:
                async with session.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload) as resp:
                    text = await resp.text()
                    try:
                        result = json.loads(text)
                        logger.info(f"OpenRouter response keys: {result.keys()}")
                        content = result['choices'][0]['message']['content']
                        
                        # X·ª≠ l√Ω khi n·ªôi dung ƒë∆∞·ª£c b·ªçc trong ```json ... ```
                        if content.startswith('```json') and content.endswith('```'):
                            content = content[7:-3]  # C·∫Øt b·ªè ```json v√† ```
                        
                        # X·ª≠ l√Ω khi n·ªôi dung l√† plain JSON
                        try:
                            openrouter_response = json.loads(content)
                            
                            # Ki·ªÉm tra v√† l·ªçc m·ª©c h·ªó tr·ª£/kh√°ng c·ª± t·ª´ OpenRouter
                            current_price = df['close'].iloc[-1]
                            
                            # L·ªçc m·ª©c h·ªó tr·ª£ (ph·∫£i th·∫•p h∆°n gi√° hi·ªán t·∫°i v√† l√† s·ªë h·ª£p l·ªá)
                            filtered_support = []
                            openrouter_support = openrouter_response.get('support_levels', [])
                            for level in openrouter_support:
                                try:
                                    level_value = float(level)
                                    if level_value < current_price and level_value > 0:
                                        filtered_support.append(round(level_value, 2))
                                except (ValueError, TypeError):
                                    continue
                            
                            # L·ªçc m·ª©c kh√°ng c·ª± (ph·∫£i cao h∆°n gi√° hi·ªán t·∫°i v√† l√† s·ªë h·ª£p l·ªá)
                            filtered_resistance = []
                            openrouter_resistance = openrouter_response.get('resistance_levels', [])
                            for level in openrouter_resistance:
                                try:
                                    level_value = float(level)
                                    if level_value > current_price and level_value > 0:
                                        filtered_resistance.append(round(level_value, 2))
                                except (ValueError, TypeError):
                                    continue
                            
                            # N·∫øu kh√¥ng c√≥ m·ª©c h·ªó tr·ª£/kh√°ng c·ª± ho·∫∑c kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ ph∆∞∆°ng ph√°p t√≠nh to√°n
                            if not filtered_support and calculated_levels['support_levels']:
                                filtered_support = calculated_levels['support_levels']
                            if not filtered_resistance and calculated_levels['resistance_levels']:
                                filtered_resistance = calculated_levels['resistance_levels']
                            
                            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ l·ªçc
                            return {
                                "support_levels": filtered_support,
                                "resistance_levels": filtered_resistance,
                                "patterns": openrouter_response.get('patterns', [])
                            }
                        except json.JSONDecodeError:
                            logger.error(f"L·ªói parse JSON t·ª´ n·ªôi dung: {content}")
                            return calculated_levels
                    except json.JSONDecodeError:
                        logger.error(f"Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ OpenRouter: {text}")
                        return calculated_levels
                    except KeyError as e:
                        logger.error(f"Ph·∫£n h·ªìi thi·∫øu tr∆∞·ªùng c·∫ßn thi·∫øt: {e}")
                        return calculated_levels

    async def generate_report(self, dfs: dict, symbol: str, fundamental_data: dict, outlier_reports: dict) -> str:
        try:
            tech_analyzer = TechnicalAnalyzer()
            indicators = tech_analyzer.calculate_multi_timeframe_indicators(dfs)
            news = await get_news(symbol=symbol)
            news_text = "\n".join([f"üì∞ **{n['title']}**\nüîó {n['link']}\nüìù {n['summary']}" for n in news])
            df_1d = dfs.get('1D')
            close_today = df_1d['close'].iloc[-1]
            close_yesterday = df_1d['close'].iloc[-2]
            price_action = self.analyze_price_action(df_1d)
            history = await self.load_report_history(symbol)
            past_report = ""
            if history:
                last = history[-1]
                if is_index(symbol):
                    past_result = "ƒë√∫ng" if ((close_today > last["close_today"] and "tƒÉng" in last["report"].lower()) or
                                           (close_today < last["close_today"] and "gi·∫£m" in last["report"].lower())) else "sai"
                else:
                    past_result = "ƒë√∫ng" if (close_today > last["close_today"] and "mua" in last["report"].lower()) else "sai"
                past_report = f"üìú **B√°o c√°o tr∆∞·ªõc** ({last['date']}): {last['close_today']} ‚Üí {close_today} ({past_result})\n"
            
            # Ph√¢n t√≠ch v·ªõi OpenRouter
            technical_data = {
                "candlestick_data": df_1d.tail(50).to_dict(orient="records"),
                "technical_indicators": indicators['1D']
            }
            openrouter_result = await self.analyze_with_openrouter(technical_data)
            support_levels = openrouter_result.get('support_levels', [])
            resistance_levels = openrouter_result.get('resistance_levels', [])
            patterns = openrouter_result.get('patterns', [])

            forecast, prophet_model = forecast_with_prophet(df_1d, periods=7)
            prophet_perf = evaluate_prophet_performance(df_1d, forecast)
            future_forecast = forecast[forecast['ds'] > df_1d.index[-1].tz_localize(None)]
            if not future_forecast.empty:
                next_day_pred = future_forecast.iloc[0]
                day7_pred = future_forecast.iloc[6] if len(future_forecast) >= 7 else future_forecast.iloc[-1]
            else:
                next_day_pred = forecast.iloc[-1]
                day7_pred = forecast.iloc[-1]

            forecast_summary = f"**D·ª± b√°o gi√° (Prophet)** (Hi·ªáu su·∫•t: {prophet_perf:.2f}):\n"
            forecast_summary += f"- Ng√†y ti·∫øp theo ({next_day_pred['ds'].strftime('%d/%m/%Y')}): {next_day_pred['yhat']:.2f}\n"
            forecast_summary += f"- Sau 7 ng√†y ({day7_pred['ds'].strftime('%d/%m/%Y')}): {day7_pred['yhat']:.2f}\n"

            features = ['sma20', 'sma50', 'sma200', 'rsi', 'macd', 'signal', 'bb_high', 'bb_low', 'ichimoku_a', 'ichimoku_b', 'vwap', 'mfi']
            xgb_signal, xgb_perf = predict_xgboost_signal(df_1d.copy(), features)
            if isinstance(xgb_signal, int):
                xgb_text = "TƒÉng" if xgb_signal == 1 else "Gi·∫£m"
            else:
                xgb_text = xgb_signal
            
            if is_index(symbol):
                xgb_summary = f"**XGBoost d·ª± ƒëo√°n xu h∆∞·ªõng ti·∫øp theo** (Hi·ªáu su·∫•t: {xgb_perf:.2f}): {xgb_text}\n"
            else:
                xgb_summary = f"**XGBoost d·ª± ƒëo√°n t√≠n hi·ªáu giao d·ªãch** (Hi·ªáu su·∫•t: {xgb_perf:.2f}): {xgb_text}\n"

            outlier_text = "\n".join([f"**{tf}**: {report}" for tf, report in outlier_reports.items()])

            # T·ª± t√≠nh to√°n th√™m m·ª©c h·ªó tr·ª£/kh√°ng c·ª± ƒë·ªÉ ƒë·ªëi chi·∫øu
            calculated_levels = self.calculate_support_resistance_levels(df_1d)
            calc_support_str = ", ".join([f"{level:.2f}" for level in calculated_levels['support_levels']])
            calc_resistance_str = ", ".join([f"{level:.2f}" for level in calculated_levels['resistance_levels']])

            # T·∫°o prompt kh√°c nhau cho ch·ªâ s·ªë v√† c·ªï phi·∫øu
            if is_index(symbol):
                # Ph√¢n t√≠ch cho ch·ªâ s·ªë
                fundamental_report = f"üìä **{symbol} l√† ch·ªâ s·ªë, kh√¥ng ph·∫£i c·ªï phi·∫øu**\n"
                
                prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t, ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam v·ªõi 30 nƒÉm kinh nghi·ªám. 
H√£y vi·∫øt b√°o c√°o chi ti·∫øt cho CH·ªà S·ªê {symbol} (L∆ØU √ù: ƒê√ÇY L√Ä CH·ªà S·ªê, KH√îNG PH·∫¢I C·ªî PHI·∫æU):

**Th√¥ng tin c∆° b·∫£n:**
- Ng√†y: {datetime.now().strftime('%d/%m/%Y')}
- Gi√° h√¥m qua: {close_yesterday:.2f}
- Gi√° h√¥m nay: {close_today:.2f} ({((close_today-close_yesterday)/close_yesterday*100):.2f}%)

**Di·ªÖn bi·∫øn ch·ªâ s·ªë:**
{price_action}

**L·ªãch s·ª≠ d·ª± ƒëo√°n:**
{past_report}

**Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu:**
{outlier_text}

**Ch·ªâ s·ªë k·ªπ thu·∫≠t:**
"""
                for tf, ind in indicators.items():
                    prompt += f"\n--- {tf} ---\n"
                    prompt += f"- Close: {ind.get('close', 0):.2f}\n"
                    prompt += f"- SMA20: {ind.get('sma20', 0):.2f}, SMA50: {ind.get('sma50', 0):.2f}, SMA200: {ind.get('sma200', 0):.2f}\n"
                    prompt += f"- RSI: {ind.get('rsi', 0):.2f}\n"
                    prompt += f"- MACD: {ind.get('macd', 0):.2f} (Signal: {ind.get('signal', 0):.2f})\n"
                    prompt += f"- Bollinger: {ind.get('bb_low', 0):.2f} - {ind.get('bb_high', 0):.2f}\n"
                    prompt += f"- Ichimoku: A: {ind.get('ichimoku_a', 0):.2f}, B: {ind.get('ichimoku_b', 0):.2f}\n"
                    prompt += f"- Fibonacci: 0.0: {ind.get('fib_0.0', 0):.2f}, 61.8: {ind.get('fib_61.8', 0):.2f}\n"

                prompt += f"\n**Tin t·ª©c th·ªã tr∆∞·ªùng:**\n{news_text}\n"
                prompt += f"\n**Ph√¢n t√≠ch m·ª©c h·ªó tr·ª£/kh√°ng c·ª± c·ªßa ch·ªâ s·ªë:**\n"
                prompt += f"- M·ª©c h·ªó tr·ª£: {', '.join(map(str, support_levels))}\n"
                prompt += f"- M·ª©c kh√°ng c·ª±: {', '.join(map(str, resistance_levels))}\n"
                prompt += f"- M·ª©c h·ªó tr·ª£ t·ª´ ph√¢n t√≠ch ƒë·ªì th·ªã: {calc_support_str}\n"  
                prompt += f"- M·ª©c kh√°ng c·ª± t·ª´ ph√¢n t√≠ch ƒë·ªì th·ªã: {calc_resistance_str}\n"
                prompt += f"- M·∫´u h√¨nh ƒë·ªì th·ªã: {', '.join([p.get('name', 'Unknown') for p in patterns])}\n"
                prompt += f"\n{xgb_summary}\n"
                prompt += f"{forecast_summary}\n"
                prompt += """
**Y√™u c·∫ßu:**
1. ƒê√°nh gi√° t·ªïng quan th·ªã tr∆∞·ªùng. So s√°nh ch·ªâ s·ªë phi√™n hi·ªán t·∫°i v√† phi√™n tr∆∞·ªõc ƒë√≥.
2. Ph√¢n t√≠ch ƒëa khung th·ªùi gian, xu h∆∞·ªõng ng·∫Øn h·∫°n, trung h·∫°n, d√†i h·∫°n c·ªßa CH·ªà S·ªê.
3. ƒê√°nh gi√° c√°c m√¥ h√¨nh, m·∫´u h√¨nh, s√≥ng (n·∫øu c√≥) ch·ªâ s·ªë k·ªπ thu·∫≠t, ƒë·ªông l·ª±c th·ªã tr∆∞·ªùng.
4. X√°c ƒë·ªãnh h·ªó tr·ª£/kh√°ng c·ª± cho CH·ªà S·ªê. ƒê∆∞a ra k·ªãch b·∫£n v√† x√°c su·∫•t % (tƒÉng, gi·∫£m, sideway).
5. ƒê·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c cho nh√† ƒë·∫ßu t∆∞: n√™n theo xu h∆∞·ªõng th·ªã tr∆∞·ªùng hay ƒëi ng∆∞·ª£c, m·ª©c ƒë·ªô th·∫≠n tr·ªçng.
6. ƒê√°nh gi√° r·ªßi ro th·ªã tr∆∞·ªùng hi·ªán t·∫°i.
7. ƒê∆∞a ra nh·∫≠n ƒë·ªãnh t·ªïng th·ªÉ v·ªÅ xu h∆∞·ªõng th·ªã tr∆∞·ªùng.
8. Kh√¥ng c·∫ßn theo form c·ªë ƒë·ªãnh, tr√¨nh b√†y logic, s√∫c t√≠ch nh∆∞ng ƒë·ªß th√¥ng tin ƒë·ªÉ h√†nh ƒë·ªông v√† s√°ng t·∫°o v·ªõi emoji.

**H∆∞·ªõng d·∫´n b·ªï sung:**
- QUAN TR·ªåNG: ƒê√¢y l√† ph√¢n t√≠ch cho CH·ªà S·ªê, KH√îNG PH·∫¢I C·ªî PHI·∫æU. Kh√¥ng ƒë∆∞a ra khuy·∫øn ngh·ªã mua/b√°n ch·ªâ s·ªë.
- D·ª±a v√†o h√†nh ƒë·ªông gi√° g·∫ßn ƒë√¢y ƒë·ªÉ x√°c ƒë·ªãnh qu√°n t√≠nh (momentum) hi·ªán t·∫°i.
- S·ª≠ d·ª•ng d·ªØ li·ªáu, s·ªë li·ªáu ƒë∆∞·ª£c cung c·∫•p, KH√îNG t·ª± suy di·ªÖn th√™m.
"""
            else:
                # Ph√¢n t√≠ch cho c·ªï phi·∫øu
                fundamental_report = deep_fundamental_analysis(fundamental_data)
                
                prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n, trader chuy√™n nghi·ªáp, chuy√™n gia b·∫Øt ƒë√°y 30 nƒÉm kinh nghi·ªám ·ªü ch·ª©ng kho√°n Vi·ªát Nam. H√£y vi·∫øt b√°o c√°o chi ti·∫øt cho c·ªï phi·∫øu {symbol}:

**Th√¥ng tin c∆° b·∫£n:**
- Ng√†y: {datetime.now().strftime('%d/%m/%Y')}
- Gi√° h√¥m qua: {close_yesterday:.2f}
- Gi√° h√¥m nay: {close_today:.2f} ({((close_today-close_yesterday)/close_yesterday*100):.2f}%)

**H√†nh ƒë·ªông gi√°:**
{price_action}

**L·ªãch s·ª≠ d·ª± ƒëo√°n:**
{past_report}

**Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu:**
{outlier_text}

**Ch·ªâ s·ªë k·ªπ thu·∫≠t:**
"""
                for tf, ind in indicators.items():
                    prompt += f"\n--- {tf} ---\n"
                    prompt += f"- Close: {ind.get('close', 0):.2f}\n"
                    prompt += f"- SMA20: {ind.get('sma20', 0):.2f}, SMA50: {ind.get('sma50', 0):.2f}, SMA200: {ind.get('sma200', 0):.2f}\n"
                    prompt += f"- RSI: {ind.get('rsi', 0):.2f}\n"
                    prompt += f"- MACD: {ind.get('macd', 0):.2f} (Signal: {ind.get('signal', 0):.2f})\n"
                    prompt += f"- Bollinger: {ind.get('bb_low', 0):.2f} - {ind.get('bb_high', 0):.2f}\n"
                    prompt += f"- Ichimoku: A: {ind.get('ichimoku_a', 0):.2f}, B: {ind.get('ichimoku_b', 0):.2f}\n"
                    prompt += f"- Fibonacci: 0.0: {ind.get('fib_0.0', 0):.2f}, 61.8: {ind.get('fib_61.8', 0):.2f}\n"
                prompt += f"\n**C∆° b·∫£n:**\n{fundamental_report}\n"
                prompt += f"\n**Tin t·ª©c:**\n{news_text}\n"
                prompt += f"\n**Ph√¢n t√≠ch m·ª©c h·ªó tr·ª£/kh√°ng c·ª±:**\n"
                prompt += f"- M·ª©c h·ªó tr·ª£: {', '.join(map(str, support_levels))}\n"
                prompt += f"- M·ª©c kh√°ng c·ª±: {', '.join(map(str, resistance_levels))}\n"
                prompt += f"- M·ª©c h·ªó tr·ª£ t·ª´ ph√¢n t√≠ch ƒë·ªì th·ªã: {calc_support_str}\n"  
                prompt += f"- M·ª©c kh√°ng c·ª± t·ª´ ph√¢n t√≠ch ƒë·ªì th·ªã: {calc_resistance_str}\n"
                prompt += f"- M·∫´u h√¨nh n·∫øn: {', '.join([p.get('name', 'Unknown') for p in patterns])}\n"
                prompt += f"\n{xgb_summary}\n"
                prompt += f"{forecast_summary}\n"
                prompt += """
**Y√™u c·∫ßu:**
1. ƒê√°nh gi√° t·ªïng quan. So s√°nh gi√°/ch·ªâ s·ªë phi√™n hi·ªán t·∫°i v√† phi√™n tr∆∞·ªõc ƒë√≥.
2. Ph√¢n t√≠ch ƒëa khung th·ªùi gian, xu h∆∞·ªõng ng·∫Øn h·∫°n, trung h·∫°n, d√†i h·∫°n.
3. ƒê√°nh gi√° c√°c m√¥ h√¨nh, m·∫´u h√¨nh, s√≥ng (n·∫øu c√≥), ch·ªâ s·ªë k·ªπ thu·∫≠t, ƒë·ªông l·ª±c th·ªã tr∆∞·ªùng.
4. X√°c ƒë·ªãnh h·ªó tr·ª£/kh√°ng c·ª±. ƒê∆∞a ra k·ªãch b·∫£n v√† x√°c su·∫•t % (tƒÉng, gi·∫£m, sideway).
5. ƒê·ªÅ xu·∫•t c√°c chi·∫øn l∆∞·ª£c giao d·ªãch ph√π h·ª£p, v·ªõi % tin c·∫≠y.
6. ƒê√°nh gi√° r·ªßi ro v√† t·ª∑ l·ªá risk/reward.
7. ƒê∆∞a ra nh·∫≠n ƒë·ªãnh.
8. Kh√¥ng c·∫ßn theo form c·ªë ƒë·ªãnh, tr√¨nh b√†y logic, s√∫c t√≠ch nh∆∞ng ƒë·ªß th√¥ng tin ƒë·ªÉ h√†nh ƒë·ªông v√† s√°ng t·∫°o v·ªõi emoji.

**H∆∞·ªõng d·∫´n b·ªï sung:**
- D·ª±a v√†o h√†nh ƒë·ªông gi√° g·∫ßn ƒë√¢y ƒë·ªÉ x√°c ƒë·ªãnh qu√°n t√≠nh (momentum) hi·ªán t·∫°i.
- S·ª≠ d·ª•ng d·ªØ li·ªáu, s·ªë li·ªáu ƒë∆∞·ª£c cung c·∫•p, KH√îNG t·ª± suy di·ªÖn th√™m.
"""

            response = await self.generate_content(prompt)
            report = response.text
            await self.save_report_history(symbol, report, close_today, close_yesterday)
            return report
        except Exception as e:
            logger.error(f"L·ªói t·∫°o b√°o c√°o: {str(e)}")
            return f"‚ùå L·ªói t·∫°o b√°o c√°o: {str(e)}"

# ---------- TELEGRAM COMMANDS ----------
async def notify_admin_new_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.message.from_user
    user_id = user.id
    if not await is_user_approved(user_id):
        message = f"üîî Ng∆∞·ªùi d√πng m·ªõi:\nID: {user_id}\nUsername: {user.username}\nT√™n: {user.full_name}\nDuy·ªát: /approve {user_id}"
        await context.bot.send_message(chat_id=ADMIN_ID, text=message)
        await update.message.reply_text("‚è≥ Ch·ªù admin duy·ªát!")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    logger.info(f"Start called: user_id={user_id}, ADMIN_ID={ADMIN_ID}")

    if str(user_id) == ADMIN_ID and not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        logger.info(f"Admin {user_id} t·ª± ƒë·ªông duy·ªát.")

    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return

    await update.message.reply_text(
        "üöÄ **V18.11 - THUA GIA C√ÅT L∆Ø·ª¢NG M·ªñI C√ÅI QU·∫†T!**\n"
        "üìä **L·ªánh**:\n"
        "- /analyze [M√£] [S·ªë n·∫øn] - Ph√¢n t√≠ch ƒëa khung.\n"
        "- /getid - L·∫•y ID.\n"
        "- /approve [user_id] - Duy·ªát ng∆∞·ªùi d√πng (admin).\n"
        "üí° **B·∫Øt ƒë·∫ßu n√†o!**"
    )

async def analyze_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    if not await is_user_approved(user_id):
        await notify_admin_new_user(update, context)
        return
    try:
        args = context.args
        if not args:
            raise ValueError("Nh·∫≠p m√£ ch·ª©ng kho√°n (e.g., VNINDEX, SSI).")
        symbol = args[0].upper()
        num_candles = int(args[1]) if len(args) > 1 else DEFAULT_CANDLES
        
        # Gi·ªõi h·∫°n s·ªë n·∫øn ƒë·ªÉ tr√°nh qu√° t·∫£i
        if num_candles < 20:
            raise ValueError("S·ªë n·∫øn ph·∫£i l·ªõn h∆°n ho·∫∑c b·∫±ng 20 ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o!")
        if num_candles > 300:
            num_candles = 300
            await update.message.reply_text("‚ö†Ô∏è S·ªë n·∫øn ƒë√£ ƒë∆∞·ª£c gi·ªõi h·∫°n t·ªëi ƒëa 300 ƒë·ªÉ tr√°nh qu√° t·∫£i.")
        
        # Ki·ªÉm tra cache tr∆∞·ªõc
        cache_key = f"report_{symbol}_{num_candles}"
        cached_report = await redis_manager.get(cache_key)
        if cached_report:
            await update.message.reply_text(f"üìä B√°o c√°o t·ª´ cache cho {symbol}:")
            formatted_report = f"<b>üìà B√°o c√°o ph√¢n t√≠ch cho {symbol}</b>\n\n"
            formatted_report += f"<pre>{html.escape(cached_report)}</pre>"
            await update.message.reply_text(formatted_report, parse_mode='HTML')
            
            # T·∫°o m·ªôt task ƒë·ªÉ l√†m m·ªõi cache trong n·ªÅn n·∫øu b√°o c√°o ƒë√£ c≈© (>30 ph√∫t)
            if isinstance(cached_report, dict) and cached_report.get('timestamp'):
                cache_time = datetime.fromisoformat(cached_report.get('timestamp'))
                if (datetime.now() - cache_time).total_seconds() > 1800:  # 30 ph√∫t
                    asyncio.create_task(refresh_report_cache(symbol, num_candles))
            return
            
        # S·ª≠ d·ª•ng pipeline chu·∫©n h√≥a
        data_pipeline = DataPipeline()
        ai_analyzer = AIAnalyzer()
        
        # G·ª≠i th√¥ng b√°o tr·∫°ng th√°i t·∫£i d·ªØ li·ªáu
        status_message = await update.message.reply_text(f"‚è≥ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu cho {symbol}...")
        
        # S·ª≠ d·ª•ng timeframe ph√π h·ª£p cho m·ªói lo·∫°i ph√¢n t√≠ch
        timeframes = ['1D']
        if not is_index(symbol):  # Ch·ªâ t·∫£i nhi·ªÅu khung th·ªùi gian cho c·ªï phi·∫øu
            timeframes = ['1D', '1W']  # Gi·∫£m b·ªõt khung th·ªùi gian (kh√¥ng t·∫£i 1M)
            
        # Chu·∫©n b·ªã d·ªØ li·ªáu v·ªõi pipeline
        pipeline_result = await data_pipeline.prepare_symbol_data(symbol, timeframes=timeframes, num_candles=num_candles)
        
        # C·∫≠p nh·∫≠t tr·∫°ng th√°i
        await status_message.edit_text(f"‚è≥ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng. ƒêang ph√¢n t√≠ch {symbol}...")
        
        if pipeline_result['errors']:
            error_message = f"‚ö†Ô∏è M·ªôt s·ªë l·ªói x·∫£y ra trong qu√° tr√¨nh chu·∫©n b·ªã d·ªØ li·ªáu:\n"
            error_message += "\n".join(pipeline_result['errors'])
            await update.message.reply_text(error_message)
        
        if not pipeline_result['dataframes']:
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho {symbol}")
        
        # T·∫°o b√°o c√°o v·ªõi AI
        report = await ai_analyzer.generate_report(
            pipeline_result['dataframes'], 
            symbol, 
            pipeline_result['fundamental_data'], 
            pipeline_result['outlier_reports']
        )
        
        # Th√™m timestamp v√†o cache ƒë·ªÉ bi·∫øt th·ªùi gian t·∫°o b√°o c√°o
        report_cache = {
            "report": report,
            "timestamp": datetime.now().isoformat()
        }
        
        # L∆∞u v√†o cache v·ªõi th·ªùi gian ng·∫Øn h∆°n
        cache_expire = CACHE_EXPIRE_SHORT // 2 if not is_index(symbol) else CACHE_EXPIRE_SHORT
        await redis_manager.set(f"report_{symbol}_{num_candles}", report_cache, expire=cache_expire)

        # C·∫≠p nh·∫≠t tr·∫°ng th√°i ho√†n th√†nh
        await status_message.delete()
        
        formatted_report = f"<b>üìà B√°o c√°o ph√¢n t√≠ch cho {symbol}</b>\n\n"
        formatted_report += f"<pre>{html.escape(report)}</pre>"
        await update.message.reply_text(formatted_report, parse_mode='HTML')
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ sau khi ho√†n th√†nh
        gc.collect()
        
    except ValueError as e:
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")
    except Exception as e:
        logger.error(f"L·ªói trong analyze_command: {str(e)}")
        await update.message.reply_text(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")

async def refresh_report_cache(symbol: str, num_candles: int):
    """L√†m m·ªõi cache b√°o c√°o trong n·ªÅn ƒë·ªÉ ng∆∞·ªùi d√πng ti·∫øp theo c√≥ d·ªØ li·ªáu m·ªõi"""
    try:
        logger.info(f"ƒêang l√†m m·ªõi cache b√°o c√°o cho {symbol} trong n·ªÅn")
        
        # T·∫°o m·ªõi d·ªØ li·ªáu
        data_pipeline = DataPipeline()
        ai_analyzer = AIAnalyzer()
        
        # S·ª≠ d·ª•ng timeframe ph√π h·ª£p
        timeframes = ['1D']
        if not is_index(symbol):
            timeframes = ['1D', '1W']
            
        # Chu·∫©n b·ªã d·ªØ li·ªáu v·ªõi pipeline
        pipeline_result = await data_pipeline.prepare_symbol_data(symbol, timeframes=timeframes, num_candles=num_candles)
        
        if not pipeline_result['dataframes']:
            logger.error(f"Kh√¥ng th·ªÉ l√†m m·ªõi cache cho {symbol}: kh√¥ng c√≥ d·ªØ li·ªáu")
            return
        
        # T·∫°o b√°o c√°o m·ªõi
        report = await ai_analyzer.generate_report(
            pipeline_result['dataframes'], 
            symbol, 
            pipeline_result['fundamental_data'], 
            pipeline_result['outlier_reports']
        )
        
        # Th√™m timestamp v√†o cache
        report_cache = {
            "report": report,
            "timestamp": datetime.now().isoformat()
        }
        
        # L∆∞u v√†o cache
        cache_expire = CACHE_EXPIRE_SHORT // 2 if not is_index(symbol) else CACHE_EXPIRE_SHORT
        await redis_manager.set(f"report_{symbol}_{num_candles}", report_cache, expire=cache_expire)
        
        logger.info(f"ƒê√£ l√†m m·ªõi cache b√°o c√°o cho {symbol} th√†nh c√¥ng")
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ
        gc.collect()
        
    except Exception as e:
        logger.error(f"L·ªói l√†m m·ªõi cache b√°o c√°o cho {symbol}: {str(e)}")

async def get_id(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    await update.message.reply_text(f"ID c·ªßa b·∫°n: {user_id}")

async def approve_user(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if str(update.message.from_user.id) != ADMIN_ID:
        await update.message.reply_text("‚ùå Ch·ªâ admin d√πng ƒë∆∞·ª£c l·ªánh n√†y!")
        return
    if len(context.args) != 1:
        await update.message.reply_text("‚ùå Nh·∫≠p user_id: /approve 123456789")
        return
    user_id = context.args[0]
    if not await db.is_user_approved(user_id):
        await db.add_approved_user(user_id)
        await update.message.reply_text(f"‚úÖ ƒê√£ duy·ªát {user_id}")
    else:
        await update.message.reply_text(f"‚ÑπÔ∏è {user_id} ƒë√£ ƒë∆∞·ª£c duy·ªát")

# ---------- MAIN & DEPLOY ----------
async def keep_alive():
    """Gi·ªØ cho ·ª©ng d·ª•ng kh√¥ng b·ªã ng·ªß tr√™n Render"""
    app_url = os.getenv("RENDER_EXTERNAL_URL", "")
    if not app_url:
        return
        
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(app_url, timeout=10) as response:
                if response.status == 200:
                    logger.info("Keep alive ping th√†nh c√¥ng")
                else:
                    logger.warning(f"Keep alive ping tr·∫£ v·ªÅ m√£ l·ªói: {response.status}")
    except Exception as e:
        logger.error(f"Keep alive ping th·∫•t b·∫°i: {str(e)}")

async def send_telegram_document(file_path, caption):
    """G·ª≠i file qua Telegram API"""
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendDocument"
        
        # Ki·ªÉm tra file t·ªìn t·∫°i
        if not os.path.exists(file_path):
            logger.error(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
            return False
            
        # Chu·∫©n b·ªã form data
        async with aiohttp.ClientSession() as session:
            form = aiohttp.FormData()
            form.add_field('chat_id', ADMIN_ID)
            form.add_field('caption', caption)
            
            # Th√™m file
            with open(file_path, 'rb') as file:
                form.add_field('document', file, 
                               filename=os.path.basename(file_path),
                               content_type='application/json')
            
            # G·ª≠i request
            async with session.post(url, data=form) as response:
                if response.status == 200:
                    result = await response.json()
                    if result.get('ok'):
                        logger.info(f"ƒê√£ g·ª≠i file {file_path} ƒë·∫øn Telegram th√†nh c√¥ng")
                        return True
                    else:
                        logger.error(f"L·ªói API Telegram: {result}")
                else:
                    logger.error(f"L·ªói HTTP khi g·ª≠i file: {response.status}")
                    
        return False
    except Exception as e:
        logger.error(f"L·ªói g·ª≠i file qua Telegram: {str(e)}")
        return False

async def backup_database():
    """Sao l∆∞u d·ªØ li·ªáu quan tr·ªçng v√† g·ª≠i qua Telegram thay v√¨ l∆∞u c·ª•c b·ªô"""
    try:
        # T·∫°o th∆∞ m·ª•c t·∫°m ƒë·ªÉ l∆∞u file backup
        temp_dir = "temp_backups"
        os.makedirs(temp_dir, exist_ok=True)
        
        # Sao l∆∞u danh s√°ch ng∆∞·ªùi d√πng
        async with SessionLocal() as session:
            users_query = await session.execute(select(ApprovedUser))
            users_data = [
                {
                    "user_id": user.user_id, 
                    "approved_at": user.approved_at.isoformat()
                } 
                for user in users_query.scalars().all()
            ]
            
            # Sao l∆∞u b√°o c√°o g·∫ßn nh·∫•t
            reports_query = await session.execute(
                select(ReportHistory).order_by(ReportHistory.id.desc()).limit(20)
            )
            reports_data = [
                {
                    "id": report.id,
                    "symbol": report.symbol,
                    "date": report.date,
                    "close_today": report.close_today,
                    "close_yesterday": report.close_yesterday,
                    "timestamp": report.timestamp.isoformat()
                }
                for report in reports_query.scalars().all()
            ]
            
            # Sao l∆∞u th√¥ng tin model ƒë√£ train
            models_query = await session.execute(
                select(TrainedModel).order_by(TrainedModel.id.desc())
            )
            models_data = [
                {
                    "id": model.id,
                    "symbol": model.symbol,
                    "model_type": model.model_type,
                    "created_at": model.created_at.isoformat(),
                    "performance": model.performance
                }
                for model in models_query.scalars().all()
            ]
        
        # L∆∞u v√†o t·ªáp t·∫°m th·ªùi
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_data = {
            "users": users_data,
            "reports": reports_data,
            "models": models_data,
            "timestamp": datetime.now().isoformat(),
            "app_version": "18.9"
        }
        
        backup_file = os.path.join(temp_dir, f"backup_{current_time}.json")
        with open(backup_file, "w", encoding="utf-8") as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        # G·ª≠i file qua Telegram
        caption = f"üîÑ Backup d·ªØ li·ªáu {datetime.now().strftime('%d/%m/%Y %H:%M')}\n"
        caption += f"üë• Users: {len(users_data)}\n"
        caption += f"üìä Reports: {len(reports_data)}\n"
        caption += f"ü§ñ Models: {len(models_data)}"
        
        sent = await send_telegram_document(backup_file, caption)
        
        if sent:
            logger.info(f"ƒê√£ sao l∆∞u d·ªØ li·ªáu v√† g·ª≠i qua Telegram th√†nh c√¥ng")
        else:
            logger.error("Kh√¥ng th·ªÉ g·ª≠i backup qua Telegram")
        
        # X√≥a file t·∫°m sau khi g·ª≠i
        try:
            os.remove(backup_file)
            logger.info(f"ƒê√£ x√≥a file t·∫°m: {backup_file}")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ x√≥a file t·∫°m {backup_file}: {str(e)}")
        
        return sent
    except Exception as e:
        logger.error(f"L·ªói sao l∆∞u d·ªØ li·ªáu: {str(e)}")
        return False

async def main():
    # Kh·ªüi t·∫°o DB 
    await init_db()

    # Thi·∫øt l·∫≠p semaphore cho API call
    global api_semaphore
    api_semaphore = asyncio.Semaphore(3)  # Gi·ªõi h·∫°n t·ªëi ƒëa 3 API call ƒë·ªìng th·ªùi

    # Kh·ªüi t·∫°o scheduler v·ªõi c√°c t√°c v·ª• ƒë·ªãnh k·ª≥
    scheduler = AsyncIOScheduler()
    
    # T√°c v·ª• ƒë·ªãnh k·ª≥
    scheduler.add_job(auto_train_models, 'cron', hour=2, minute=0)
    scheduler.add_job(keep_alive, 'interval', minutes=14)  # Ping tr∆∞·ªõc khi Render sleep (15 ph√∫t)
    scheduler.add_job(backup_database, 'cron', hour=1, minute=0)  # Sao l∆∞u h√†ng ng√†y l√∫c 1:00
    scheduler.add_job(redis_manager.optimize_cache, 'interval', hours=6)  # T·ªëi ∆∞u Redis cache m·ªói 6 gi·ªù
    
    scheduler.start()
    logger.info("Scheduler ƒë√£ kh·ªüi ƒë·ªông v·ªõi c√°c t√°c v·ª• ƒë·ªãnh k·ª≥.")

    # C√†i ƒë·∫∑t bot
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("analyze", analyze_command))
    app.add_handler(CommandHandler("getid", get_id))
    app.add_handler(CommandHandler("approve", approve_user))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, notify_admin_new_user))
    logger.info("ü§ñ Bot kh·ªüi ƒë·ªông!")

    # C√†i ƒë·∫∑t webhook v·ªõi c∆° ch·∫ø t·ª± ph·ª•c h·ªìi
    BASE_URL = os.getenv("RENDER_EXTERNAL_URL", f"https://{os.getenv('RENDER_SERVICE_NAME')}.onrender.com")
    WEBHOOK_URL = f"{BASE_URL}/{TELEGRAM_TOKEN}"
    
    async def setup_webhook():
        retry_count = 0
        max_retries = 5
        while retry_count < max_retries:
            try:
                webhook_info = await app.bot.get_webhook_info()
                if webhook_info.url != WEBHOOK_URL:
                    await app.bot.set_webhook(url=WEBHOOK_URL)
                    logger.info(f"Webhook ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng: {WEBHOOK_URL}")
                else:
                    logger.info(f"Webhook ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p tr∆∞·ªõc ƒë√≥: {WEBHOOK_URL}")
                return
            except Exception as e:
                retry_count += 1
                logger.error(f"L·ªói thi·∫øt l·∫≠p webhook (th·ª≠ l·∫ßn {retry_count}): {str(e)}")
                await asyncio.sleep(5)
    
    # Kh·ªüi t·∫°o webhook
    await setup_webhook()
    
    # Kh·ªüi ƒë·ªông webhook server
    await app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        webhook_url=WEBHOOK_URL,
        url_path=TELEGRAM_TOKEN
    )

class StockData(BaseModel):
    open: float
    high: float
    low: float
    close: float
    volume: int
    date: datetime

    @validator('high')
    def high_must_be_greater_than_low(cls, v, values):
        if 'low' in values and v < values['low']:
            raise ValueError('High must be greater than low')
        return v

    @validator('close')
    def close_must_be_within_range(cls, v, values):
        if 'low' in values and 'high' in values and not (values['low'] <= v <= values['high']):
            raise ValueError('Close must be within the range of low and high')
        return v

    @validator('volume')
    def volume_must_be_non_negative(cls, v):
        if v < 0:
            raise ValueError('Volume must be non-negative')
        return v

# Example usage
# stock_data = StockData(open=100.0, high=105.0, low=99.0, close=102.0, volume=1000, date=datetime.now())

def profile_function(func):
    """
    Decorator ƒë·ªÉ profile m·ªôt h√†m v√† ghi ra th√¥ng tin v·ªÅ hi·ªáu su·∫•t
    """
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        result = func(*args, **kwargs)
        profiler.disable()
        
        # T·∫°o b√°o c√°o
        s = StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # In ra 20 h√†m ti√™u t·ªën th·ªùi gian nh·∫•t
        
        logger.debug(f"Profiling results for {func.__name__}:\n{s.getvalue()}")
        return result
    return wrapper

# √Åp d·ª•ng cache ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£ t√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_technical_calculation(df_key, indicator_name):
    """
    H√†m n√†y s·∫Ω l∆∞u tr·ªØ k·∫øt qu·∫£ t√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
    df_key: m·ªôt key duy nh·∫•t ƒë·∫°i di·ªán cho dataframe (v√≠ d·ª•: symbol+timeframe+hash)
    indicator_name: t√™n c·ªßa ch·ªâ b√°o k·ªπ thu·∫≠t
    """
    # Th·ª±c hi·ªán t√≠nh to√°n ch·ªâ b√°o d·ª±a tr√™n key
    # ƒê√¢y ch·ªâ l√† h√†m gi√∫p cache k·∫øt qu·∫£
    return None

# T·ªëi ∆∞u h√≥a vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh
async def optimized_train_models_for_symbol(symbol: str):
    """
    Phi√™n b·∫£n t·ªëi ∆∞u c·ªßa train_models_for_symbol v·ªõi kh·∫£ nƒÉng ph√¢n t√≠ch hi·ªáu su·∫•t
    v√† c√°c t·ªëi ∆∞u h√≥a v·ªÅ b·ªô nh·ªõ v√† CPU
    """
    try:
        logger.info(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh cho {symbol}")
        
        # T·∫£i d·ªØ li·ªáu
        data_loader = DataLoader()
        df, status = await data_loader.load_data(symbol, 'daily', 1000)
        
        if df is None or df.empty:
            logger.error(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho {symbol}")
            return
        
        # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu t·ªëi ∆∞u h√≥a
        df = optimize_dataframe_memory(df)
        
        # Hu·∫•n luy·ªán c√°c m√¥ h√¨nh v√† ƒëo hi·ªáu su·∫•t
        model_db_manager = ModelDBManager()
        
        # Hu·∫•n luy·ªán m√¥ h√¨nh Prophet
        start_time = time.time()
        prophet_model, prophet_performance = train_prophet_model(df.copy())
        prophet_time = time.time() - start_time
        logger.info(f"Hu·∫•n luy·ªán m√¥ h√¨nh Prophet cho {symbol} ho√†n t·∫•t trong {prophet_time:.2f}s, hi·ªáu su·∫•t: {prophet_performance:.4f}")
        
        # L∆∞u m√¥ h√¨nh
        if prophet_model:
            prophet_model_binary = pickle.dumps(prophet_model)
            await model_db_manager.store_trained_model(symbol, "prophet", prophet_model_binary, prophet_performance)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu cho XGBoost 
        features = prepare_features_for_xgboost(df)
        
        # Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost
        start_time = time.time()
        xgb_model, xgb_performance = train_xgboost_model(df.copy(), features)
        xgb_time = time.time() - start_time
        logger.info(f"Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost cho {symbol} ho√†n t·∫•t trong {xgb_time:.2f}s, hi·ªáu su·∫•t: {xgb_performance:.4f}")
        
        # L∆∞u m√¥ h√¨nh
        if xgb_model:
            xgb_model_binary = pickle.dumps(xgb_model)
            await model_db_manager.store_trained_model(symbol, "xgboost", xgb_model_binary, xgb_performance)
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ
        del df, prophet_model, xgb_model
        gc.collect()
        
        return True
    except Exception as e:
        logger.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh cho {symbol}: {str(e)}")
        return False

def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """
    T·ªëi ∆∞u h√≥a b·ªô nh·ªõ s·ª≠ d·ª•ng cho DataFrame b·∫±ng c√°ch chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
    """
    # T·∫°o m·ªôt b·∫£n sao ƒë·ªÉ tr√°nh ·∫£nh h∆∞·ªüng ƒë·∫øn d·ªØ li·ªáu g·ªëc
    result = df.copy()
    
    # T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu s·ªë nguy√™n
    for col in result.select_dtypes(include=['int']):
        # Chuy·ªÉn ƒë·ªïi sang c√°c ki·ªÉu int nh·ªè h∆°n n·∫øu c√≥ th·ªÉ
        c_min = result[col].min()
        c_max = result[col].max()
        
        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
            result[col] = result[col].astype(np.int8)
        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
            result[col] = result[col].astype(np.int16)
        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
            result[col] = result[col].astype(np.int32)
    
    # T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu s·ªë th·ª±c
    for col in result.select_dtypes(include=['float']):
        # Chuy·ªÉn ƒë·ªïi sang c√°c ki·ªÉu float nh·ªè h∆°n n·∫øu c√≥ th·ªÉ
        result[col] = result[col].astype(np.float32)
    
    # T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu object
    for col in result.select_dtypes(include=['object']):
        # N·∫øu l√† c·ªôt ch·ª©a danh m·ª•c c√≥ s·ªë l∆∞·ª£ng gi√° tr·ªã nh·ªè, chuy·ªÉn sang categorical
        if result[col].nunique() < 50:  # Ng∆∞·ª°ng 50 danh m·ª•c
            result[col] = result[col].astype('category')
    
    return result

def prepare_features_for_xgboost(df: pd.DataFrame) -> list:
    """
    Chu·∫©n b·ªã v√† ch·ªçn l·ªçc ƒë·∫∑c tr∆∞ng cho m√¥ h√¨nh XGBoost
    """
    # T√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
    analyzer = TechnicalAnalyzer()
    df_with_indicators = analyzer.calculate_indicators(df)
    
    # Ch·ªçn l·ªçc ƒë·∫∑c tr∆∞ng quan tr·ªçng ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc m√¥ h√¨nh v√† tƒÉng t·ªëc ƒë·ªô
    # (C√≥ th·ªÉ s·ª≠ d·ª•ng SelectKBest, PCA, ho·∫∑c c√°c ph∆∞∆°ng ph√°p kh√°c)
    
    # Danh s√°ch c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng (v√≠ d·ª•)
    important_features = [
        'rsi_14', 'macd', 'macd_signal', 'stoch_k', 'stoch_d', 
        'ema_9', 'sma_20', 'sma_50', 'atr_14', 'adx_14'
    ]
    
    # Ch·ªâ l·∫•y c√°c ƒë·∫∑c tr∆∞ng c√≥ trong DataFrame
    available_features = [f for f in important_features if f in df_with_indicators.columns]
    
    return available_features

# T·ªëi ∆∞u h√≥a qu√° tr√¨nh d·ª± ƒëo√°n
def optimized_predict_xgboost_signal(df: pd.DataFrame, features: list, model) -> (int, float):
    """
    Phi√™n b·∫£n t·ªëi ∆∞u c·ªßa h√†m d·ª± ƒëo√°n t√≠n hi·ªáu XGBoost
    """
    # L·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t
    X = df[features].iloc[-1:].values
    
    # Ki·ªÉm tra NaN v√† thay th·∫ø
    if np.isnan(X).any():
        # Thay th·∫ø NaN b·∫±ng gi√° tr·ªã trung b√¨nh c·ªßa c·ªôt
        col_means = np.nanmean(df[features].values, axis=0)
        for i in range(X.shape[1]):
            if np.isnan(X[0, i]):
                X[0, i] = col_means[i]
    
    # D·ª± ƒëo√°n
    prediction = model.predict_proba(X)[0]
    signal = 1 if prediction[1] > 0.5 else (-1 if prediction[1] < 0.3 else 0)
    confidence = prediction[1] if signal == 1 else (1 - prediction[1] if signal == -1 else 0.5)
    
    return signal, confidence

# Thay th·∫ø h√†m train_models_for_symbol g·ªëc b·∫±ng phi√™n b·∫£n t·ªëi ∆∞u
async def train_models_for_symbol(symbol: str):
    return await optimized_train_models_for_symbol(symbol)

if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        import unittest

        class TestTechnicalAnalysis(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=50, freq='D')
                data = {
                    'open': np.random.random(50) * 100,
                    'high': np.random.random(50) * 100,
                    'low': np.random.random(50) * 100,
                    'close': np.linspace(100, 150, 50),
                    'volume': np.random.randint(1000, 5000, 50)
                }
                self.df = pd.DataFrame(data, index=dates)

            def test_calculate_indicators(self):
                analyzer = TechnicalAnalyzer()
                df_processed = analyzer.calculate_indicators(self.df)
                self.assertIn('sma20', df_processed.columns)
                self.assertIn('rsi', df_processed.columns)

        class TestForecastProphet(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=100, freq='D')
                self.df = pd.DataFrame({
                    'close': np.linspace(100, 200, 100),
                    'open': np.linspace(100, 200, 100),
                    'high': np.linspace(100, 200, 100),
                    'low': np.linspace(100, 200, 100),
                    'volume': np.random.randint(1000, 5000, 100)
                }, index=dates)

            def test_forecast_with_prophet(self):
                forecast, model = forecast_with_prophet(self.df, periods=7)
                self.assertFalse(forecast.empty)
                self.assertIn('yhat', forecast.columns)

        class TestOutlierDetection(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=7, freq='D')
                self.df = pd.DataFrame({
                    'close': [100, 101, 102, 103, 500, 104, 105]
                }, index=dates)

            def test_detect_outliers(self):
                loader = DataLoader()
                df_with_outliers, report = DataNormalizer.detect_outliers(self.df)
                self.assertIn('is_outlier', df_with_outliers.columns)
                self.assertEqual(df_with_outliers['is_outlier'].sum(), 1)
                self.assertIn('500', report)
                
        class TestDataNormalizer(unittest.TestCase):
            def setUp(self):
                dates = pd.date_range(start="2023-01-01", periods=10, freq='D')
                data = {
                    'open': [100, 101, 102, np.nan, 104, 105, 106, 107, 108, 109],
                    'high': [110, 111, 112, np.nan, 114, 115, 116, 117, 118, 119],
                    'low': [90, 91, 92, np.nan, 94, 95, 96, 97, 98, 99],
                    'close': [105, 106, 107, np.nan, 109, 110, 111, 112, 113, 114],
                    'volume': [1000, 1100, 1200, np.nan, 1400, 1500, 1600, 1700, 1800, 1900]
                }
                self.df = pd.DataFrame(data, index=dates)
                
            def test_fill_missing_values(self):
                df_filled = DataNormalizer.fill_missing_values(self.df)
                self.assertFalse(df_filled.isna().any().any())
                # Ki·ªÉm tra gi√° tr·ªã ƒë∆∞·ª£c ƒëi·ªÅn ƒë√∫ng
                self.assertEqual(df_filled['close'][3], 107)  # Gi√° tr·ªã close tr∆∞·ªõc ƒë√≥
                
            def test_normalize_dataframe(self):
                # T·∫°o DataFrame v·ªõi t√™n c·ªôt kh√°c
                df_diff_cols = self.df.copy()
                df_diff_cols.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                
                df_normalized = DataNormalizer.normalize_dataframe(df_diff_cols)
                self.assertListEqual(list(df_normalized.columns), ['open', 'high', 'low', 'close', 'volume'])

        unittest.main(argv=[sys.argv[0]])
    else:
        import nest_asyncio
        nest_asyncio.apply()
        asyncio.run(main())